{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gesture Recognization \n",
    "##### Khan Imran Jabbar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T12:53:58.910332Z",
     "iopub.status.busy": "2021-10-26T12:53:58.910092Z",
     "iopub.status.idle": "2021-10-26T12:54:03.397890Z",
     "shell.execute_reply": "2021-10-26T12:54:03.397084Z",
     "shell.execute_reply.started": "2021-10-26T12:53:58.910306Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import os\n",
    "from imageio.v2 import imread\n",
    "from skimage.transform import resize\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T12:54:03.400867Z",
     "iopub.status.busy": "2021-10-26T12:54:03.400364Z",
     "iopub.status.idle": "2021-10-26T12:54:03.953445Z",
     "shell.execute_reply": "2021-10-26T12:54:03.952688Z",
     "shell.execute_reply.started": "2021-10-26T12:54:03.400828Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setting Random Seed\n",
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T12:54:03.956784Z",
     "iopub.status.busy": "2021-10-26T12:54:03.956558Z",
     "iopub.status.idle": "2021-10-26T12:54:03.985369Z",
     "shell.execute_reply": "2021-10-26T12:54:03.984650Z",
     "shell.execute_reply.started": "2021-10-26T12:54:03.956736Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reading in the CSV that contains the information about our training and validaton data\n",
    "train_doc = np.random.permutation(open('datasets/Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('datasets/Project_data/val.csv').readlines())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Quick look at the data before we start the modeling process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the CSV as DataFrame\n",
    "#Assigning column names as the CSV doesn't contain a header\n",
    "colnames=[\"Folder\",\"Action\",\"labels\"] \n",
    "train_df =pd.read_csv(\"datasets/Project_data/train.csv\",sep=';', names=colnames ) \n",
    "val_df =pd.read_csv(\"datasets/Project_data/val.csv\",sep=';', names=colnames )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Train :  (663, 3)\n",
      "Shape of Val :  (100, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Train : \",train_df.shape)\n",
    "print(\"Shape of Val : \",val_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Action                         labels\n",
       "Left Swipe_new_Left Swipe_new  0          40\n",
       "Left_Swipe_new                 0          96\n",
       "Right Swipe_new                1          34\n",
       "Right_Swipe_new                1         103\n",
       "Stop Gesture_new               2          37\n",
       "Stop_new                       2          93\n",
       "Thumbs Down_new                3          37\n",
       "Thumbs Up_new                  4          36\n",
       "Thumbs_Down_new                3         100\n",
       "Thumbs_Up_new                  4          87\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have 663 rows in train df and 100 inn val df which correspond to the number of folders we have in our \n",
    "#training and validation dataset, so its clear that the csv are picking up the right number of files\n",
    "\n",
    "# Cheking if class labels are assigned properly in the train CSV\n",
    "train_df.groupby(\"Action\")['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The CSV has multiple names for the same action, but the class labels are correct.\n",
    "#No preprocessing required as only labels are being used for Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Action                         labels\n",
       "Left Swipe_new_Left Swipe_new  0          5\n",
       "Left_Swipe_new                 0         13\n",
       "Right Swipe_new                1         10\n",
       "Right_Swipe_new                1         13\n",
       "Stop Gesture_new               2          8\n",
       "Stop_new                       2         14\n",
       "Thumbs Down_new                3          9\n",
       "Thumbs Up_new                  4          6\n",
       "Thumbs_Down_new                3         12\n",
       "Thumbs_Up_new                  4         10\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cheking if class labels are assigned properly in the validation csv\n",
    "val_df.groupby(\"Action\")['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As with the training csv, the validation csv also has multiple name for same labels. No preprocessing required \n",
    "# as only labels are being used for validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary with class name and labels- To be used during model evaluation\n",
    "classname =\t{\"Swipe Left\": 0,\"Swipe Right\":1,\"Stop\": 2,\"Thumbs Down\": 3,\"Thumbs Up\": 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    4110\n",
       "3    4110\n",
       "0    4080\n",
       "2    3900\n",
       "4    3690\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking Class distribution for training set (Multiplied by 30 as each folder has 30 images)\n",
    "train_df[\"labels\"].value_counts()*30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a small class imbalance with class 1 and 3 having the highest number of \n",
    "# samples followed by class 0, class 2 and class respectively. The imbalance is not extremely large \n",
    "# hence we won't be correcting the imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generator\n",
    "#### Step 1 ) Create Generator\n",
    "#### Step 2) Test Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T12:54:03.988035Z",
     "iopub.status.busy": "2021-10-26T12:54:03.987614Z",
     "iopub.status.idle": "2021-10-26T12:54:04.011523Z",
     "shell.execute_reply": "2021-10-26T12:54:04.010583Z",
     "shell.execute_reply.started": "2021-10-26T12:54:03.987999Z"
    }
   },
   "outputs": [],
   "source": [
    "# This function will be used to create batch data to be fed to the Neural network\n",
    "#The generator function takes folder list, batch size , image dimension and image index as input\n",
    "# and outputs data of shape (batchsize,lengthofimageindex, image dimension(in Height,width format)\n",
    "# and channels) along with the data's label in one-hot encoded format\n",
    "\n",
    "def generator(source_path, folder_list, batch_size,imgdim,img_idx):\n",
    "    print() # blank line\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "\n",
    "  \n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        \n",
    "        # Calculating the number of batches needed to run through the data once\n",
    "        num_batches = len(folder_list) // batch_size\n",
    "        \n",
    "        # Storing the number of leftover folder to be added to the batch after all full batches\n",
    "        # are yielded\n",
    "        leftover=len(folder_list)% batch_size\n",
    "    \n",
    "        for batch in range(num_batches):      \n",
    "            batch_data = np.zeros((batch_size,len(img_idx),imgdim,imgdim,3))   \n",
    "            batch_labels = np.zeros((batch_size,5))                                \n",
    "            for folder in range(batch_size):                                         \n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) \n",
    "                for idx,item in enumerate(img_idx):         \n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image= resize(image,[imgdim,imgdim]) #----- Resize the image to dimension passed by variable imgdim\n",
    "                    \n",
    "                    # Normalizing each channel\n",
    "                    # Three normalization techniques were experimented with\n",
    "                    ## 1 - (image竏地p.min(image))/(np.max(image)竏地p.min(image))\n",
    "                    ## 2 - (image竏地p.percentile(image,5))/(np.percentile(image,95)竏地p.percentile(image,5))\n",
    "                    ## 3 - image/255 \n",
    "                    ## Please note, image above refers to the channel\n",
    "                    \n",
    "                    #### The option no 2 was chosen as  it was giving  better results during \n",
    "                    #### trial model runs\n",
    "                    batch_data[folder,idx,:,:,0] = (image[:, : , 0] - np.percentile(image[:, : , 0],5))/ (np.percentile(image[:, : , 0],95) - np.percentile(image[:, : , 0],5))\n",
    "                    batch_data[folder,idx,:,:,1] = (image[:, : , 1] - np.percentile(image[:, : , 1],5))/ (np.percentile(image[:, : , 1],95) - np.percentile(image[:, : , 1],5))\n",
    "                    batch_data[folder,idx,:,:,2] = (image[:, : , 2] - np.percentile(image[:, : , 2],5))/ (np.percentile(image[:, : , 2],95) - np.percentile(image[:, : , 2],5))\n",
    "                # Adding labels in one-hot encoding format\n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels \n",
    "\n",
    "        # Processing extra batch in case the num of batches calculated above was unable to run\n",
    "        # all data . All other processing remains same as above\n",
    "        if (leftover):\n",
    "            batch_data = np.zeros((leftover,len(img_idx),imgdim,imgdim,3)) \n",
    "            batch_labels = np.zeros((leftover,5)) \n",
    "            for folder in range(leftover): \n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) \n",
    "                for idx,item in enumerate(img_idx): \n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image= resize(image,[imgdim,imgdim])     \n",
    "                    batch_data[folder,idx,:,:,0] = (image[:, : , 0] - np.percentile(image[:, : , 0],5))/ (np.percentile(image[:, : , 0],95) - np.percentile(image[:, : , 0],5))\n",
    "                    batch_data[folder,idx,:,:,1] = (image[:, : , 1] - np.percentile(image[:, : , 1],5))/ (np.percentile(image[:, : , 1],95) - np.percentile(image[:, : , 1],5))\n",
    "                    batch_data[folder,idx,:,:,2] = (image[:, : , 2] - np.percentile(image[:, : , 2],5))/ (np.percentile(image[:, : , 2],95) - np.percentile(image[:, : , 2],5))\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Generator Test\n",
    "#### Testing generator with random input to ensure its generating data in the correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializating variable num_train_sequences and num_train_sequences, which are the number of \n",
    "# folder we have and this will be used to calculate steps_per_epoch and is also needed now to \n",
    "# test the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T12:54:04.013245Z",
     "iopub.status.busy": "2021-10-26T12:54:04.012927Z",
     "iopub.status.idle": "2021-10-26T12:54:04.026406Z",
     "shell.execute_reply": "2021-10-26T12:54:04.025546Z",
     "shell.execute_reply.started": "2021-10-26T12:54:04.013167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sequences = 663\n",
      "Number of validation sequences = 100\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'datasets/Project_data/train'\n",
    "val_path = 'datasets/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('Number of training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('Number of validation sequences =', num_val_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Source path =  datasets/Project_data/train ; batch size = 4\n",
      "Shape of training input : (4, 30, 120, 120, 3)\n",
      "One-Hot code encoded labels :\n",
      "[[0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Data Generator - Test 1\n",
    "batch_size =4\n",
    "imgdim =120\n",
    "img_idx =  [x for x in range(0,30)]\n",
    "train_generator = generator(train_path, train_doc, batch_size,imgdim,img_idx)\n",
    "frames, label = next(train_generator)\n",
    "print(\"Shape of training input :\",frames.shape)\n",
    "print(\"One-Hot code encoded labels :\")\n",
    "print(label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result\n",
    "Data Generator Test 1 passed as the data received is as expected i.e. batch size, number of images per folder , image dimension and channels are whats was passed to the generator function\n",
    "The generator also gave 4 one-hot encoded labels which correspond to the number of folder in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Source path =  datasets/Project_data/train ; batch size = 8\n",
      "Shape of training input : (8, 15, 220, 220, 3)\n",
      "One-Hot code encoded labels :\n",
      "[[0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Data Generator - Test 2\n",
    "batch_size =8 # Increased batch size\n",
    "imgdim =220 # Increased image dimensions\n",
    "img_idx =  [x for x in range(0,30,2)] # Decreased number of image to half\n",
    "train_generator = generator(train_path, train_doc, batch_size,imgdim,img_idx)\n",
    "frames, label = next(train_generator)\n",
    "print(\"Shape of training input :\",frames.shape)\n",
    "print(\"One-Hot code encoded labels :\")\n",
    "print(label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result\n",
    "Data Generator Test 2 passed as the data received is as expected\n",
    "i.e. batch size, number of images per folder , image dimension and channels are whats was passed.\n",
    "The generator also gave 8 one-hot encoded labels which correspond to the number of  folder in the batch. Also the dimension were scaled compared to previous test and number of images per folder were halved for the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to calculate number of steps required per epoch (train and validation)\n",
    "# Converted code to function to avoid code clutter\n",
    "\n",
    "def setSteps_epoch(batch_size):\n",
    "    if (num_train_sequences%batch_size) == 0:\n",
    "        steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "        return steps_per_epoch\n",
    "    else:\n",
    "        steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "        return steps_per_epoch\n",
    "def setSteps_validation(batch_size):\n",
    "    if (num_val_sequences%batch_size) == 0:\n",
    "        validation_steps = int(num_val_sequences/batch_size)\n",
    "        return validation_steps\n",
    "    else:\n",
    "        validation_steps = (num_val_sequences//batch_size) + 1\n",
    "        return validation_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The trainmodel function ##\n",
    "#### We will be using this function to fit all the models\n",
    "#### The function will perform all necessary steps like generating steps per epoch etc\n",
    "## Error Exception implemented to reduce batch size in case of ExhaustedResourcesError\n",
    "\n",
    "def trainmodel(modelname,batch_size):\n",
    "    # Calculation steps per epoch/validation for every model in case of change of batch size by \n",
    "    # calling the setSteps functions\n",
    "    print(\"_________________\")\n",
    "    print(\"Batch Size is :\",batch_size)\n",
    "    \n",
    "    steps_per_epoch= setSteps_epoch(batch_size)\n",
    "    validation_steps= setSteps_validation(batch_size)\n",
    "    \n",
    "    #Initializing the generator function so we can iterate through the batches\n",
    "    \n",
    "    train_generator = generator(train_path, train_doc, batch_size,imgdim,img_idx)\n",
    "    val_generator = generator(val_path, val_doc, batch_size,imgdim,img_idx)\n",
    "    \n",
    "    # Fitting the compiled model\n",
    "    \n",
    "    try:\n",
    "        train_start_time=datetime.datetime.now()\n",
    "        modelname.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                        callbacks=callbacks_list, validation_data=val_generator, \n",
    "                        validation_steps=validation_steps, class_weight=None, initial_epoch=0)\n",
    "        time_taken_to_train =datetime.datetime.now()-train_start_time\n",
    "        print(\"Time Taken to Train this model was : \",time_taken_to_train)\n",
    "    \n",
    "## The project contains different type of models with different parameters and architecture, and hence some model exhaust\n",
    "## resources even with a lower batch size, hence we will use error catching to set the batch size. We will start with a \n",
    "## value of 8 for all and than keep coming down till we find a batchs size that works\n",
    "\n",
    "    # Catching resource exhausted error and decreasing the batch size to prevent \n",
    "    # having to manually update the batch size in case of failure\n",
    "    \n",
    "    \n",
    "    except tf.errors.ResourceExhaustedError:\n",
    "        print(\"Reducing Batch Size\")\n",
    "        if(batch_size==1):\n",
    "            print(\"Cannot reduce Batch size anymore, it cannot be less than 1\")\n",
    "        elif (batch_size>=8):\n",
    "            batch_size -=4\n",
    "            trainmodel(modelname,batch_size)\n",
    "        elif (batch_size>=4):\n",
    "            batch_size -=2\n",
    "            trainmodel(modelname,batch_size)\n",
    "        elif (batch_size>=2):\n",
    "            batch_size -=1\n",
    "            trainmodel(modelname,batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting initial default Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_idx =  [x for x in range(0,30)]  #--- Number of images per folder to be used for training\n",
    "                                    #-Can be manipulated to decrease image between 1-30(0-29-idx)\n",
    "\n",
    "# Image dimension for the resizing - using 120 to match the smallest dimension available in the dataset\n",
    "\n",
    "imgdim =120 # --- Image Dimension-- Only one parameter specified as we will use a square image\n",
    "\n",
    "num_epochs=20 #----- Set default number of epochs --- Can be overridden if needed before call to model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing some of the libraries we will be using to create our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization, Activation, Dropout, ReLU,GlobalAveragePooling3D\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D,LayerNormalization,Input,GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import LSTM, GRU, TimeDistributed\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting parameters that will be used will during training the model\n",
    "#### We will be setting ReduceLROnPlateau to modify the LR in case of no improvement in Val_loss\n",
    "#### after 3 epoch\n",
    "#### Earlystopping was experimented with, but was not implemented in the final model fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Save to file parameters \n",
    "## Repurposed from starter code \n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', save_freq='epoch')\n",
    "\n",
    "# We will Reduce the learning rate if no improvement in val_loss is seen after 3 epochs\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, cooldown=1, verbose=1)\n",
    "\n",
    "callbacks_list = [LR,checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: CNN - Conv3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental model - Random architecture - \n",
    "#Initial Filter of (1,3,3)followed by (3,1,1) and repeated (1,3,3) and (3,1,1) with \n",
    "#Layer Normalization and ReLU activation, batch Normalization is only used once after the first 2 \n",
    "#Conv3d\n",
    "# Filter size was chosen after some articles suggested that this filter size could be a good fit to extract feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "\n",
    "Input_shape = (len(img_idx), imgdim, imgdim, 3)\n",
    "model_cnn_1 = Sequential()\n",
    "model_cnn_1.add(Conv3D(16, (1,3,3),padding=\"same\",input_shape=Input_shape))\n",
    "model_cnn_1.add(Conv3D(16, (3,1,1),padding=\"same\"))\n",
    "model_cnn_1.add(BatchNormalization())\n",
    "model_cnn_1.add(ReLU())\n",
    "\n",
    "model_cnn_1.add(Conv3D(16, (1,3,3),padding=\"same\"))\n",
    "model_cnn_1.add(Conv3D(16, (3,1,1),padding=\"same\"))\n",
    "model_cnn_1.add(LayerNormalization())\n",
    "model_cnn_1.add(ReLU())\n",
    "model_cnn_1.add(Conv3D(16, (1,3,3),padding=\"same\"))\n",
    "model_cnn_1.add(Conv3D(16, (3,1,1),padding=\"same\"))\n",
    "model_cnn_1.add(LayerNormalization())\n",
    "\n",
    "model_cnn_1.add(Conv3D(32, (1,3,3),padding=\"same\"))\n",
    "model_cnn_1.add(Conv3D(32, (3,1,1),padding=\"same\"))\n",
    "model_cnn_1.add(LayerNormalization())\n",
    "model_cnn_1.add(ReLU())\n",
    "model_cnn_1.add(Conv3D(32, (1,3,3),padding=\"same\"))\n",
    "model_cnn_1.add(Conv3D(32, (3,1,1),padding=\"same\"))\n",
    "model_cnn_1.add(LayerNormalization())\n",
    "\n",
    "\n",
    "model_cnn_1.add(Flatten())\n",
    "\n",
    "\n",
    "model_cnn_1.add(Dense(5,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T06:20:07.757837Z",
     "iopub.status.busy": "2021-10-26T06:20:07.757588Z",
     "iopub.status.idle": "2021-10-26T06:20:07.929675Z",
     "shell.execute_reply": "2021-10-26T06:20:07.929025Z",
     "shell.execute_reply.started": "2021-10-26T06:20:07.757808Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d (Conv3D)             (None, 30, 120, 120, 16)  448       \n",
      "                                                                 \n",
      " conv3d_1 (Conv3D)           (None, 30, 120, 120, 16)  784       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 30, 120, 120, 16)  64       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 30, 120, 120, 16)  0         \n",
      "                                                                 \n",
      " conv3d_2 (Conv3D)           (None, 30, 120, 120, 16)  2320      \n",
      "                                                                 \n",
      " conv3d_3 (Conv3D)           (None, 30, 120, 120, 16)  784       \n",
      "                                                                 \n",
      " layer_normalization (LayerN  (None, 30, 120, 120, 16)  32       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 30, 120, 120, 16)  0         \n",
      "                                                                 \n",
      " conv3d_4 (Conv3D)           (None, 30, 120, 120, 16)  2320      \n",
      "                                                                 \n",
      " conv3d_5 (Conv3D)           (None, 30, 120, 120, 16)  784       \n",
      "                                                                 \n",
      " layer_normalization_1 (Laye  (None, 30, 120, 120, 16)  32       \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " conv3d_6 (Conv3D)           (None, 30, 120, 120, 32)  4640      \n",
      "                                                                 \n",
      " conv3d_7 (Conv3D)           (None, 30, 120, 120, 32)  3104      \n",
      "                                                                 \n",
      " layer_normalization_2 (Laye  (None, 30, 120, 120, 32)  64       \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_2 (ReLU)              (None, 30, 120, 120, 32)  0         \n",
      "                                                                 \n",
      " conv3d_8 (Conv3D)           (None, 30, 120, 120, 32)  9248      \n",
      "                                                                 \n",
      " conv3d_9 (Conv3D)           (None, 30, 120, 120, 32)  3104      \n",
      "                                                                 \n",
      " layer_normalization_3 (Laye  (None, 30, 120, 120, 32)  64       \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 13824000)          0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 69120005  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 69,147,797\n",
      "Trainable params: 69,147,765\n",
      "Non-trainable params: 32\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Compiling the model using Adam as optimizer\n",
    "model_cnn_1.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_cnn_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________\n",
      "Batch Size is : 8\n",
      "\n",
      "Source path =  datasets/Project_data/train ; batch size = 8\n",
      "Epoch 1/20\n",
      "Reducing Batch Size\n",
      "_________________\n",
      "Batch Size is : 4\n",
      "\n",
      "Source path =  datasets/Project_data/train ; batch size = 4\n",
      "Epoch 1/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 2578.1406 - categorical_accuracy: 0.3831\n",
      "Source path =  datasets/Project_data/val ; batch size = 4\n",
      "\n",
      "Epoch 1: saving model to model_init_2023-02-1110_46_15.734516\\model-00001-2578.14062-0.38311-1777.26843-0.46000.h5\n",
      "166/166 [==============================] - 127s 754ms/step - loss: 2578.1406 - categorical_accuracy: 0.3831 - val_loss: 1777.2684 - val_categorical_accuracy: 0.4600 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1075.3059 - categorical_accuracy: 0.6305\n",
      "Epoch 2: saving model to model_init_2023-02-1110_46_15.734516\\model-00002-1075.30591-0.63047-1538.53113-0.53000.h5\n",
      "166/166 [==============================] - 123s 741ms/step - loss: 1075.3059 - categorical_accuracy: 0.6305 - val_loss: 1538.5311 - val_categorical_accuracy: 0.5300 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 495.1875 - categorical_accuracy: 0.7919\n",
      "Epoch 3: saving model to model_init_2023-02-1110_46_15.734516\\model-00003-495.18750-0.79186-988.55096-0.64000.h5\n",
      "166/166 [==============================] - 123s 742ms/step - loss: 495.1875 - categorical_accuracy: 0.7919 - val_loss: 988.5510 - val_categorical_accuracy: 0.6400 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 208.5093 - categorical_accuracy: 0.8688\n",
      "Epoch 4: saving model to model_init_2023-02-1110_46_15.734516\\model-00004-208.50929-0.86878-1057.19604-0.63000.h5\n",
      "166/166 [==============================] - 124s 748ms/step - loss: 208.5093 - categorical_accuracy: 0.8688 - val_loss: 1057.1960 - val_categorical_accuracy: 0.6300 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 174.7245 - categorical_accuracy: 0.8793\n",
      "Epoch 5: saving model to model_init_2023-02-1110_46_15.734516\\model-00005-174.72449-0.87934-1166.36792-0.61000.h5\n",
      "166/166 [==============================] - 124s 751ms/step - loss: 174.7245 - categorical_accuracy: 0.8793 - val_loss: 1166.3679 - val_categorical_accuracy: 0.6100 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 97.6505 - categorical_accuracy: 0.9276\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 6: saving model to model_init_2023-02-1110_46_15.734516\\model-00006-97.65045-0.92760-1264.48718-0.63000.h5\n",
      "166/166 [==============================] - 123s 740ms/step - loss: 97.6505 - categorical_accuracy: 0.9276 - val_loss: 1264.4872 - val_categorical_accuracy: 0.6300 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 40.0918 - categorical_accuracy: 0.9683\n",
      "Epoch 7: saving model to model_init_2023-02-1110_46_15.734516\\model-00007-40.09179-0.96833-1052.20850-0.67000.h5\n",
      "166/166 [==============================] - 122s 738ms/step - loss: 40.0918 - categorical_accuracy: 0.9683 - val_loss: 1052.2085 - val_categorical_accuracy: 0.6700 - lr: 5.0000e-04\n",
      "Epoch 8/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 3.5066 - categorical_accuracy: 0.9894\n",
      "Epoch 8: saving model to model_init_2023-02-1110_46_15.734516\\model-00008-3.50657-0.98944-1109.63049-0.67000.h5\n",
      "166/166 [==============================] - 123s 741ms/step - loss: 3.5066 - categorical_accuracy: 0.9894 - val_loss: 1109.6305 - val_categorical_accuracy: 0.6700 - lr: 5.0000e-04\n",
      "Epoch 9/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 6.7868 - categorical_accuracy: 0.9879\n",
      "Epoch 9: saving model to model_init_2023-02-1110_46_15.734516\\model-00009-6.78684-0.98793-920.43353-0.62000.h5\n",
      "166/166 [==============================] - 122s 735ms/step - loss: 6.7868 - categorical_accuracy: 0.9879 - val_loss: 920.4335 - val_categorical_accuracy: 0.6200 - lr: 5.0000e-04\n",
      "Epoch 10/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 5.4489 - categorical_accuracy: 0.9879\n",
      "Epoch 10: saving model to model_init_2023-02-1110_46_15.734516\\model-00010-5.44888-0.98793-833.48462-0.65000.h5\n",
      "166/166 [==============================] - 122s 739ms/step - loss: 5.4489 - categorical_accuracy: 0.9879 - val_loss: 833.4846 - val_categorical_accuracy: 0.6500 - lr: 5.0000e-04\n",
      "Epoch 11/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 10.6925 - categorical_accuracy: 0.9894\n",
      "Epoch 11: saving model to model_init_2023-02-1110_46_15.734516\\model-00011-10.69255-0.98944-902.16235-0.68000.h5\n",
      "166/166 [==============================] - 123s 740ms/step - loss: 10.6925 - categorical_accuracy: 0.9894 - val_loss: 902.1624 - val_categorical_accuracy: 0.6800 - lr: 5.0000e-04\n",
      "Epoch 12/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 19.2683 - categorical_accuracy: 0.9774\n",
      "Epoch 12: saving model to model_init_2023-02-1110_46_15.734516\\model-00012-19.26831-0.97738-776.43445-0.69000.h5\n",
      "166/166 [==============================] - 122s 738ms/step - loss: 19.2683 - categorical_accuracy: 0.9774 - val_loss: 776.4344 - val_categorical_accuracy: 0.6900 - lr: 5.0000e-04\n",
      "Epoch 13/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 15.4432 - categorical_accuracy: 0.9819\n",
      "Epoch 13: saving model to model_init_2023-02-1110_46_15.734516\\model-00013-15.44317-0.98190-1131.18787-0.63000.h5\n",
      "166/166 [==============================] - 124s 746ms/step - loss: 15.4432 - categorical_accuracy: 0.9819 - val_loss: 1131.1879 - val_categorical_accuracy: 0.6300 - lr: 5.0000e-04\n",
      "Epoch 14/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 31.6012 - categorical_accuracy: 0.9789\n",
      "Epoch 14: saving model to model_init_2023-02-1110_46_15.734516\\model-00014-31.60124-0.97888-807.13562-0.72000.h5\n",
      "166/166 [==============================] - 122s 738ms/step - loss: 31.6012 - categorical_accuracy: 0.9789 - val_loss: 807.1356 - val_categorical_accuracy: 0.7200 - lr: 5.0000e-04\n",
      "Epoch 15/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 16.1641 - categorical_accuracy: 0.9879\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 15: saving model to model_init_2023-02-1110_46_15.734516\\model-00015-16.16411-0.98793-1037.47229-0.61000.h5\n",
      "166/166 [==============================] - 123s 745ms/step - loss: 16.1641 - categorical_accuracy: 0.9879 - val_loss: 1037.4723 - val_categorical_accuracy: 0.6100 - lr: 5.0000e-04\n",
      "Epoch 16/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 6.0494 - categorical_accuracy: 0.9894\n",
      "Epoch 16: saving model to model_init_2023-02-1110_46_15.734516\\model-00016-6.04939-0.98944-1262.61841-0.60000.h5\n",
      "166/166 [==============================] - 123s 741ms/step - loss: 6.0494 - categorical_accuracy: 0.9894 - val_loss: 1262.6184 - val_categorical_accuracy: 0.6000 - lr: 2.5000e-04\n",
      "Epoch 17/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.3915 - categorical_accuracy: 0.9970\n",
      "Epoch 17: saving model to model_init_2023-02-1110_46_15.734516\\model-00017-0.39146-0.99698-954.05890-0.65000.h5\n",
      "166/166 [==============================] - 123s 743ms/step - loss: 0.3915 - categorical_accuracy: 0.9970 - val_loss: 954.0589 - val_categorical_accuracy: 0.6500 - lr: 2.5000e-04\n",
      "Epoch 18/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0000e+00 - categorical_accuracy: 1.0000\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 18: saving model to model_init_2023-02-1110_46_15.734516\\model-00018-0.00000-1.00000-1107.17053-0.65000.h5\n",
      "166/166 [==============================] - 123s 740ms/step - loss: 0.0000e+00 - categorical_accuracy: 1.0000 - val_loss: 1107.1705 - val_categorical_accuracy: 0.6500 - lr: 2.5000e-04\n",
      "Epoch 19/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1525 - categorical_accuracy: 0.9985\n",
      "Epoch 19: saving model to model_init_2023-02-1110_46_15.734516\\model-00019-0.15245-0.99849-1145.87183-0.63000.h5\n",
      "166/166 [==============================] - 123s 743ms/step - loss: 0.1525 - categorical_accuracy: 0.9985 - val_loss: 1145.8718 - val_categorical_accuracy: 0.6300 - lr: 1.2500e-04\n",
      "Epoch 20/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 2.2561 - categorical_accuracy: 0.9955\n",
      "Epoch 20: saving model to model_init_2023-02-1110_46_15.734516\\model-00020-2.25615-0.99548-1032.81689-0.69000.h5\n",
      "166/166 [==============================] - 122s 738ms/step - loss: 2.2561 - categorical_accuracy: 0.9955 - val_loss: 1032.8169 - val_categorical_accuracy: 0.6900 - lr: 1.2500e-04\n",
      "Time Taken to Train this model was :  0:41:00.752651\n"
     ]
    }
   ],
   "source": [
    "## Using trainmodel function to train, values passed is model name and batch size\n",
    "batch_size = 8\n",
    "trainmodel(model_cnn_1,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_1.save(\"model_cnn_1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: CNN - Conv3D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental model - Random architecture - \n",
    "#Initial Filter of (3,3,3) and maxpooling with constant pool size of (3,3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture for model_cnn_2\n",
    "Input_shape = (len(img_idx), imgdim, imgdim, 3)\n",
    "\n",
    "model_cnn_2 = Sequential()\n",
    "model_cnn_2.add(Conv3D(16, (3,3,3),input_shape=Input_shape,padding=\"same\"))\n",
    "model_cnn_2.add(BatchNormalization())\n",
    "model_cnn_2.add(Activation('relu'))\n",
    "model_cnn_2.add(MaxPooling3D(pool_size=(3,3,3)))\n",
    "\n",
    "model_cnn_2.add(Conv3D(32,(3,3,3),padding='same'))\n",
    "model_cnn_2.add(BatchNormalization())\n",
    "model_cnn_2.add(Activation('relu'))\n",
    "model_cnn_2.add(MaxPooling3D(pool_size=(3,3,3)))\n",
    "model_cnn_2.add(layers.Dropout(0.25))\n",
    "\n",
    "model_cnn_2.add(Conv3D(64,(3,3,3),padding='same'))\n",
    "model_cnn_2.add(BatchNormalization())\n",
    "model_cnn_2.add(Activation('relu'))\n",
    "model_cnn_2.add(MaxPooling3D(pool_size=(3,3,3)))\n",
    "\n",
    "\n",
    "model_cnn_2.add(Flatten())\n",
    "model_cnn_2.add(Dense(512, activation='relu'))\n",
    "model_cnn_2.add(layers.Dropout(0.5))\n",
    "\n",
    "model_cnn_2.add(Dense(5,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T06:20:07.757837Z",
     "iopub.status.busy": "2021-10-26T06:20:07.757588Z",
     "iopub.status.idle": "2021-10-26T06:20:07.929675Z",
     "shell.execute_reply": "2021-10-26T06:20:07.929025Z",
     "shell.execute_reply.started": "2021-10-26T06:20:07.757808Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_10 (Conv3D)          (None, 30, 120, 120, 16)  1312      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 30, 120, 120, 16)  64       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation (Activation)     (None, 30, 120, 120, 16)  0         \n",
      "                                                                 \n",
      " max_pooling3d (MaxPooling3D  (None, 10, 40, 40, 16)   0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv3d_11 (Conv3D)          (None, 10, 40, 40, 32)    13856     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 10, 40, 40, 32)   128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 10, 40, 40, 32)    0         \n",
      "                                                                 \n",
      " max_pooling3d_1 (MaxPooling  (None, 3, 13, 13, 32)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 3, 13, 13, 32)     0         \n",
      "                                                                 \n",
      " conv3d_12 (Conv3D)          (None, 3, 13, 13, 64)     55360     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 3, 13, 13, 64)    256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 3, 13, 13, 64)     0         \n",
      "                                                                 \n",
      " max_pooling3d_2 (MaxPooling  (None, 1, 4, 4, 64)      0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 598,341\n",
      "Trainable params: 598,117\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Compiling the model using Adam as optimizer\n",
    "model_cnn_2.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_cnn_2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________\n",
      "Batch Size is : 8\n",
      "\n",
      "Source path =  datasets/Project_data/train ; batch size = 8\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 3.3947 - categorical_accuracy: 0.2881\n",
      "Source path =  datasets/Project_data/val ; batch size = 8\n",
      "\n",
      "Epoch 1: saving model to model_init_2023-02-0720_24_17.938535\\model-00001-3.39474-0.28808-2.17865-0.16000.h5\n",
      "83/83 [==============================] - 123s 1s/step - loss: 3.3947 - categorical_accuracy: 0.2881 - val_loss: 2.1787 - val_categorical_accuracy: 0.1600 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.3645 - categorical_accuracy: 0.4419\n",
      "Epoch 2: saving model to model_init_2023-02-0720_24_17.938535\\model-00002-1.36450-0.44193-3.44257-0.19000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 1.3645 - categorical_accuracy: 0.4419 - val_loss: 3.4426 - val_categorical_accuracy: 0.1900 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.1844 - categorical_accuracy: 0.5294\n",
      "Epoch 3: saving model to model_init_2023-02-0720_24_17.938535\\model-00003-1.18436-0.52941-2.88452-0.23000.h5\n",
      "83/83 [==============================] - 122s 1s/step - loss: 1.1844 - categorical_accuracy: 0.5294 - val_loss: 2.8845 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.8976 - categorical_accuracy: 0.6546\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 4: saving model to model_init_2023-02-0720_24_17.938535\\model-00004-0.89759-0.65460-4.04710-0.21000.h5\n",
      "83/83 [==============================] - 123s 1s/step - loss: 0.8976 - categorical_accuracy: 0.6546 - val_loss: 4.0471 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.6146 - categorical_accuracy: 0.7738\n",
      "Epoch 5: saving model to model_init_2023-02-0720_24_17.938535\\model-00005-0.61463-0.77376-2.64578-0.24000.h5\n",
      "83/83 [==============================] - 123s 1s/step - loss: 0.6146 - categorical_accuracy: 0.7738 - val_loss: 2.6458 - val_categorical_accuracy: 0.2400 - lr: 5.0000e-04\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.4737 - categorical_accuracy: 0.8250\n",
      "Epoch 6: saving model to model_init_2023-02-0720_24_17.938535\\model-00006-0.47366-0.82504-0.90250-0.69000.h5\n",
      "83/83 [==============================] - 122s 1s/step - loss: 0.4737 - categorical_accuracy: 0.8250 - val_loss: 0.9025 - val_categorical_accuracy: 0.6900 - lr: 5.0000e-04\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.4687 - categorical_accuracy: 0.8371\n",
      "Epoch 7: saving model to model_init_2023-02-0720_24_17.938535\\model-00007-0.46867-0.83710-0.70112-0.69000.h5\n",
      "83/83 [==============================] - 123s 1s/step - loss: 0.4687 - categorical_accuracy: 0.8371 - val_loss: 0.7011 - val_categorical_accuracy: 0.6900 - lr: 5.0000e-04\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.3683 - categorical_accuracy: 0.8658\n",
      "Epoch 8: saving model to model_init_2023-02-0720_24_17.938535\\model-00008-0.36833-0.86576-0.38444-0.87000.h5\n",
      "83/83 [==============================] - 123s 1s/step - loss: 0.3683 - categorical_accuracy: 0.8658 - val_loss: 0.3844 - val_categorical_accuracy: 0.8700 - lr: 5.0000e-04\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.3333 - categorical_accuracy: 0.8914\n",
      "Epoch 9: saving model to model_init_2023-02-0720_24_17.938535\\model-00009-0.33332-0.89140-0.44328-0.84000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 0.3333 - categorical_accuracy: 0.8914 - val_loss: 0.4433 - val_categorical_accuracy: 0.8400 - lr: 5.0000e-04\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.3076 - categorical_accuracy: 0.8914\n",
      "Epoch 10: saving model to model_init_2023-02-0720_24_17.938535\\model-00010-0.30756-0.89140-0.47950-0.84000.h5\n",
      "83/83 [==============================] - 123s 1s/step - loss: 0.3076 - categorical_accuracy: 0.8914 - val_loss: 0.4795 - val_categorical_accuracy: 0.8400 - lr: 5.0000e-04\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.2323 - categorical_accuracy: 0.9186\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 11: saving model to model_init_2023-02-0720_24_17.938535\\model-00011-0.23230-0.91855-0.60766-0.76000.h5\n",
      "83/83 [==============================] - 123s 1s/step - loss: 0.2323 - categorical_accuracy: 0.9186 - val_loss: 0.6077 - val_categorical_accuracy: 0.7600 - lr: 5.0000e-04\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.2440 - categorical_accuracy: 0.9125\n",
      "Epoch 12: saving model to model_init_2023-02-0720_24_17.938535\\model-00012-0.24396-0.91252-0.31872-0.90000.h5\n",
      "83/83 [==============================] - 122s 1s/step - loss: 0.2440 - categorical_accuracy: 0.9125 - val_loss: 0.3187 - val_categorical_accuracy: 0.9000 - lr: 2.5000e-04\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.1749 - categorical_accuracy: 0.9397\n",
      "Epoch 13: saving model to model_init_2023-02-0720_24_17.938535\\model-00013-0.17492-0.93967-0.29558-0.90000.h5\n",
      "83/83 [==============================] - 122s 1s/step - loss: 0.1749 - categorical_accuracy: 0.9397 - val_loss: 0.2956 - val_categorical_accuracy: 0.9000 - lr: 2.5000e-04\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.1767 - categorical_accuracy: 0.9397\n",
      "Epoch 14: saving model to model_init_2023-02-0720_24_17.938535\\model-00014-0.17672-0.93967-0.33948-0.88000.h5\n",
      "83/83 [==============================] - 123s 1s/step - loss: 0.1767 - categorical_accuracy: 0.9397 - val_loss: 0.3395 - val_categorical_accuracy: 0.8800 - lr: 2.5000e-04\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.1442 - categorical_accuracy: 0.9502\n",
      "Epoch 15: saving model to model_init_2023-02-0720_24_17.938535\\model-00015-0.14425-0.95023-0.30165-0.90000.h5\n",
      "83/83 [==============================] - 123s 1s/step - loss: 0.1442 - categorical_accuracy: 0.9502 - val_loss: 0.3017 - val_categorical_accuracy: 0.9000 - lr: 2.5000e-04\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.1299 - categorical_accuracy: 0.9517\n",
      "Epoch 16: saving model to model_init_2023-02-0720_24_17.938535\\model-00016-0.12994-0.95173-0.21656-0.95000.h5\n",
      "83/83 [==============================] - 122s 1s/step - loss: 0.1299 - categorical_accuracy: 0.9517 - val_loss: 0.2166 - val_categorical_accuracy: 0.9500 - lr: 2.5000e-04\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.1198 - categorical_accuracy: 0.9668\n",
      "Epoch 17: saving model to model_init_2023-02-0720_24_17.938535\\model-00017-0.11983-0.96682-0.31624-0.92000.h5\n",
      "83/83 [==============================] - 123s 2s/step - loss: 0.1198 - categorical_accuracy: 0.9668 - val_loss: 0.3162 - val_categorical_accuracy: 0.9200 - lr: 2.5000e-04\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.1005 - categorical_accuracy: 0.9653\n",
      "Epoch 18: saving model to model_init_2023-02-0720_24_17.938535\\model-00018-0.10048-0.96531-0.35981-0.88000.h5\n",
      "83/83 [==============================] - 122s 1s/step - loss: 0.1005 - categorical_accuracy: 0.9653 - val_loss: 0.3598 - val_categorical_accuracy: 0.8800 - lr: 2.5000e-04\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.1268 - categorical_accuracy: 0.9623\n",
      "Epoch 19: saving model to model_init_2023-02-0720_24_17.938535\\model-00019-0.12681-0.96229-0.08446-0.98000.h5\n",
      "83/83 [==============================] - 123s 2s/step - loss: 0.1268 - categorical_accuracy: 0.9623 - val_loss: 0.0845 - val_categorical_accuracy: 0.9800 - lr: 2.5000e-04\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.0949 - categorical_accuracy: 0.9698\n",
      "Epoch 20: saving model to model_init_2023-02-0720_24_17.938535\\model-00020-0.09486-0.96983-0.30102-0.90000.h5\n",
      "83/83 [==============================] - 123s 1s/step - loss: 0.0949 - categorical_accuracy: 0.9698 - val_loss: 0.3010 - val_categorical_accuracy: 0.9000 - lr: 2.5000e-04\n",
      "Time Taken to Train this model was :  0:40:57.366427\n"
     ]
    }
   ],
   "source": [
    "batch_size=8\n",
    "trainmodel(model_cnn_2,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_2.save(\"model_cnn_2.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 - C3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Model based on the paper \"Learning Spatiotemporal Features with 3D Convolutional Networks\" \n",
    "# https://arxiv.org/pdf/1412.0767.pdf\n",
    "\n",
    "# Consisting of 8 CONv3D with 5 max pooling and 2 fully connected layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Architecture for C3D\n",
    "Input_shape = (len(img_idx), imgdim, imgdim, 3)\n",
    "model_C3D = Sequential()\n",
    "model_C3D.add(Conv3D(64, (3,3,3),strides=(1, 1, 1),padding=\"same\",input_shape=Input_shape))\n",
    "model_C3D.add(MaxPooling3D(pool_size=(1, 2, 2)))\n",
    "\n",
    "model_C3D.add(Conv3D(128, (3,3,3),strides=(1, 1, 1),padding=\"same\",activation='relu'))\n",
    "model_C3D.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_C3D.add(Conv3D(256, (3,3,3),strides=(1, 1, 1),padding=\"same\",activation='relu'))\n",
    "model_C3D.add(Conv3D(256, (3,3,3),strides=(1, 1, 1),padding=\"same\",activation='relu'))\n",
    "model_C3D.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_C3D.add(Conv3D(512, (3,3,3),strides=(1, 1, 1),padding=\"same\",activation='relu'))\n",
    "model_C3D.add(Conv3D(512, (3,3,3),strides=(1, 1, 1),padding=\"same\",activation='relu'))\n",
    "model_C3D.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_C3D.add(Conv3D(512, (3,3,3),strides=(1, 1, 1),padding=\"same\",activation='relu'))\n",
    "model_C3D.add(Conv3D(512, (3,3,3),strides=(1, 1, 1),padding=\"same\",activation='relu'))\n",
    "model_C3D.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_C3D.add(Flatten())\n",
    "\n",
    "model_C3D.add(Dense(4096, activation='relu'))\n",
    "\n",
    "model_C3D.add(Dense(4096, activation='relu'))\n",
    "\n",
    "model_C3D.add(Dense(5,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_13 (Conv3D)          (None, 30, 120, 120, 64)  5248      \n",
      "                                                                 \n",
      " max_pooling3d_3 (MaxPooling  (None, 30, 60, 60, 64)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_14 (Conv3D)          (None, 30, 60, 60, 128)   221312    \n",
      "                                                                 \n",
      " max_pooling3d_4 (MaxPooling  (None, 15, 30, 30, 128)  0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_15 (Conv3D)          (None, 15, 30, 30, 256)   884992    \n",
      "                                                                 \n",
      " conv3d_16 (Conv3D)          (None, 15, 30, 30, 256)   1769728   \n",
      "                                                                 \n",
      " max_pooling3d_5 (MaxPooling  (None, 7, 15, 15, 256)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_17 (Conv3D)          (None, 7, 15, 15, 512)    3539456   \n",
      "                                                                 \n",
      " conv3d_18 (Conv3D)          (None, 7, 15, 15, 512)    7078400   \n",
      "                                                                 \n",
      " max_pooling3d_6 (MaxPooling  (None, 3, 7, 7, 512)     0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_19 (Conv3D)          (None, 3, 7, 7, 512)      7078400   \n",
      "                                                                 \n",
      " conv3d_20 (Conv3D)          (None, 3, 7, 7, 512)      7078400   \n",
      "                                                                 \n",
      " max_pooling3d_7 (MaxPooling  (None, 1, 3, 3, 512)     0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 4608)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4096)              18878464  \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 20485     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 63,336,197\n",
      "Trainable params: 63,336,197\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Compiling the model using Adam as optimizer\n",
    "model_C3D.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_C3D.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________\n",
      "Batch Size is : 8\n",
      "\n",
      "Source path =  datasets/Project_data/train ; batch size = 8\n",
      "Epoch 1/20\n",
      "29/83 [=========>....................] - ETA: 1:07 - loss: 35.3006 - categorical_accuracy: 0.1810Reducing Batch Size\n",
      "_________________\n",
      "Batch Size is : 4\n",
      "\n",
      "Source path =  datasets/Project_data/train ; batch size = 4\n",
      "Epoch 1/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.7855 - categorical_accuracy: 0.1991\n",
      "Source path =  datasets/Project_data/val ; batch size = 4\n",
      "\n",
      "Epoch 1: saving model to model_init_2023-02-0720_24_17.938535\\model-00001-1.78549-0.19910-1.60910-0.18000.h5\n",
      "166/166 [==============================] - 130s 768ms/step - loss: 1.7855 - categorical_accuracy: 0.1991 - val_loss: 1.6091 - val_categorical_accuracy: 0.1800 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.6102 - categorical_accuracy: 0.1961\n",
      "Epoch 2: saving model to model_init_2023-02-0720_24_17.938535\\model-00002-1.61023-0.19608-1.60759-0.21000.h5\n",
      "166/166 [==============================] - 126s 761ms/step - loss: 1.6102 - categorical_accuracy: 0.1961 - val_loss: 1.6076 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.6097 - categorical_accuracy: 0.1810\n",
      "Epoch 3: saving model to model_init_2023-02-0720_24_17.938535\\model-00003-1.60971-0.18100-1.60817-0.22000.h5\n",
      "166/166 [==============================] - 126s 763ms/step - loss: 1.6097 - categorical_accuracy: 0.1810 - val_loss: 1.6082 - val_categorical_accuracy: 0.2200 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.6096 - categorical_accuracy: 0.1735\n",
      "Epoch 4: saving model to model_init_2023-02-0720_24_17.938535\\model-00004-1.60965-0.17345-1.60692-0.24000.h5\n",
      "166/166 [==============================] - 125s 758ms/step - loss: 1.6096 - categorical_accuracy: 0.1735 - val_loss: 1.6069 - val_categorical_accuracy: 0.2400 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.6093 - categorical_accuracy: 0.1991\n",
      "Epoch 5: saving model to model_init_2023-02-0720_24_17.938535\\model-00005-1.60929-0.19910-1.60732-0.23000.h5\n",
      "166/166 [==============================] - 125s 755ms/step - loss: 1.6093 - categorical_accuracy: 0.1991 - val_loss: 1.6073 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.6095 - categorical_accuracy: 0.1931\n",
      "Epoch 6: saving model to model_init_2023-02-0720_24_17.938535\\model-00006-1.60948-0.19306-1.60931-0.21000.h5\n",
      "166/166 [==============================] - 126s 762ms/step - loss: 1.6095 - categorical_accuracy: 0.1931 - val_loss: 1.6093 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.6096 - categorical_accuracy: 0.2021\n",
      "Epoch 7: saving model to model_init_2023-02-0720_24_17.938535\\model-00007-1.60958-0.20211-1.60389-0.20000.h5\n",
      "166/166 [==============================] - 125s 756ms/step - loss: 1.6096 - categorical_accuracy: 0.2021 - val_loss: 1.6039 - val_categorical_accuracy: 0.2000 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.6094 - categorical_accuracy: 0.2066\n",
      "Epoch 8: saving model to model_init_2023-02-0720_24_17.938535\\model-00008-1.60936-0.20664-1.60876-0.21000.h5\n",
      "166/166 [==============================] - 125s 753ms/step - loss: 1.6094 - categorical_accuracy: 0.2066 - val_loss: 1.6088 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.6098 - categorical_accuracy: 0.1961\n",
      "Epoch 9: saving model to model_init_2023-02-0720_24_17.938535\\model-00009-1.60982-0.19608-1.61203-0.18000.h5\n",
      "166/166 [==============================] - 126s 760ms/step - loss: 1.6098 - categorical_accuracy: 0.1961 - val_loss: 1.6120 - val_categorical_accuracy: 0.1800 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.6098 - categorical_accuracy: 0.1810\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 10: saving model to model_init_2023-02-0720_24_17.938535\\model-00010-1.60980-0.18100-1.60586-0.22000.h5\n",
      "166/166 [==============================] - 124s 752ms/step - loss: 1.6098 - categorical_accuracy: 0.1810 - val_loss: 1.6059 - val_categorical_accuracy: 0.2200 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.6091 - categorical_accuracy: 0.1855\n",
      "Epoch 11: saving model to model_init_2023-02-0720_24_17.938535\\model-00011-1.60913-0.18552-1.60717-0.23000.h5\n",
      "166/166 [==============================] - 125s 756ms/step - loss: 1.6091 - categorical_accuracy: 0.1855 - val_loss: 1.6072 - val_categorical_accuracy: 0.2300 - lr: 5.0000e-04\n",
      "Epoch 12/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.6091 - categorical_accuracy: 0.1780\n",
      "Epoch 12: saving model to model_init_2023-02-0720_24_17.938535\\model-00012-1.60912-0.17798-1.60871-0.21000.h5\n",
      "166/166 [==============================] - 126s 761ms/step - loss: 1.6091 - categorical_accuracy: 0.1780 - val_loss: 1.6087 - val_categorical_accuracy: 0.2100 - lr: 5.0000e-04\n",
      "Epoch 13/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.6091 - categorical_accuracy: 0.1750\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 13: saving model to model_init_2023-02-0720_24_17.938535\\model-00013-1.60908-0.17496-1.60748-0.16000.h5\n",
      "166/166 [==============================] - 125s 754ms/step - loss: 1.6091 - categorical_accuracy: 0.1750 - val_loss: 1.6075 - val_categorical_accuracy: 0.1600 - lr: 5.0000e-04\n",
      "Epoch 14/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.6087 - categorical_accuracy: 0.1916\n",
      "Epoch 14: saving model to model_init_2023-02-0720_24_17.938535\\model-00014-1.60875-0.19155-1.60545-0.27000.h5\n",
      "166/166 [==============================] - 126s 760ms/step - loss: 1.6087 - categorical_accuracy: 0.1916 - val_loss: 1.6055 - val_categorical_accuracy: 0.2700 - lr: 2.5000e-04\n",
      "Epoch 15/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.6089 - categorical_accuracy: 0.1870\n",
      "Epoch 15: saving model to model_init_2023-02-0720_24_17.938535\\model-00015-1.60886-0.18703-1.60639-0.14000.h5\n",
      "166/166 [==============================] - 126s 760ms/step - loss: 1.6089 - categorical_accuracy: 0.1870 - val_loss: 1.6064 - val_categorical_accuracy: 0.1400 - lr: 2.5000e-04\n",
      "Epoch 16/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.6087 - categorical_accuracy: 0.1976\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 16: saving model to model_init_2023-02-0720_24_17.938535\\model-00016-1.60870-0.19759-1.60912-0.22000.h5\n",
      "166/166 [==============================] - 125s 755ms/step - loss: 1.6087 - categorical_accuracy: 0.1976 - val_loss: 1.6091 - val_categorical_accuracy: 0.2200 - lr: 2.5000e-04\n",
      "Epoch 17/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.6087 - categorical_accuracy: 0.1976\n",
      "Epoch 17: saving model to model_init_2023-02-0720_24_17.938535\\model-00017-1.60871-0.19759-1.61141-0.23000.h5\n",
      "166/166 [==============================] - 125s 758ms/step - loss: 1.6087 - categorical_accuracy: 0.1976 - val_loss: 1.6114 - val_categorical_accuracy: 0.2300 - lr: 1.2500e-04\n",
      "Epoch 18/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.6085 - categorical_accuracy: 0.1976\n",
      "Epoch 18: saving model to model_init_2023-02-0720_24_17.938535\\model-00018-1.60850-0.19759-1.60549-0.17000.h5\n",
      "166/166 [==============================] - 125s 755ms/step - loss: 1.6085 - categorical_accuracy: 0.1976 - val_loss: 1.6055 - val_categorical_accuracy: 0.1700 - lr: 1.2500e-04\n",
      "Epoch 19/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.6085 - categorical_accuracy: 0.2006\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 19: saving model to model_init_2023-02-0720_24_17.938535\\model-00019-1.60850-0.20060-1.60593-0.22000.h5\n",
      "166/166 [==============================] - 125s 758ms/step - loss: 1.6085 - categorical_accuracy: 0.2006 - val_loss: 1.6059 - val_categorical_accuracy: 0.2200 - lr: 1.2500e-04\n",
      "Epoch 20/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.6085 - categorical_accuracy: 0.2081\n",
      "Epoch 20: saving model to model_init_2023-02-0720_24_17.938535\\model-00020-1.60847-0.20814-1.60771-0.22000.h5\n",
      "166/166 [==============================] - 125s 758ms/step - loss: 1.6085 - categorical_accuracy: 0.2081 - val_loss: 1.6077 - val_categorical_accuracy: 0.2200 - lr: 6.2500e-05\n",
      "Time Taken to Train this model was :  0:41:51.959033\n"
     ]
    }
   ],
   "source": [
    "batch_size=8\n",
    "trainmodel(model_C3D,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_C3D.save(\"model_C3D.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4 - (2 + 1)D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model based on the paper \"A Closer Look at Spatiotemporal Convolutions for Action Recognition\"\n",
    "# https://arxiv.org/pdf/1711.11248v3.pdf\n",
    "# This model used (2+1)D with residual connections. \n",
    "#The steps used are as described in the Keras documentation and video classification tutorial project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras Functional API's will be used to make the model because the model is \"Not Sequential\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating classes that will be called by Keras functional APIs for custom layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class returns a layers object that is inherited from keras.layers.layers class\n",
    "# and consists of 2 layers that use the supplied kernel size to create the layer object\n",
    "# Variable x is the custom layer object\n",
    "\n",
    "class Conv2Plus1D(keras.layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, padding):\n",
    "        super().__init__()\n",
    "        self.seq = keras.Sequential([layers.Conv3D(filters=filters,kernel_size=(1, kernel_size[1], kernel_size[2]),padding=padding),\n",
    "                                 layers.Conv3D(filters=filters, kernel_size=(kernel_size[0], 1, 1),padding=padding)])\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        return cfg \n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.seq(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block of code consists of 2 classes and 1 function that are used as described in \n",
    "# Keras functional API documentation and tutorial on video classification on tensorflow.org\n",
    "\n",
    "# Class to add residual connection to the model\n",
    "# This class is called by the add_residual_block function which is called during model architecture\n",
    "# building. This class returns the layer object to add_residual_block function which checks to see\n",
    "# if the returned object shape matches the input based on filter size and project the dimension \n",
    "# if it doesn't\n",
    "\n",
    "class ResidualMain(keras.layers.Layer):\n",
    "    def __init__(self, filters, kernel_size):\n",
    "        super().__init__()\n",
    "        self.seq = keras.Sequential([Conv2Plus1D(filters=filters,kernel_size=kernel_size,padding='same'),\n",
    "            layers.LayerNormalization(),\n",
    "            layers.ReLU(),\n",
    "            Conv2Plus1D(filters=filters,kernel_size=kernel_size,padding='same'),\n",
    "            layers.LayerNormalization()\n",
    "        ])\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        return cfg\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.seq(x)\n",
    "\n",
    "# Class called by add_residual_block to project the dimension of the tensor to match the kernal size\n",
    "# See explanation above\n",
    "class Project(keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.seq = keras.Sequential([layers.Dense(units),layers.LayerNormalization()])\n",
    "        \n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        return cfg\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.seq(x)\n",
    "\n",
    "# Function called during model architecture build to add residual layers\n",
    "# See explanation under Class ResidualMain\n",
    "\n",
    "def add_residual_block(input, filters, kernel_size):\n",
    "    out = ResidualMain(filters, kernel_size)(input)\n",
    "    res = input\n",
    "    if out.shape[-1] != input.shape[-1]:\n",
    "        res = Project(out.shape[-1])(res)\n",
    "    return layers.add([res, out])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case its not installed\n",
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "# More details on the einops.rearrange here : https://einops.rocks/api/rearrange/ \n",
    "# https://einops.rocks/1-einops-basics/ -- Tutorial\n",
    "# Class to resize video \n",
    "# The video is resized to half after every block\n",
    "# We will use einops to help us retreive the array without time (frames)  and batch size\n",
    "# so it can be resized\n",
    "\n",
    "\n",
    "class ResizeVideo(keras.layers.Layer):\n",
    "    def __init__(self, height, width):\n",
    "        super().__init__()\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.resizing_layer = layers.Resizing(self.height, self.width)\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        return cfg\n",
    "        \n",
    "    def call(self, video):\n",
    "\n",
    "        # b stands for batch size, t stands for time, h stands for height, \n",
    "        # w stands for width, and c stands for the number of channels.\n",
    "        old_shape = einops.parse_shape(video, 'b t h w c')\n",
    "        images = einops.rearrange(video, 'b t h w c -> (b t) h w c')\n",
    "        images = self.resizing_layer(images)\n",
    "        videos = einops.rearrange(\n",
    "            images, '(b t) h w c -> b t h w c',\n",
    "            t = old_shape['t'])\n",
    "        return videos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Architecture for 2(D+1)\n",
    "\n",
    "New_input_shape = (None, len(img_idx), imgdim, imgdim, 3)\n",
    "input = layers.Input(shape=(New_input_shape[1:]))\n",
    "x = input\n",
    "\n",
    "x = Conv2Plus1D(filters=16, kernel_size=(3, 7, 7), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = ResizeVideo(imgdim // 2, imgdim // 2)(x)\n",
    "\n",
    "# Block 1\n",
    "x = add_residual_block(x, 16, (3, 3, 3))\n",
    "x = ResizeVideo(imgdim // 4, imgdim // 4)(x)\n",
    "\n",
    "# Block 2\n",
    "x = add_residual_block(x, 32, (3, 3, 3))\n",
    "x = ResizeVideo(imgdim // 8, imgdim // 8)(x)\n",
    "\n",
    "# Block 2\n",
    "x = add_residual_block(x, 64, (3, 3, 3))\n",
    "x = ResizeVideo(imgdim // 16, imgdim // 16)(x)\n",
    "\n",
    "# Block 3\n",
    "x = add_residual_block(x, 128, (3, 3, 3))\n",
    "\n",
    "x = layers.GlobalAveragePooling3D()(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(5)(x)\n",
    "\n",
    "model_2Dplus1 = keras.Model(input, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Because the model was build using Keras functional API, we will need to call build function\n",
    "## on a batch of data before this model can be compiled \n",
    "frames, label = next(train_generator)\n",
    "model_2Dplus1.build(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 30, 120, 12  0           []                               \n",
      "                                0, 3)]                                                            \n",
      "                                                                                                  \n",
      " conv2_plus1d (Conv2Plus1D)     (None, 30, 120, 120  3152        ['input_1[0][0]']                \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 30, 120, 120  64         ['conv2_plus1d[0][0]']           \n",
      " rmalization)                   , 16)                                                             \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)                 (None, 30, 120, 120  0           ['batch_normalization_4[0][0]']  \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " resize_video (ResizeVideo)     (None, 30, 60, 60,   0           ['re_lu_3[0][0]']                \n",
      "                                16)                                                               \n",
      "                                                                                                  \n",
      " residual_main (ResidualMain)   (None, 30, 60, 60,   6272        ['resize_video[0][0]']           \n",
      "                                16)                                                               \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 30, 60, 60,   0           ['resize_video[0][0]',           \n",
      "                                16)                               'residual_main[0][0]']          \n",
      "                                                                                                  \n",
      " resize_video_1 (ResizeVideo)   (None, 30, 30, 30,   0           ['add[0][0]']                    \n",
      "                                16)                                                               \n",
      "                                                                                                  \n",
      " project (Project)              (None, 30, 30, 30,   608         ['resize_video_1[0][0]']         \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " residual_main_1 (ResidualMain)  (None, 30, 30, 30,   20224      ['resize_video_1[0][0]']         \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 30, 30, 30,   0           ['project[0][0]',                \n",
      "                                32)                               'residual_main_1[0][0]']        \n",
      "                                                                                                  \n",
      " resize_video_2 (ResizeVideo)   (None, 30, 15, 15,   0           ['add_1[0][0]']                  \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " project_1 (Project)            (None, 30, 15, 15,   2240        ['resize_video_2[0][0]']         \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " residual_main_2 (ResidualMain)  (None, 30, 15, 15,   80384      ['resize_video_2[0][0]']         \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 30, 15, 15,   0           ['project_1[0][0]',              \n",
      "                                64)                               'residual_main_2[0][0]']        \n",
      "                                                                                                  \n",
      " resize_video_3 (ResizeVideo)   (None, 30, 7, 7, 64  0           ['add_2[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " project_2 (Project)            (None, 30, 7, 7, 12  8576        ['resize_video_3[0][0]']         \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " residual_main_3 (ResidualMain)  (None, 30, 7, 7, 12  320512     ['resize_video_3[0][0]']         \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 30, 7, 7, 12  0           ['project_2[0][0]',              \n",
      "                                8)                                'residual_main_3[0][0]']        \n",
      "                                                                                                  \n",
      " global_average_pooling3d (Glob  (None, 128)         0           ['add_3[0][0]']                  \n",
      " alAveragePooling3D)                                                                              \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 128)          0           ['global_average_pooling3d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 5)            645         ['flatten_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 442,677\n",
      "Trainable params: 442,645\n",
      "Non-trainable params: 32\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Compiling the model using Adam as optimizer\n",
    "model_2Dplus1.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_2Dplus1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________\n",
      "Batch Size is : 8\n",
      "\n",
      "Source path =  datasets/Project_data/train ; batch size = 8\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 7.6213 - categorical_accuracy: 0.2293\n",
      "Source path =  datasets/Project_data/val ; batch size = 8\n",
      "\n",
      "Epoch 1: saving model to model_init_2023-02-0720_24_17.938535\\model-00001-7.62127-0.22926-5.48015-0.25000.h5\n",
      "83/83 [==============================] - 127s 2s/step - loss: 7.6213 - categorical_accuracy: 0.2293 - val_loss: 5.4802 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 6.2722 - categorical_accuracy: 0.2066\n",
      "Epoch 2: saving model to model_init_2023-02-0720_24_17.938535\\model-00002-6.27220-0.20664-6.12488-0.22000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 6.2722 - categorical_accuracy: 0.2066 - val_loss: 6.1249 - val_categorical_accuracy: 0.2200 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 6.2236 - categorical_accuracy: 0.2051\n",
      "Epoch 3: saving model to model_init_2023-02-0720_24_17.938535\\model-00003-6.22358-0.20513-5.80251-0.24000.h5\n",
      "83/83 [==============================] - 123s 1s/step - loss: 6.2236 - categorical_accuracy: 0.2051 - val_loss: 5.8025 - val_categorical_accuracy: 0.2400 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 6.3208 - categorical_accuracy: 0.2051\n",
      "Epoch 4: saving model to model_init_2023-02-0720_24_17.938535\\model-00004-6.32082-0.20513-5.15779-0.21000.h5\n",
      "83/83 [==============================] - 123s 1s/step - loss: 6.3208 - categorical_accuracy: 0.2051 - val_loss: 5.1578 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 6.3451 - categorical_accuracy: 0.2066\n",
      "Epoch 5: saving model to model_init_2023-02-0720_24_17.938535\\model-00005-6.34513-0.20664-4.51307-0.25000.h5\n",
      "83/83 [==============================] - 123s 1s/step - loss: 6.3451 - categorical_accuracy: 0.2066 - val_loss: 4.5131 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 6.3694 - categorical_accuracy: 0.2051\n",
      "Epoch 6: saving model to model_init_2023-02-0720_24_17.938535\\model-00006-6.36944-0.20513-5.31897-0.27000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 6.3694 - categorical_accuracy: 0.2051 - val_loss: 5.3190 - val_categorical_accuracy: 0.2700 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 6.2236 - categorical_accuracy: 0.2051\n",
      "Epoch 7: saving model to model_init_2023-02-0720_24_17.938535\\model-00007-6.22358-0.20513-5.80251-0.22000.h5\n",
      "83/83 [==============================] - 123s 2s/step - loss: 6.2236 - categorical_accuracy: 0.2051 - val_loss: 5.8025 - val_categorical_accuracy: 0.2200 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 6.3208 - categorical_accuracy: 0.2066\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 8: saving model to model_init_2023-02-0720_24_17.938535\\model-00008-6.32082-0.20664-4.51307-0.21000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 6.3208 - categorical_accuracy: 0.2066 - val_loss: 4.5131 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 6.2722 - categorical_accuracy: 0.2081\n",
      "Epoch 9: saving model to model_init_2023-02-0720_24_17.938535\\model-00009-6.27220-0.20814-5.96370-0.24000.h5\n",
      "83/83 [==============================] - 123s 2s/step - loss: 6.2722 - categorical_accuracy: 0.2081 - val_loss: 5.9637 - val_categorical_accuracy: 0.2400 - lr: 5.0000e-04\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 6.2965 - categorical_accuracy: 0.2051\n",
      "Epoch 10: saving model to model_init_2023-02-0720_24_17.938535\\model-00010-6.29651-0.20513-5.48015-0.23000.h5\n",
      "83/83 [==============================] - 123s 2s/step - loss: 6.2965 - categorical_accuracy: 0.2051 - val_loss: 5.4802 - val_categorical_accuracy: 0.2300 - lr: 5.0000e-04\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 6.3938 - categorical_accuracy: 0.2036\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 11: saving model to model_init_2023-02-0720_24_17.938535\\model-00011-6.39375-0.20362-5.80251-0.24000.h5\n",
      "83/83 [==============================] - 123s 1s/step - loss: 6.3938 - categorical_accuracy: 0.2036 - val_loss: 5.8025 - val_categorical_accuracy: 0.2400 - lr: 5.0000e-04\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 6.3451 - categorical_accuracy: 0.2036\n",
      "Epoch 12: saving model to model_init_2023-02-0720_24_17.938535\\model-00012-6.34513-0.20362-5.64133-0.23000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 6.3451 - categorical_accuracy: 0.2036 - val_loss: 5.6413 - val_categorical_accuracy: 0.2300 - lr: 2.5000e-04\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 6.2236 - categorical_accuracy: 0.2066\n",
      "Epoch 13: saving model to model_init_2023-02-0720_24_17.938535\\model-00013-6.22358-0.20664-5.48015-0.20000.h5\n",
      "83/83 [==============================] - 123s 1s/step - loss: 6.2236 - categorical_accuracy: 0.2066 - val_loss: 5.4802 - val_categorical_accuracy: 0.2000 - lr: 2.5000e-04\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 6.3694 - categorical_accuracy: 0.2051\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 14: saving model to model_init_2023-02-0720_24_17.938535\\model-00014-6.36944-0.20513-5.64133-0.23000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 6.3694 - categorical_accuracy: 0.2051 - val_loss: 5.6413 - val_categorical_accuracy: 0.2300 - lr: 2.5000e-04\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 6.3208 - categorical_accuracy: 0.2066\n",
      "Epoch 15: saving model to model_init_2023-02-0720_24_17.938535\\model-00015-6.32082-0.20664-4.67425-0.25000.h5\n",
      "83/83 [==============================] - 123s 2s/step - loss: 6.3208 - categorical_accuracy: 0.2066 - val_loss: 4.6742 - val_categorical_accuracy: 0.2500 - lr: 1.2500e-04\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 6.3208 - categorical_accuracy: 0.2066\n",
      "Epoch 16: saving model to model_init_2023-02-0720_24_17.938535\\model-00016-6.32082-0.20664-6.60842-0.20000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 6.3208 - categorical_accuracy: 0.2066 - val_loss: 6.6084 - val_categorical_accuracy: 0.2000 - lr: 1.2500e-04\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 6.3451 - categorical_accuracy: 0.2006\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 17: saving model to model_init_2023-02-0720_24_17.938535\\model-00017-6.34513-0.20060-5.48015-0.21000.h5\n",
      "83/83 [==============================] - 123s 1s/step - loss: 6.3451 - categorical_accuracy: 0.2006 - val_loss: 5.4802 - val_categorical_accuracy: 0.2100 - lr: 1.2500e-04\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 6.3208 - categorical_accuracy: 0.2097\n",
      "Epoch 18: saving model to model_init_2023-02-0720_24_17.938535\\model-00018-6.32082-0.20965-5.15779-0.23000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 6.3208 - categorical_accuracy: 0.2097 - val_loss: 5.1578 - val_categorical_accuracy: 0.2300 - lr: 6.2500e-05\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 6.3451 - categorical_accuracy: 0.2021\n",
      "Epoch 19: saving model to model_init_2023-02-0720_24_17.938535\\model-00019-6.34513-0.20211-4.67425-0.25000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 6.3451 - categorical_accuracy: 0.2021 - val_loss: 4.6742 - val_categorical_accuracy: 0.2500 - lr: 6.2500e-05\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 6.2965 - categorical_accuracy: 0.2081\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 20: saving model to model_init_2023-02-0720_24_17.938535\\model-00020-6.29651-0.20814-5.31897-0.24000.h5\n",
      "83/83 [==============================] - 123s 1s/step - loss: 6.2965 - categorical_accuracy: 0.2081 - val_loss: 5.3190 - val_categorical_accuracy: 0.2400 - lr: 6.2500e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken to Train this model was :  0:41:14.985149\n"
     ]
    }
   ],
   "source": [
    "batch_size=8\n",
    "trainmodel(model_2Dplus1,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2Dplus1.save(\"model_2Dplus1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5:  CNN with LSTM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use the COV2D layers with time distribution and pass it to the LSTM Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture for model_cnn_LSTM\n",
    "Input_shape = (len(img_idx), imgdim, imgdim, 3)\n",
    "\n",
    "model_cnn_LSTM = Sequential()\n",
    "\n",
    "model_cnn_LSTM.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'),input_shape=Input_shape))\n",
    "model_cnn_LSTM.add(TimeDistributed(BatchNormalization()))\n",
    "model_cnn_LSTM.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model_cnn_LSTM.add(Dropout(0.25))\n",
    "\n",
    "        \n",
    "model_cnn_LSTM.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n",
    "model_cnn_LSTM.add(TimeDistributed(BatchNormalization()))\n",
    "model_cnn_LSTM.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model_cnn_LSTM.add(Dropout(0.25))\n",
    "        \n",
    "model_cnn_LSTM.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\n",
    "model_cnn_LSTM.add(TimeDistributed(BatchNormalization()))\n",
    "model_cnn_LSTM.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model_cnn_LSTM.add(Dropout(0.25))\n",
    "        \n",
    "model_cnn_LSTM.add(TimeDistributed(Conv2D(128, (3, 3) , padding='same', activation='relu')))\n",
    "model_cnn_LSTM.add(TimeDistributed(BatchNormalization()))\n",
    "model_cnn_LSTM.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model_cnn_LSTM.add(Dropout(0.25))\n",
    "          \n",
    "\n",
    "model_cnn_LSTM.add(TimeDistributed(Flatten()))\n",
    "model_cnn_LSTM.add(Dropout(0.25))\n",
    "\n",
    "model_cnn_LSTM.add(LSTM(1024))\n",
    "model_cnn_LSTM.add(Dropout(0.5))\n",
    "\n",
    "model_cnn_LSTM.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T06:20:07.757837Z",
     "iopub.status.busy": "2021-10-26T06:20:07.757588Z",
     "iopub.status.idle": "2021-10-26T06:20:07.929675Z",
     "shell.execute_reply": "2021-10-26T06:20:07.929025Z",
     "shell.execute_reply.started": "2021-10-26T06:20:07.757808Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed (TimeDistr  (None, 30, 120, 120, 16)  448      \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 30, 120, 120, 16)  64       \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDis  (None, 30, 60, 60, 16)   0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 30, 60, 60, 16)    0         \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDis  (None, 30, 60, 60, 32)   4640      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_4 (TimeDis  (None, 30, 60, 60, 32)   128       \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_5 (TimeDis  (None, 30, 30, 30, 32)   0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 30, 30, 30, 32)    0         \n",
      "                                                                 \n",
      " time_distributed_6 (TimeDis  (None, 30, 30, 30, 64)   18496     \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_7 (TimeDis  (None, 30, 30, 30, 64)   256       \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_8 (TimeDis  (None, 30, 15, 15, 64)   0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 30, 15, 15, 64)    0         \n",
      "                                                                 \n",
      " time_distributed_9 (TimeDis  (None, 30, 15, 15, 128)  73856     \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_10 (TimeDi  (None, 30, 15, 15, 128)  512       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_11 (TimeDi  (None, 30, 7, 7, 128)    0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 30, 7, 7, 128)     0         \n",
      "                                                                 \n",
      " time_distributed_12 (TimeDi  (None, 30, 6272)         0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 30, 6272)          0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 1024)              29888512  \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 5)                 5125      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29,992,037\n",
      "Trainable params: 29,991,557\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Compiling the model using Adam as optimizer\n",
    "model_cnn_LSTM.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_cnn_LSTM.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________\n",
      "Batch Size is : 8\n",
      "\n",
      "Source path =  datasets/Project_data/train ; batch size = 8\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6949 - categorical_accuracy: 0.3258\n",
      "Source path =  datasets/Project_data/val ; batch size = 8\n",
      "\n",
      "Epoch 1: saving model to model_init_2023-02-0720_24_17.938535\\model-00001-1.69489-0.32579-3.00417-0.19000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 1.6949 - categorical_accuracy: 0.3258 - val_loss: 3.0042 - val_categorical_accuracy: 0.1900 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.2190 - categorical_accuracy: 0.5098\n",
      "Epoch 2: saving model to model_init_2023-02-0720_24_17.938535\\model-00002-1.21895-0.50980-2.03727-0.15000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 1.2190 - categorical_accuracy: 0.5098 - val_loss: 2.0373 - val_categorical_accuracy: 0.1500 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.9874 - categorical_accuracy: 0.6244\n",
      "Epoch 3: saving model to model_init_2023-02-0720_24_17.938535\\model-00003-0.98741-0.62443-3.18122-0.22000.h5\n",
      "83/83 [==============================] - 122s 1s/step - loss: 0.9874 - categorical_accuracy: 0.6244 - val_loss: 3.1812 - val_categorical_accuracy: 0.2200 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.9217 - categorical_accuracy: 0.6576\n",
      "Epoch 4: saving model to model_init_2023-02-0720_24_17.938535\\model-00004-0.92174-0.65762-2.76008-0.20000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 0.9217 - categorical_accuracy: 0.6576 - val_loss: 2.7601 - val_categorical_accuracy: 0.2000 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.8074 - categorical_accuracy: 0.7149\n",
      "Epoch 5: saving model to model_init_2023-02-0720_24_17.938535\\model-00005-0.80738-0.71493-1.60138-0.47000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 0.8074 - categorical_accuracy: 0.7149 - val_loss: 1.6014 - val_categorical_accuracy: 0.4700 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.6921 - categorical_accuracy: 0.7481\n",
      "Epoch 6: saving model to model_init_2023-02-0720_24_17.938535\\model-00006-0.69211-0.74811-2.28760-0.35000.h5\n",
      "83/83 [==============================] - 123s 2s/step - loss: 0.6921 - categorical_accuracy: 0.7481 - val_loss: 2.2876 - val_categorical_accuracy: 0.3500 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.5926 - categorical_accuracy: 0.7858\n",
      "Epoch 7: saving model to model_init_2023-02-0720_24_17.938535\\model-00007-0.59257-0.78582-1.12635-0.58000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 0.5926 - categorical_accuracy: 0.7858 - val_loss: 1.1264 - val_categorical_accuracy: 0.5800 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.4778 - categorical_accuracy: 0.8220\n",
      "Epoch 8: saving model to model_init_2023-02-0720_24_17.938535\\model-00008-0.47782-0.82202-0.82288-0.71000.h5\n",
      "83/83 [==============================] - 123s 2s/step - loss: 0.4778 - categorical_accuracy: 0.8220 - val_loss: 0.8229 - val_categorical_accuracy: 0.7100 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.4527 - categorical_accuracy: 0.8356\n",
      "Epoch 9: saving model to model_init_2023-02-0720_24_17.938535\\model-00009-0.45270-0.83560-0.51874-0.79000.h5\n",
      "83/83 [==============================] - 122s 1s/step - loss: 0.4527 - categorical_accuracy: 0.8356 - val_loss: 0.5187 - val_categorical_accuracy: 0.7900 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.4291 - categorical_accuracy: 0.8386\n",
      "Epoch 10: saving model to model_init_2023-02-0720_24_17.938535\\model-00010-0.42909-0.83861-0.71565-0.76000.h5\n",
      "83/83 [==============================] - 122s 1s/step - loss: 0.4291 - categorical_accuracy: 0.8386 - val_loss: 0.7157 - val_categorical_accuracy: 0.7600 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.3307 - categorical_accuracy: 0.8839\n",
      "Epoch 11: saving model to model_init_2023-02-0720_24_17.938535\\model-00011-0.33072-0.88386-0.71352-0.80000.h5\n",
      "83/83 [==============================] - 123s 1s/step - loss: 0.3307 - categorical_accuracy: 0.8839 - val_loss: 0.7135 - val_categorical_accuracy: 0.8000 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.2876 - categorical_accuracy: 0.8974\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 12: saving model to model_init_2023-02-0720_24_17.938535\\model-00012-0.28762-0.89744-0.71860-0.79000.h5\n",
      "83/83 [==============================] - 122s 1s/step - loss: 0.2876 - categorical_accuracy: 0.8974 - val_loss: 0.7186 - val_categorical_accuracy: 0.7900 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.2129 - categorical_accuracy: 0.9321\n",
      "Epoch 13: saving model to model_init_2023-02-0720_24_17.938535\\model-00013-0.21290-0.93213-0.58545-0.84000.h5\n",
      "83/83 [==============================] - 122s 1s/step - loss: 0.2129 - categorical_accuracy: 0.9321 - val_loss: 0.5855 - val_categorical_accuracy: 0.8400 - lr: 5.0000e-04\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.1668 - categorical_accuracy: 0.9487\n",
      "Epoch 14: saving model to model_init_2023-02-0720_24_17.938535\\model-00014-0.16683-0.94872-0.57690-0.81000.h5\n",
      "83/83 [==============================] - 123s 1s/step - loss: 0.1668 - categorical_accuracy: 0.9487 - val_loss: 0.5769 - val_categorical_accuracy: 0.8100 - lr: 5.0000e-04\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.1385 - categorical_accuracy: 0.9563\n",
      "Epoch 15: saving model to model_init_2023-02-0720_24_17.938535\\model-00015-0.13845-0.95626-0.38100-0.87000.h5\n",
      "83/83 [==============================] - 123s 1s/step - loss: 0.1385 - categorical_accuracy: 0.9563 - val_loss: 0.3810 - val_categorical_accuracy: 0.8700 - lr: 5.0000e-04\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.1017 - categorical_accuracy: 0.9744\n",
      "Epoch 16: saving model to model_init_2023-02-0720_24_17.938535\\model-00016-0.10166-0.97436-0.51985-0.82000.h5\n",
      "83/83 [==============================] - 122s 1s/step - loss: 0.1017 - categorical_accuracy: 0.9744 - val_loss: 0.5199 - val_categorical_accuracy: 0.8200 - lr: 5.0000e-04\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.0929 - categorical_accuracy: 0.9759\n",
      "Epoch 17: saving model to model_init_2023-02-0720_24_17.938535\\model-00017-0.09288-0.97587-0.72723-0.80000.h5\n",
      "83/83 [==============================] - 123s 1s/step - loss: 0.0929 - categorical_accuracy: 0.9759 - val_loss: 0.7272 - val_categorical_accuracy: 0.8000 - lr: 5.0000e-04\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.0752 - categorical_accuracy: 0.9819\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 18: saving model to model_init_2023-02-0720_24_17.938535\\model-00018-0.07516-0.98190-0.62478-0.83000.h5\n",
      "83/83 [==============================] - 123s 2s/step - loss: 0.0752 - categorical_accuracy: 0.9819 - val_loss: 0.6248 - val_categorical_accuracy: 0.8300 - lr: 5.0000e-04\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.0659 - categorical_accuracy: 0.9834\n",
      "Epoch 19: saving model to model_init_2023-02-0720_24_17.938535\\model-00019-0.06585-0.98341-0.36831-0.84000.h5\n",
      "83/83 [==============================] - 123s 1s/step - loss: 0.0659 - categorical_accuracy: 0.9834 - val_loss: 0.3683 - val_categorical_accuracy: 0.8400 - lr: 2.5000e-04\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.0499 - categorical_accuracy: 0.9894\n",
      "Epoch 20: saving model to model_init_2023-02-0720_24_17.938535\\model-00020-0.04987-0.98944-0.33419-0.82000.h5\n",
      "83/83 [==============================] - 123s 1s/step - loss: 0.0499 - categorical_accuracy: 0.9894 - val_loss: 0.3342 - val_categorical_accuracy: 0.8200 - lr: 2.5000e-04\n",
      "Time Taken to Train this model was :  0:41:03.600928\n"
     ]
    }
   ],
   "source": [
    "batch_size=8\n",
    "trainmodel(model_cnn_LSTM,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_LSTM.save(\"model_cnn_LSTM.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 6: CNN with GRU\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use the COV2D layers with time distribution and pass it to the GRU Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture for model_cnn_GRU\n",
    "Input_shape = (len(img_idx), imgdim, imgdim, 3)\n",
    "\n",
    "model_cnn_GRU = Sequential()\n",
    "\n",
    "model_cnn_GRU.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'),input_shape=Input_shape))\n",
    "model_cnn_GRU.add(TimeDistributed(BatchNormalization()))\n",
    "model_cnn_GRU.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model_cnn_GRU.add(Dropout(0.25))\n",
    "\n",
    "        \n",
    "model_cnn_GRU.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n",
    "model_cnn_GRU.add(TimeDistributed(BatchNormalization()))\n",
    "model_cnn_GRU.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model_cnn_GRU.add(Dropout(0.25))\n",
    "        \n",
    "model_cnn_GRU.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\n",
    "model_cnn_GRU.add(TimeDistributed(BatchNormalization()))\n",
    "model_cnn_GRU.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model_cnn_GRU.add(Dropout(0.25))\n",
    "        \n",
    "model_cnn_GRU.add(TimeDistributed(Conv2D(128, (3, 3) , padding='same', activation='relu')))\n",
    "model_cnn_GRU.add(TimeDistributed(BatchNormalization()))\n",
    "model_cnn_GRU.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model_cnn_GRU.add(Dropout(0.25))\n",
    "          \n",
    "\n",
    "model_cnn_GRU.add(TimeDistributed(Flatten()))\n",
    "model_cnn_GRU.add(Dropout(0.25))\n",
    "\n",
    "model_cnn_GRU.add(GRU(1024))\n",
    "model_cnn_GRU.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model_cnn_GRU.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T06:20:07.757837Z",
     "iopub.status.busy": "2021-10-26T06:20:07.757588Z",
     "iopub.status.idle": "2021-10-26T06:20:07.929675Z",
     "shell.execute_reply": "2021-10-26T06:20:07.929025Z",
     "shell.execute_reply.started": "2021-10-26T06:20:07.757808Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_13 (TimeDi  (None, 30, 120, 120, 16)  448      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_14 (TimeDi  (None, 30, 120, 120, 16)  64       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_15 (TimeDi  (None, 30, 60, 60, 16)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 30, 60, 60, 16)    0         \n",
      "                                                                 \n",
      " time_distributed_16 (TimeDi  (None, 30, 60, 60, 32)   4640      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_17 (TimeDi  (None, 30, 60, 60, 32)   128       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_18 (TimeDi  (None, 30, 30, 30, 32)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 30, 30, 30, 32)    0         \n",
      "                                                                 \n",
      " time_distributed_19 (TimeDi  (None, 30, 30, 30, 64)   18496     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_20 (TimeDi  (None, 30, 30, 30, 64)   256       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_21 (TimeDi  (None, 30, 15, 15, 64)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 30, 15, 15, 64)    0         \n",
      "                                                                 \n",
      " time_distributed_22 (TimeDi  (None, 30, 15, 15, 128)  73856     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_23 (TimeDi  (None, 30, 15, 15, 128)  512       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_24 (TimeDi  (None, 30, 7, 7, 128)    0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 30, 7, 7, 128)     0         \n",
      "                                                                 \n",
      " time_distributed_25 (TimeDi  (None, 30, 6272)         0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 30, 6272)          0         \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 1024)              22419456  \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 5)                 5125      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,522,981\n",
      "Trainable params: 22,522,501\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Compiling the model using Adam as optimizer\n",
    "model_cnn_GRU.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_cnn_GRU.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________\n",
      "Batch Size is : 8\n",
      "\n",
      "Source path =  datasets/Project_data/train ; batch size = 8\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 2.3855 - categorical_accuracy: 0.2670\n",
      "Source path =  datasets/Project_data/val ; batch size = 8\n",
      "\n",
      "Epoch 1: saving model to model_init_2023-02-0720_24_17.938535\\model-00001-2.38550-0.26697-4.34948-0.21000.h5\n",
      "83/83 [==============================] - 124s 1s/step - loss: 2.3855 - categorical_accuracy: 0.2670 - val_loss: 4.3495 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6972 - categorical_accuracy: 0.3363\n",
      "Epoch 2: saving model to model_init_2023-02-0720_24_17.938535\\model-00002-1.69718-0.33635-2.74405-0.15000.h5\n",
      "83/83 [==============================] - 122s 1s/step - loss: 1.6972 - categorical_accuracy: 0.3363 - val_loss: 2.7440 - val_categorical_accuracy: 0.1500 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.4534 - categorical_accuracy: 0.4540\n",
      "Epoch 3: saving model to model_init_2023-02-0720_24_17.938535\\model-00003-1.45336-0.45400-2.10604-0.28000.h5\n",
      "83/83 [==============================] - 123s 1s/step - loss: 1.4534 - categorical_accuracy: 0.4540 - val_loss: 2.1060 - val_categorical_accuracy: 0.2800 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.2097 - categorical_accuracy: 0.5324\n",
      "Epoch 4: saving model to model_init_2023-02-0720_24_17.938535\\model-00004-1.20972-0.53243-2.20754-0.31000.h5\n",
      "83/83 [==============================] - 123s 1s/step - loss: 1.2097 - categorical_accuracy: 0.5324 - val_loss: 2.2075 - val_categorical_accuracy: 0.3100 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.0201 - categorical_accuracy: 0.6229\n",
      "Epoch 5: saving model to model_init_2023-02-0720_24_17.938535\\model-00005-1.02013-0.62293-1.76832-0.44000.h5\n",
      "83/83 [==============================] - 123s 1s/step - loss: 1.0201 - categorical_accuracy: 0.6229 - val_loss: 1.7683 - val_categorical_accuracy: 0.4400 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.8776 - categorical_accuracy: 0.6652\n",
      "Epoch 6: saving model to model_init_2023-02-0720_24_17.938535\\model-00006-0.87759-0.66516-1.52222-0.54000.h5\n",
      "83/83 [==============================] - 122s 1s/step - loss: 0.8776 - categorical_accuracy: 0.6652 - val_loss: 1.5222 - val_categorical_accuracy: 0.5400 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.7035 - categorical_accuracy: 0.7300\n",
      "Epoch 7: saving model to model_init_2023-02-0720_24_17.938535\\model-00007-0.70346-0.73002-0.64132-0.78000.h5\n",
      "83/83 [==============================] - 122s 1s/step - loss: 0.7035 - categorical_accuracy: 0.7300 - val_loss: 0.6413 - val_categorical_accuracy: 0.7800 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.5975 - categorical_accuracy: 0.7964\n",
      "Epoch 8: saving model to model_init_2023-02-0720_24_17.938535\\model-00008-0.59751-0.79638-0.84316-0.71000.h5\n",
      "83/83 [==============================] - 123s 1s/step - loss: 0.5975 - categorical_accuracy: 0.7964 - val_loss: 0.8432 - val_categorical_accuracy: 0.7100 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.5891 - categorical_accuracy: 0.7692\n",
      "Epoch 9: saving model to model_init_2023-02-0720_24_17.938535\\model-00009-0.58905-0.76923-1.08832-0.73000.h5\n",
      "83/83 [==============================] - 123s 1s/step - loss: 0.5891 - categorical_accuracy: 0.7692 - val_loss: 1.0883 - val_categorical_accuracy: 0.7300 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.5537 - categorical_accuracy: 0.8039\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 10: saving model to model_init_2023-02-0720_24_17.938535\\model-00010-0.55373-0.80392-0.73514-0.77000.h5\n",
      "83/83 [==============================] - 123s 1s/step - loss: 0.5537 - categorical_accuracy: 0.8039 - val_loss: 0.7351 - val_categorical_accuracy: 0.7700 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.4414 - categorical_accuracy: 0.8175\n",
      "Epoch 11: saving model to model_init_2023-02-0720_24_17.938535\\model-00011-0.44143-0.81750-0.66580-0.80000.h5\n",
      "83/83 [==============================] - 123s 1s/step - loss: 0.4414 - categorical_accuracy: 0.8175 - val_loss: 0.6658 - val_categorical_accuracy: 0.8000 - lr: 5.0000e-04\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.3038 - categorical_accuracy: 0.8959\n",
      "Epoch 12: saving model to model_init_2023-02-0720_24_17.938535\\model-00012-0.30381-0.89593-0.68935-0.75000.h5\n",
      "83/83 [==============================] - 122s 1s/step - loss: 0.3038 - categorical_accuracy: 0.8959 - val_loss: 0.6894 - val_categorical_accuracy: 0.7500 - lr: 5.0000e-04\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.2431 - categorical_accuracy: 0.9020\n",
      "Epoch 13: saving model to model_init_2023-02-0720_24_17.938535\\model-00013-0.24312-0.90196-0.58906-0.82000.h5\n",
      "83/83 [==============================] - 122s 1s/step - loss: 0.2431 - categorical_accuracy: 0.9020 - val_loss: 0.5891 - val_categorical_accuracy: 0.8200 - lr: 5.0000e-04\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.2040 - categorical_accuracy: 0.9321\n",
      "Epoch 14: saving model to model_init_2023-02-0720_24_17.938535\\model-00014-0.20404-0.93213-0.58624-0.83000.h5\n",
      "83/83 [==============================] - 123s 1s/step - loss: 0.2040 - categorical_accuracy: 0.9321 - val_loss: 0.5862 - val_categorical_accuracy: 0.8300 - lr: 5.0000e-04\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.1733 - categorical_accuracy: 0.9397\n",
      "Epoch 15: saving model to model_init_2023-02-0720_24_17.938535\\model-00015-0.17329-0.93967-0.37240-0.91000.h5\n",
      "83/83 [==============================] - 123s 1s/step - loss: 0.1733 - categorical_accuracy: 0.9397 - val_loss: 0.3724 - val_categorical_accuracy: 0.9100 - lr: 5.0000e-04\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.1624 - categorical_accuracy: 0.9367\n",
      "Epoch 16: saving model to model_init_2023-02-0720_24_17.938535\\model-00016-0.16237-0.93665-0.37091-0.89000.h5\n",
      "83/83 [==============================] - 122s 1s/step - loss: 0.1624 - categorical_accuracy: 0.9367 - val_loss: 0.3709 - val_categorical_accuracy: 0.8900 - lr: 5.0000e-04\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.1452 - categorical_accuracy: 0.9472\n",
      "Epoch 17: saving model to model_init_2023-02-0720_24_17.938535\\model-00017-0.14523-0.94721-0.62743-0.84000.h5\n",
      "83/83 [==============================] - 123s 1s/step - loss: 0.1452 - categorical_accuracy: 0.9472 - val_loss: 0.6274 - val_categorical_accuracy: 0.8400 - lr: 5.0000e-04\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.1215 - categorical_accuracy: 0.9608\n",
      "Epoch 18: saving model to model_init_2023-02-0720_24_17.938535\\model-00018-0.12153-0.96078-0.37776-0.88000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 0.1215 - categorical_accuracy: 0.9608 - val_loss: 0.3778 - val_categorical_accuracy: 0.8800 - lr: 5.0000e-04\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.1117 - categorical_accuracy: 0.9683\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 19: saving model to model_init_2023-02-0720_24_17.938535\\model-00019-0.11173-0.96833-0.48712-0.85000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 0.1117 - categorical_accuracy: 0.9683 - val_loss: 0.4871 - val_categorical_accuracy: 0.8500 - lr: 5.0000e-04\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.0914 - categorical_accuracy: 0.9683\n",
      "Epoch 20: saving model to model_init_2023-02-0720_24_17.938535\\model-00020-0.09139-0.96833-0.58108-0.83000.h5\n",
      "83/83 [==============================] - 122s 1s/step - loss: 0.0914 - categorical_accuracy: 0.9683 - val_loss: 0.5811 - val_categorical_accuracy: 0.8300 - lr: 2.5000e-04\n",
      "Time Taken to Train this model was :  0:40:57.944320\n"
     ]
    }
   ],
   "source": [
    "batch_size=8\n",
    "trainmodel(model_cnn_GRU,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_GRU.save(\"model_cnn_GRU.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning - Using Pretrained Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Resnet 50 with GRU layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94765736/94765736 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "# Importing the model and removing the top layer\n",
    "restnet_50_model = ResNet50(weights='imagenet', include_top=False,pooling='max')\n",
    "\n",
    "# We do not want to lose the learning obtained form the Pre-Trained model so we will freeze all but\n",
    "# 10 layers\n",
    "for layer in restnet_50_model.layers[:-10]:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 7: Resnet50 with GRU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture \n",
    "# We have added a GRU layer after the resnet\n",
    "model_resnetPlusGRU = Sequential()\n",
    "model_resnetPlusGRU.add(TimeDistributed(restnet_50_model,input_shape=(30,120,120,3)))\n",
    "\n",
    "model_resnetPlusGRU.add(GRU(512))\n",
    "model_resnetPlusGRU.add(Dropout(0.5))\n",
    "\n",
    "model_resnetPlusGRU.add(Dense(5, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_26 (TimeDi  (None, 30, 2048)         23587712  \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 512)               3935232   \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,525,509\n",
      "Trainable params: 7,354,373\n",
      "Non-trainable params: 20,171,136\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Compiling the model using Adam as optimizer\n",
    "model_resnetPlusGRU.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_resnetPlusGRU.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________\n",
      "Batch Size is : 8\n",
      "\n",
      "Source path =  datasets/Project_data/train ; batch size = 8\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 2.2349 - categorical_accuracy: 0.2127\n",
      "Source path =  datasets/Project_data/val ; batch size = 8\n",
      "\n",
      "Epoch 1: saving model to model_init_2023-02-0720_24_17.938535\\model-00001-2.23491-0.21267-1.74423-0.24000.h5\n",
      "83/83 [==============================] - 130s 2s/step - loss: 2.2349 - categorical_accuracy: 0.2127 - val_loss: 1.7442 - val_categorical_accuracy: 0.2400 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 2.0905 - categorical_accuracy: 0.1931\n",
      "Epoch 2: saving model to model_init_2023-02-0720_24_17.938535\\model-00002-2.09053-0.19306-1.60070-0.23000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 2.0905 - categorical_accuracy: 0.1931 - val_loss: 1.6007 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 2.0518 - categorical_accuracy: 0.2006\n",
      "Epoch 3: saving model to model_init_2023-02-0720_24_17.938535\\model-00003-2.05183-0.20060-1.60880-0.18000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 2.0518 - categorical_accuracy: 0.2006 - val_loss: 1.6088 - val_categorical_accuracy: 0.1800 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.9662 - categorical_accuracy: 0.2262\n",
      "Epoch 4: saving model to model_init_2023-02-0720_24_17.938535\\model-00004-1.96623-0.22624-1.62996-0.23000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 1.9662 - categorical_accuracy: 0.2262 - val_loss: 1.6300 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.9692 - categorical_accuracy: 0.2157\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 5: saving model to model_init_2023-02-0720_24_17.938535\\model-00005-1.96916-0.21569-1.61884-0.28000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 1.9692 - categorical_accuracy: 0.2157 - val_loss: 1.6188 - val_categorical_accuracy: 0.2800 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.9187 - categorical_accuracy: 0.1885\n",
      "Epoch 6: saving model to model_init_2023-02-0720_24_17.938535\\model-00006-1.91875-0.18854-1.64181-0.20000.h5\n",
      "83/83 [==============================] - 123s 2s/step - loss: 1.9187 - categorical_accuracy: 0.1885 - val_loss: 1.6418 - val_categorical_accuracy: 0.2000 - lr: 5.0000e-04\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.9256 - categorical_accuracy: 0.1931\n",
      "Epoch 7: saving model to model_init_2023-02-0720_24_17.938535\\model-00007-1.92557-0.19306-1.58219-0.29000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 1.9256 - categorical_accuracy: 0.1931 - val_loss: 1.5822 - val_categorical_accuracy: 0.2900 - lr: 5.0000e-04\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.8690 - categorical_accuracy: 0.2157\n",
      "Epoch 8: saving model to model_init_2023-02-0720_24_17.938535\\model-00008-1.86904-0.21569-1.62988-0.17000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 1.8690 - categorical_accuracy: 0.2157 - val_loss: 1.6299 - val_categorical_accuracy: 0.1700 - lr: 5.0000e-04\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.9356 - categorical_accuracy: 0.1885\n",
      "Epoch 9: saving model to model_init_2023-02-0720_24_17.938535\\model-00009-1.93563-0.18854-1.60411-0.22000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 1.9356 - categorical_accuracy: 0.1885 - val_loss: 1.6041 - val_categorical_accuracy: 0.2200 - lr: 5.0000e-04\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.8108 - categorical_accuracy: 0.2232\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 10: saving model to model_init_2023-02-0720_24_17.938535\\model-00010-1.81076-0.22323-1.66725-0.16000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 1.8108 - categorical_accuracy: 0.2232 - val_loss: 1.6673 - val_categorical_accuracy: 0.1600 - lr: 5.0000e-04\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.8728 - categorical_accuracy: 0.1750\n",
      "Epoch 11: saving model to model_init_2023-02-0720_24_17.938535\\model-00011-1.87278-0.17496-1.61945-0.18000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 1.8728 - categorical_accuracy: 0.1750 - val_loss: 1.6195 - val_categorical_accuracy: 0.1800 - lr: 2.5000e-04\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.8116 - categorical_accuracy: 0.2232\n",
      "Epoch 12: saving model to model_init_2023-02-0720_24_17.938535\\model-00012-1.81157-0.22323-1.62159-0.19000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 1.8116 - categorical_accuracy: 0.2232 - val_loss: 1.6216 - val_categorical_accuracy: 0.1900 - lr: 2.5000e-04\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.7933 - categorical_accuracy: 0.2006\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 13: saving model to model_init_2023-02-0720_24_17.938535\\model-00013-1.79332-0.20060-1.64886-0.16000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 1.7933 - categorical_accuracy: 0.2006 - val_loss: 1.6489 - val_categorical_accuracy: 0.1600 - lr: 2.5000e-04\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.7979 - categorical_accuracy: 0.2066\n",
      "Epoch 14: saving model to model_init_2023-02-0720_24_17.938535\\model-00014-1.79792-0.20664-1.60995-0.23000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 1.7979 - categorical_accuracy: 0.2066 - val_loss: 1.6100 - val_categorical_accuracy: 0.2300 - lr: 1.2500e-04\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.8158 - categorical_accuracy: 0.1870\n",
      "Epoch 15: saving model to model_init_2023-02-0720_24_17.938535\\model-00015-1.81584-0.18703-1.61410-0.23000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 1.8158 - categorical_accuracy: 0.1870 - val_loss: 1.6141 - val_categorical_accuracy: 0.2300 - lr: 1.2500e-04\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.8161 - categorical_accuracy: 0.2021\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 16: saving model to model_init_2023-02-0720_24_17.938535\\model-00016-1.81612-0.20211-1.61218-0.19000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 1.8161 - categorical_accuracy: 0.2021 - val_loss: 1.6122 - val_categorical_accuracy: 0.1900 - lr: 1.2500e-04\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.7589 - categorical_accuracy: 0.2021\n",
      "Epoch 17: saving model to model_init_2023-02-0720_24_17.938535\\model-00017-1.75888-0.20211-1.59944-0.21000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 1.7589 - categorical_accuracy: 0.2021 - val_loss: 1.5994 - val_categorical_accuracy: 0.2100 - lr: 6.2500e-05\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.7983 - categorical_accuracy: 0.1931\n",
      "Epoch 18: saving model to model_init_2023-02-0720_24_17.938535\\model-00018-1.79833-0.19306-1.61188-0.22000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 1.7983 - categorical_accuracy: 0.1931 - val_loss: 1.6119 - val_categorical_accuracy: 0.2200 - lr: 6.2500e-05\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.7881 - categorical_accuracy: 0.2097\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 19: saving model to model_init_2023-02-0720_24_17.938535\\model-00019-1.78809-0.20965-1.60645-0.24000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 1.7881 - categorical_accuracy: 0.2097 - val_loss: 1.6064 - val_categorical_accuracy: 0.2400 - lr: 6.2500e-05\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.8071 - categorical_accuracy: 0.2051\n",
      "Epoch 20: saving model to model_init_2023-02-0720_24_17.938535\\model-00020-1.80709-0.20513-1.60580-0.21000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 1.8071 - categorical_accuracy: 0.2051 - val_loss: 1.6058 - val_categorical_accuracy: 0.2100 - lr: 3.1250e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken to Train this model was :  0:41:32.164463\n"
     ]
    }
   ],
   "source": [
    "batch_size=8\n",
    "trainmodel(model_resnetPlusGRU,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnetPlusGRU.save(\"model_resnetPlusGRU.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimenting with lesser trainable layers from the Resnet pretrained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the model and removing the top layer\n",
    "restnet_50_model = ResNet50(weights='imagenet', include_top=False,pooling='max')\n",
    "\n",
    "# Experimenting with the same model as above but with only 5 layer as trainable\n",
    "for layer in restnet_50_model.layers[:-5]:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 8: Resnet50 with GRU-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture \n",
    "# We have added a GRU layer after the resnet\n",
    "model_resnetPlusGRU_2 = Sequential()\n",
    "model_resnetPlusGRU_2.add(TimeDistributed(restnet_50_model,input_shape=(30,120,120,3)))\n",
    "\n",
    "model_resnetPlusGRU_2.add(GRU(512))\n",
    "model_resnetPlusGRU_2.add(Dropout(0.5))\n",
    "\n",
    "model_resnetPlusGRU_2.add(Dense(5, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_27 (TimeDi  (None, 30, 2048)         23587712  \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, 512)               3935232   \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,525,509\n",
      "Trainable params: 4,992,517\n",
      "Non-trainable params: 22,532,992\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Compiling the model using Adam as optimizer\n",
    "model_resnetPlusGRU_2.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_resnetPlusGRU_2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________\n",
      "Batch Size is : 8\n",
      "\n",
      "Source path =  datasets/Project_data/train ; batch size = 8\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 2.3033 - categorical_accuracy: 0.1629\n",
      "Source path =  datasets/Project_data/val ; batch size = 8\n",
      "\n",
      "Epoch 1: saving model to model_init_2023-02-0720_24_17.938535\\model-00001-2.30334-0.16290-1.62454-0.21000.h5\n",
      "83/83 [==============================] - 128s 2s/step - loss: 2.3033 - categorical_accuracy: 0.1629 - val_loss: 1.6245 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 2.1964 - categorical_accuracy: 0.1825\n",
      "Epoch 2: saving model to model_init_2023-02-0720_24_17.938535\\model-00002-2.19641-0.18250-1.66627-0.23000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 2.1964 - categorical_accuracy: 0.1825 - val_loss: 1.6663 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 2.1239 - categorical_accuracy: 0.1885\n",
      "Epoch 3: saving model to model_init_2023-02-0720_24_17.938535\\model-00003-2.12386-0.18854-1.62955-0.23000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 2.1239 - categorical_accuracy: 0.1885 - val_loss: 1.6296 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 2.0038 - categorical_accuracy: 0.2142\n",
      "Epoch 4: saving model to model_init_2023-02-0720_24_17.938535\\model-00004-2.00383-0.21418-1.62441-0.20000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 2.0038 - categorical_accuracy: 0.2142 - val_loss: 1.6244 - val_categorical_accuracy: 0.2000 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.9932 - categorical_accuracy: 0.1961\n",
      "Epoch 5: saving model to model_init_2023-02-0720_24_17.938535\\model-00005-1.99319-0.19608-1.62350-0.23000.h5\n",
      "83/83 [==============================] - 123s 2s/step - loss: 1.9932 - categorical_accuracy: 0.1961 - val_loss: 1.6235 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.9885 - categorical_accuracy: 0.1946\n",
      "Epoch 6: saving model to model_init_2023-02-0720_24_17.938535\\model-00006-1.98851-0.19457-1.60251-0.23000.h5\n",
      "83/83 [==============================] - 123s 2s/step - loss: 1.9885 - categorical_accuracy: 0.1946 - val_loss: 1.6025 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.9601 - categorical_accuracy: 0.1719\n",
      "Epoch 7: saving model to model_init_2023-02-0720_24_17.938535\\model-00007-1.96010-0.17195-1.61026-0.16000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 1.9601 - categorical_accuracy: 0.1719 - val_loss: 1.6103 - val_categorical_accuracy: 0.1600 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.9453 - categorical_accuracy: 0.1750\n",
      "Epoch 8: saving model to model_init_2023-02-0720_24_17.938535\\model-00008-1.94535-0.17496-1.66723-0.16000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 1.9453 - categorical_accuracy: 0.1750 - val_loss: 1.6672 - val_categorical_accuracy: 0.1600 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.9278 - categorical_accuracy: 0.1825\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 9: saving model to model_init_2023-02-0720_24_17.938535\\model-00009-1.92783-0.18250-1.62953-0.19000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 1.9278 - categorical_accuracy: 0.1825 - val_loss: 1.6295 - val_categorical_accuracy: 0.1900 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.8687 - categorical_accuracy: 0.2051\n",
      "Epoch 10: saving model to model_init_2023-02-0720_24_17.938535\\model-00010-1.86865-0.20513-1.64597-0.22000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 1.8687 - categorical_accuracy: 0.2051 - val_loss: 1.6460 - val_categorical_accuracy: 0.2200 - lr: 5.0000e-04\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.8383 - categorical_accuracy: 0.1931\n",
      "Epoch 11: saving model to model_init_2023-02-0720_24_17.938535\\model-00011-1.83830-0.19306-1.66868-0.20000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 1.8383 - categorical_accuracy: 0.1931 - val_loss: 1.6687 - val_categorical_accuracy: 0.2000 - lr: 5.0000e-04\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.8667 - categorical_accuracy: 0.1885\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 12: saving model to model_init_2023-02-0720_24_17.938535\\model-00012-1.86670-0.18854-1.60530-0.28000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 1.8667 - categorical_accuracy: 0.1885 - val_loss: 1.6053 - val_categorical_accuracy: 0.2800 - lr: 5.0000e-04\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.7893 - categorical_accuracy: 0.2112\n",
      "Epoch 13: saving model to model_init_2023-02-0720_24_17.938535\\model-00013-1.78927-0.21116-1.61522-0.24000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 1.7893 - categorical_accuracy: 0.2112 - val_loss: 1.6152 - val_categorical_accuracy: 0.2400 - lr: 2.5000e-04\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.7759 - categorical_accuracy: 0.2097\n",
      "Epoch 14: saving model to model_init_2023-02-0720_24_17.938535\\model-00014-1.77593-0.20965-1.61308-0.23000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 1.7759 - categorical_accuracy: 0.2097 - val_loss: 1.6131 - val_categorical_accuracy: 0.2300 - lr: 2.5000e-04\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.8099 - categorical_accuracy: 0.2006\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 15: saving model to model_init_2023-02-0720_24_17.938535\\model-00015-1.80991-0.20060-1.60732-0.20000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 1.8099 - categorical_accuracy: 0.2006 - val_loss: 1.6073 - val_categorical_accuracy: 0.2000 - lr: 2.5000e-04\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.7515 - categorical_accuracy: 0.2217\n",
      "Epoch 16: saving model to model_init_2023-02-0720_24_17.938535\\model-00016-1.75153-0.22172-1.61419-0.20000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 1.7515 - categorical_accuracy: 0.2217 - val_loss: 1.6142 - val_categorical_accuracy: 0.2000 - lr: 1.2500e-04\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.7716 - categorical_accuracy: 0.2036\n",
      "Epoch 17: saving model to model_init_2023-02-0720_24_17.938535\\model-00017-1.77156-0.20362-1.60128-0.25000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 1.7716 - categorical_accuracy: 0.2036 - val_loss: 1.6013 - val_categorical_accuracy: 0.2500 - lr: 1.2500e-04\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.7636 - categorical_accuracy: 0.2157\n",
      "Epoch 18: saving model to model_init_2023-02-0720_24_17.938535\\model-00018-1.76361-0.21569-1.62459-0.19000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 1.7636 - categorical_accuracy: 0.2157 - val_loss: 1.6246 - val_categorical_accuracy: 0.1900 - lr: 1.2500e-04\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.7632 - categorical_accuracy: 0.1961\n",
      "Epoch 19: saving model to model_init_2023-02-0720_24_17.938535\\model-00019-1.76323-0.19608-1.59414-0.22000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 1.7632 - categorical_accuracy: 0.1961 - val_loss: 1.5941 - val_categorical_accuracy: 0.2200 - lr: 1.2500e-04\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.7545 - categorical_accuracy: 0.2021\n",
      "Epoch 20: saving model to model_init_2023-02-0720_24_17.938535\\model-00020-1.75450-0.20211-1.60003-0.20000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 1.7545 - categorical_accuracy: 0.2021 - val_loss: 1.6000 - val_categorical_accuracy: 0.2000 - lr: 1.2500e-04\n",
      "Time Taken to Train this model was :  0:41:27.183254\n"
     ]
    }
   ],
   "source": [
    "batch_size=8\n",
    "trainmodel(model_resnetPlusGRU_2,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnetPlusGRU_2.save(\"model_resnetPlusGRU_2.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Using InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 9: InceptionV3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87910968/87910968 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Importing the model and removing the top layer\n",
    "InceptionV3_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# We do not want to lose the learning obtained form the Pre-Trained model so we will freeze all but\n",
    "# 10 layers\n",
    "for layer in InceptionV3_model.layers[:-10]:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Architecture \n",
    "## GlobalAveragePooling3D layer and a dense layer with 1024 neurons added after Inception layers\n",
    "model_Inception = Sequential()\n",
    "model_Inception.add(TimeDistributed(InceptionV3_model,input_shape=(30,120,120,3)))\n",
    "model_Inception.add(GlobalAveragePooling3D())\n",
    "model_Inception.add(Dense(1024, activation='relu'))\n",
    "model_Inception.add(Dropout(0.5))\n",
    "model_Inception.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_28 (TimeDi  (None, 30, 2, 2, 2048)   21802784  \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " global_average_pooling3d_1   (None, 2048)             0         \n",
      " (GlobalAveragePooling3D)                                        \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 5)                 5125      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,906,085\n",
      "Trainable params: 2,103,493\n",
      "Non-trainable params: 21,802,592\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Compiling the model using Adam as optimizer\n",
    "model_Inception.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_Inception.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________\n",
      "Batch Size is : 8\n",
      "\n",
      "Source path =  datasets/Project_data/train ; batch size = 8\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 4.1066 - categorical_accuracy: 0.3514\n",
      "Source path =  datasets/Project_data/val ; batch size = 8\n",
      "\n",
      "Epoch 1: saving model to model_init_2023-02-0720_24_17.938535\\model-00001-4.10659-0.35143-1.46262-0.53000.h5\n",
      "83/83 [==============================] - 129s 2s/step - loss: 4.1066 - categorical_accuracy: 0.3514 - val_loss: 1.4626 - val_categorical_accuracy: 0.5300 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.1863 - categorical_accuracy: 0.5460\n",
      "Epoch 2: saving model to model_init_2023-02-0720_24_17.938535\\model-00002-1.18634-0.54600-0.90204-0.54000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 1.1863 - categorical_accuracy: 0.5460 - val_loss: 0.9020 - val_categorical_accuracy: 0.5400 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.9503 - categorical_accuracy: 0.6214\n",
      "Epoch 3: saving model to model_init_2023-02-0720_24_17.938535\\model-00003-0.95035-0.62142-1.10398-0.60000.h5\n",
      "83/83 [==============================] - 123s 2s/step - loss: 0.9503 - categorical_accuracy: 0.6214 - val_loss: 1.1040 - val_categorical_accuracy: 0.6000 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.8010 - categorical_accuracy: 0.6727\n",
      "Epoch 4: saving model to model_init_2023-02-0720_24_17.938535\\model-00004-0.80098-0.67270-0.85824-0.67000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 0.8010 - categorical_accuracy: 0.6727 - val_loss: 0.8582 - val_categorical_accuracy: 0.6700 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.7655 - categorical_accuracy: 0.6878\n",
      "Epoch 5: saving model to model_init_2023-02-0720_24_17.938535\\model-00005-0.76551-0.68778-0.79845-0.67000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 0.7655 - categorical_accuracy: 0.6878 - val_loss: 0.7984 - val_categorical_accuracy: 0.6700 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.7107 - categorical_accuracy: 0.7179\n",
      "Epoch 6: saving model to model_init_2023-02-0720_24_17.938535\\model-00006-0.71065-0.71795-0.95443-0.67000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 0.7107 - categorical_accuracy: 0.7179 - val_loss: 0.9544 - val_categorical_accuracy: 0.6700 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.6552 - categorical_accuracy: 0.7406\n",
      "Epoch 7: saving model to model_init_2023-02-0720_24_17.938535\\model-00007-0.65524-0.74057-1.07915-0.59000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 0.6552 - categorical_accuracy: 0.7406 - val_loss: 1.0791 - val_categorical_accuracy: 0.5900 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.6212 - categorical_accuracy: 0.7602\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 8: saving model to model_init_2023-02-0720_24_17.938535\\model-00008-0.62116-0.76018-0.81020-0.70000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 0.6212 - categorical_accuracy: 0.7602 - val_loss: 0.8102 - val_categorical_accuracy: 0.7000 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.4345 - categorical_accuracy: 0.8431\n",
      "Epoch 9: saving model to model_init_2023-02-0720_24_17.938535\\model-00009-0.43450-0.84314-1.05532-0.57000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 0.4345 - categorical_accuracy: 0.8431 - val_loss: 1.0553 - val_categorical_accuracy: 0.5700 - lr: 5.0000e-04\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.4037 - categorical_accuracy: 0.8462\n",
      "Epoch 10: saving model to model_init_2023-02-0720_24_17.938535\\model-00010-0.40365-0.84615-0.86640-0.67000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 0.4037 - categorical_accuracy: 0.8462 - val_loss: 0.8664 - val_categorical_accuracy: 0.6700 - lr: 5.0000e-04\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.3394 - categorical_accuracy: 0.8869\n",
      "Epoch 11: saving model to model_init_2023-02-0720_24_17.938535\\model-00011-0.33943-0.88688-0.76807-0.73000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 0.3394 - categorical_accuracy: 0.8869 - val_loss: 0.7681 - val_categorical_accuracy: 0.7300 - lr: 5.0000e-04\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.3301 - categorical_accuracy: 0.8748\n",
      "Epoch 12: saving model to model_init_2023-02-0720_24_17.938535\\model-00012-0.33006-0.87481-0.94932-0.65000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 0.3301 - categorical_accuracy: 0.8748 - val_loss: 0.9493 - val_categorical_accuracy: 0.6500 - lr: 5.0000e-04\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.2668 - categorical_accuracy: 0.9095\n",
      "Epoch 13: saving model to model_init_2023-02-0720_24_17.938535\\model-00013-0.26681-0.90950-0.94307-0.67000.h5\n",
      "83/83 [==============================] - 123s 2s/step - loss: 0.2668 - categorical_accuracy: 0.9095 - val_loss: 0.9431 - val_categorical_accuracy: 0.6700 - lr: 5.0000e-04\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.2693 - categorical_accuracy: 0.8929\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 14: saving model to model_init_2023-02-0720_24_17.938535\\model-00014-0.26927-0.89291-0.86820-0.63000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 0.2693 - categorical_accuracy: 0.8929 - val_loss: 0.8682 - val_categorical_accuracy: 0.6300 - lr: 5.0000e-04\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.2193 - categorical_accuracy: 0.9336\n",
      "Epoch 15: saving model to model_init_2023-02-0720_24_17.938535\\model-00015-0.21929-0.93363-0.96414-0.70000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 0.2193 - categorical_accuracy: 0.9336 - val_loss: 0.9641 - val_categorical_accuracy: 0.7000 - lr: 2.5000e-04\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.1738 - categorical_accuracy: 0.9532\n",
      "Epoch 16: saving model to model_init_2023-02-0720_24_17.938535\\model-00016-0.17383-0.95324-0.85772-0.70000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 0.1738 - categorical_accuracy: 0.9532 - val_loss: 0.8577 - val_categorical_accuracy: 0.7000 - lr: 2.5000e-04\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.1736 - categorical_accuracy: 0.9487\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 17: saving model to model_init_2023-02-0720_24_17.938535\\model-00017-0.17363-0.94872-0.96362-0.64000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 0.1736 - categorical_accuracy: 0.9487 - val_loss: 0.9636 - val_categorical_accuracy: 0.6400 - lr: 2.5000e-04\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.1668 - categorical_accuracy: 0.9548\n",
      "Epoch 18: saving model to model_init_2023-02-0720_24_17.938535\\model-00018-0.16675-0.95475-1.04007-0.61000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 0.1668 - categorical_accuracy: 0.9548 - val_loss: 1.0401 - val_categorical_accuracy: 0.6100 - lr: 1.2500e-04\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.1564 - categorical_accuracy: 0.9593\n",
      "Epoch 19: saving model to model_init_2023-02-0720_24_17.938535\\model-00019-0.15642-0.95928-1.01210-0.63000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 0.1564 - categorical_accuracy: 0.9593 - val_loss: 1.0121 - val_categorical_accuracy: 0.6300 - lr: 1.2500e-04\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.1448 - categorical_accuracy: 0.9623\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 20: saving model to model_init_2023-02-0720_24_17.938535\\model-00020-0.14483-0.96229-0.78077-0.68000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 0.1448 - categorical_accuracy: 0.9623 - val_loss: 0.7808 - val_categorical_accuracy: 0.6800 - lr: 1.2500e-04\n",
      "Time Taken to Train this model was :  0:41:28.448926\n"
     ]
    }
   ],
   "source": [
    "batch_size=8\n",
    "trainmodel(model_Inception,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Inception.save(\"model_Inception.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimenting with lesser trainable layers from the pretrained InceptionV3 model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the model and removing the top layer\n",
    "InceptionV3_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# Experimenting with the same model as above but with only 5 layer as trainable\n",
    "for layer in InceptionV3_model.layers[:-5]:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 10: InceptionV3 -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Architecture \n",
    "## GlobalAveragePooling3D layer and a dense layer with 1024 neurons added after Inception layers\n",
    "model_Inception_2 = Sequential()\n",
    "model_Inception_2.add(TimeDistributed(InceptionV3_model,input_shape=(30,120,120,3)))\n",
    "model_Inception_2.add(GlobalAveragePooling3D())\n",
    "model_Inception_2.add(Dense(1024, activation='relu'))\n",
    "model_Inception_2.add(Dropout(0.5))\n",
    "model_Inception_2.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_29 (TimeDi  (None, 30, 2, 2, 2048)   21802784  \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " global_average_pooling3d_2   (None, 2048)             0         \n",
      " (GlobalAveragePooling3D)                                        \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 5)                 5125      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,906,085\n",
      "Trainable params: 2,103,301\n",
      "Non-trainable params: 21,802,784\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Compiling the model using Adam as optimizer\n",
    "model_Inception_2.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_Inception_2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________\n",
      "Batch Size is : 8\n",
      "\n",
      "Source path =  datasets/Project_data/train ; batch size = 8\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 4.1422 - categorical_accuracy: 0.3786\n",
      "Source path =  datasets/Project_data/val ; batch size = 8\n",
      "\n",
      "Epoch 1: saving model to model_init_2023-02-0720_24_17.938535\\model-00001-4.14218-0.37858-1.38498-0.50000.h5\n",
      "83/83 [==============================] - 129s 2s/step - loss: 4.1422 - categorical_accuracy: 0.3786 - val_loss: 1.3850 - val_categorical_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.1894 - categorical_accuracy: 0.5701\n",
      "Epoch 2: saving model to model_init_2023-02-0720_24_17.938535\\model-00002-1.18941-0.57014-1.03227-0.59000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 1.1894 - categorical_accuracy: 0.5701 - val_loss: 1.0323 - val_categorical_accuracy: 0.5900 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.9292 - categorical_accuracy: 0.6320\n",
      "Epoch 3: saving model to model_init_2023-02-0720_24_17.938535\\model-00003-0.92916-0.63198-0.91973-0.58000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 0.9292 - categorical_accuracy: 0.6320 - val_loss: 0.9197 - val_categorical_accuracy: 0.5800 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.8258 - categorical_accuracy: 0.6712\n",
      "Epoch 4: saving model to model_init_2023-02-0720_24_17.938535\\model-00004-0.82582-0.67119-1.02619-0.53000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 0.8258 - categorical_accuracy: 0.6712 - val_loss: 1.0262 - val_categorical_accuracy: 0.5300 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.7388 - categorical_accuracy: 0.7119\n",
      "Epoch 5: saving model to model_init_2023-02-0720_24_17.938535\\model-00005-0.73879-0.71192-0.85707-0.53000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 0.7388 - categorical_accuracy: 0.7119 - val_loss: 0.8571 - val_categorical_accuracy: 0.5300 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.6912 - categorical_accuracy: 0.7315\n",
      "Epoch 6: saving model to model_init_2023-02-0720_24_17.938535\\model-00006-0.69118-0.73152-0.70635-0.72000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 0.6912 - categorical_accuracy: 0.7315 - val_loss: 0.7064 - val_categorical_accuracy: 0.7200 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.6981 - categorical_accuracy: 0.7195\n",
      "Epoch 7: saving model to model_init_2023-02-0720_24_17.938535\\model-00007-0.69809-0.71946-0.78643-0.65000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 0.6981 - categorical_accuracy: 0.7195 - val_loss: 0.7864 - val_categorical_accuracy: 0.6500 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.6028 - categorical_accuracy: 0.7828\n",
      "Epoch 8: saving model to model_init_2023-02-0720_24_17.938535\\model-00008-0.60280-0.78281-0.90687-0.53000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 0.6028 - categorical_accuracy: 0.7828 - val_loss: 0.9069 - val_categorical_accuracy: 0.5300 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.4682 - categorical_accuracy: 0.8265\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 9: saving model to model_init_2023-02-0720_24_17.938535\\model-00009-0.46825-0.82655-0.94418-0.65000.h5\n",
      "83/83 [==============================] - 123s 2s/step - loss: 0.4682 - categorical_accuracy: 0.8265 - val_loss: 0.9442 - val_categorical_accuracy: 0.6500 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.3710 - categorical_accuracy: 0.8703\n",
      "Epoch 10: saving model to model_init_2023-02-0720_24_17.938535\\model-00010-0.37097-0.87029-0.94601-0.64000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 0.3710 - categorical_accuracy: 0.8703 - val_loss: 0.9460 - val_categorical_accuracy: 0.6400 - lr: 5.0000e-04\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.3426 - categorical_accuracy: 0.8748\n",
      "Epoch 11: saving model to model_init_2023-02-0720_24_17.938535\\model-00011-0.34260-0.87481-0.82522-0.69000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 0.3426 - categorical_accuracy: 0.8748 - val_loss: 0.8252 - val_categorical_accuracy: 0.6900 - lr: 5.0000e-04\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.2914 - categorical_accuracy: 0.8899\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 12: saving model to model_init_2023-02-0720_24_17.938535\\model-00012-0.29140-0.88989-0.97834-0.71000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 0.2914 - categorical_accuracy: 0.8899 - val_loss: 0.9783 - val_categorical_accuracy: 0.7100 - lr: 5.0000e-04\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.2495 - categorical_accuracy: 0.9170\n",
      "Epoch 13: saving model to model_init_2023-02-0720_24_17.938535\\model-00013-0.24947-0.91704-0.86651-0.66000.h5\n",
      "83/83 [==============================] - 123s 2s/step - loss: 0.2495 - categorical_accuracy: 0.9170 - val_loss: 0.8665 - val_categorical_accuracy: 0.6600 - lr: 2.5000e-04\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.2127 - categorical_accuracy: 0.9321\n",
      "Epoch 14: saving model to model_init_2023-02-0720_24_17.938535\\model-00014-0.21270-0.93213-1.05276-0.62000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 0.2127 - categorical_accuracy: 0.9321 - val_loss: 1.0528 - val_categorical_accuracy: 0.6200 - lr: 2.5000e-04\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.2076 - categorical_accuracy: 0.9216\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 15: saving model to model_init_2023-02-0720_24_17.938535\\model-00015-0.20764-0.92157-0.88528-0.63000.h5\n",
      "83/83 [==============================] - 123s 2s/step - loss: 0.2076 - categorical_accuracy: 0.9216 - val_loss: 0.8853 - val_categorical_accuracy: 0.6300 - lr: 2.5000e-04\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.2106 - categorical_accuracy: 0.9351\n",
      "Epoch 16: saving model to model_init_2023-02-0720_24_17.938535\\model-00016-0.21061-0.93514-0.89656-0.70000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 0.2106 - categorical_accuracy: 0.9351 - val_loss: 0.8966 - val_categorical_accuracy: 0.7000 - lr: 1.2500e-04\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.1767 - categorical_accuracy: 0.9472\n",
      "Epoch 17: saving model to model_init_2023-02-0720_24_17.938535\\model-00017-0.17672-0.94721-0.86599-0.69000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 0.1767 - categorical_accuracy: 0.9472 - val_loss: 0.8660 - val_categorical_accuracy: 0.6900 - lr: 1.2500e-04\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.1585 - categorical_accuracy: 0.9563\n",
      "Epoch 18: saving model to model_init_2023-02-0720_24_17.938535\\model-00018-0.15850-0.95626-0.64981-0.70000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 0.1585 - categorical_accuracy: 0.9563 - val_loss: 0.6498 - val_categorical_accuracy: 0.7000 - lr: 1.2500e-04\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.1665 - categorical_accuracy: 0.9548\n",
      "Epoch 19: saving model to model_init_2023-02-0720_24_17.938535\\model-00019-0.16651-0.95475-0.91224-0.70000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 0.1665 - categorical_accuracy: 0.9548 - val_loss: 0.9122 - val_categorical_accuracy: 0.7000 - lr: 1.2500e-04\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.1545 - categorical_accuracy: 0.9608\n",
      "Epoch 20: saving model to model_init_2023-02-0720_24_17.938535\\model-00020-0.15453-0.96078-0.91532-0.68000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 0.1545 - categorical_accuracy: 0.9608 - val_loss: 0.9153 - val_categorical_accuracy: 0.6800 - lr: 1.2500e-04\n",
      "Time Taken to Train this model was :  0:41:26.690478\n"
     ]
    }
   ],
   "source": [
    "batch_size=8\n",
    "trainmodel(model_Inception_2,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Inception_2.save(\"model_Inception_2.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg19 import VGG19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 11 VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80134624/80134624 [==============================] - 5s 0us/step\n"
     ]
    }
   ],
   "source": [
    "VGG19_model = VGG19(weights='imagenet', include_top=False)\n",
    "# We do not want to lose the learning obtained form the Pre-Trained model so we will freeze all but\n",
    "# 10 layers\n",
    "for layer in VGG19_model.layers[:-10]:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "## Added GlobalAveragePooling3D and a dense layer of 1024 neurons followed by a dropout of 0.5\n",
    "model_vgg19 = Sequential()\n",
    "model_vgg19.add(TimeDistributed(VGG19_model,input_shape=(30,120,120,3)))\n",
    "model_vgg19.add(GlobalAveragePooling3D())\n",
    "model_vgg19.add(Dense(1024, activation='relu'))\n",
    "model_vgg19.add(Dropout(0.5))\n",
    "model_vgg19.add(Dense(5, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_30 (TimeDi  (None, 30, 3, 3, 512)    20024384  \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " global_average_pooling3d_3   (None, 512)              0         \n",
      " (GlobalAveragePooling3D)                                        \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 1024)              525312    \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 5)                 5125      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,554,821\n",
      "Trainable params: 18,229,253\n",
      "Non-trainable params: 2,325,568\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Compiling the model using Adam as optimizer\n",
    "model_vgg19.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_vgg19.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________\n",
      "Batch Size is : 8\n",
      "\n",
      "Source path =  datasets/Project_data/train ; batch size = 8\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 2.9117 - categorical_accuracy: 0.1946\n",
      "Source path =  datasets/Project_data/val ; batch size = 8\n",
      "\n",
      "Epoch 1: saving model to model_init_2023-02-0720_24_17.938535\\model-00001-2.91170-0.19457-1.60858-0.20000.h5\n",
      "83/83 [==============================] - 135s 2s/step - loss: 2.9117 - categorical_accuracy: 0.1946 - val_loss: 1.6086 - val_categorical_accuracy: 0.2000 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6115 - categorical_accuracy: 0.1885\n",
      "Epoch 2: saving model to model_init_2023-02-0720_24_17.938535\\model-00002-1.61147-0.18854-1.60683-0.23000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 1.6115 - categorical_accuracy: 0.1885 - val_loss: 1.6068 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6108 - categorical_accuracy: 0.1931\n",
      "Epoch 3: saving model to model_init_2023-02-0720_24_17.938535\\model-00003-1.61082-0.19306-1.60953-0.13000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 1.6108 - categorical_accuracy: 0.1931 - val_loss: 1.6095 - val_categorical_accuracy: 0.1300 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6096 - categorical_accuracy: 0.1900\n",
      "Epoch 4: saving model to model_init_2023-02-0720_24_17.938535\\model-00004-1.60958-0.19005-1.60674-0.18000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 1.6096 - categorical_accuracy: 0.1900 - val_loss: 1.6067 - val_categorical_accuracy: 0.1800 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6096 - categorical_accuracy: 0.1916\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 5: saving model to model_init_2023-02-0720_24_17.938535\\model-00005-1.60960-0.19155-1.61104-0.19000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 1.6096 - categorical_accuracy: 0.1916 - val_loss: 1.6110 - val_categorical_accuracy: 0.1900 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6081 - categorical_accuracy: 0.2142\n",
      "Epoch 6: saving model to model_init_2023-02-0720_24_17.938535\\model-00006-1.60811-0.21418-1.60497-0.24000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 1.6081 - categorical_accuracy: 0.2142 - val_loss: 1.6050 - val_categorical_accuracy: 0.2400 - lr: 5.0000e-04\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6092 - categorical_accuracy: 0.2021\n",
      "Epoch 7: saving model to model_init_2023-02-0720_24_17.938535\\model-00007-1.60917-0.20211-1.60322-0.23000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 1.6092 - categorical_accuracy: 0.2021 - val_loss: 1.6032 - val_categorical_accuracy: 0.2300 - lr: 5.0000e-04\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6094 - categorical_accuracy: 0.2051\n",
      "Epoch 8: saving model to model_init_2023-02-0720_24_17.938535\\model-00008-1.60940-0.20513-1.60824-0.21000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 1.6094 - categorical_accuracy: 0.2051 - val_loss: 1.6082 - val_categorical_accuracy: 0.2100 - lr: 5.0000e-04\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6089 - categorical_accuracy: 0.2006\n",
      "Epoch 9: saving model to model_init_2023-02-0720_24_17.938535\\model-00009-1.60887-0.20060-1.60996-0.19000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 1.6089 - categorical_accuracy: 0.2006 - val_loss: 1.6100 - val_categorical_accuracy: 0.1900 - lr: 5.0000e-04\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6095 - categorical_accuracy: 0.1885\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 10: saving model to model_init_2023-02-0720_24_17.938535\\model-00010-1.60949-0.18854-1.60576-0.23000.h5\n",
      "83/83 [==============================] - 126s 2s/step - loss: 1.6095 - categorical_accuracy: 0.1885 - val_loss: 1.6058 - val_categorical_accuracy: 0.2300 - lr: 5.0000e-04\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6089 - categorical_accuracy: 0.2142\n",
      "Epoch 11: saving model to model_init_2023-02-0720_24_17.938535\\model-00011-1.60894-0.21418-1.60657-0.26000.h5\n",
      "83/83 [==============================] - 126s 2s/step - loss: 1.6089 - categorical_accuracy: 0.2142 - val_loss: 1.6066 - val_categorical_accuracy: 0.2600 - lr: 2.5000e-04\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6090 - categorical_accuracy: 0.2202\n",
      "Epoch 12: saving model to model_init_2023-02-0720_24_17.938535\\model-00012-1.60900-0.22021-1.60528-0.23000.h5\n",
      "83/83 [==============================] - 126s 2s/step - loss: 1.6090 - categorical_accuracy: 0.2202 - val_loss: 1.6053 - val_categorical_accuracy: 0.2300 - lr: 2.5000e-04\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6097 - categorical_accuracy: 0.1916\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 13: saving model to model_init_2023-02-0720_24_17.938535\\model-00013-1.60970-0.19155-1.60690-0.24000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 1.6097 - categorical_accuracy: 0.1916 - val_loss: 1.6069 - val_categorical_accuracy: 0.2400 - lr: 2.5000e-04\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6083 - categorical_accuracy: 0.1976\n",
      "Epoch 14: saving model to model_init_2023-02-0720_24_17.938535\\model-00014-1.60828-0.19759-1.60817-0.24000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 1.6083 - categorical_accuracy: 0.1976 - val_loss: 1.6082 - val_categorical_accuracy: 0.2400 - lr: 1.2500e-04\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6081 - categorical_accuracy: 0.2157\n",
      "Epoch 15: saving model to model_init_2023-02-0720_24_17.938535\\model-00015-1.60814-0.21569-1.60827-0.22000.h5\n",
      "83/83 [==============================] - 126s 2s/step - loss: 1.6081 - categorical_accuracy: 0.2157 - val_loss: 1.6083 - val_categorical_accuracy: 0.2200 - lr: 1.2500e-04\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6094 - categorical_accuracy: 0.1961\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 16: saving model to model_init_2023-02-0720_24_17.938535\\model-00016-1.60935-0.19608-1.60591-0.25000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 1.6094 - categorical_accuracy: 0.1961 - val_loss: 1.6059 - val_categorical_accuracy: 0.2500 - lr: 1.2500e-04\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6082 - categorical_accuracy: 0.2293\n",
      "Epoch 17: saving model to model_init_2023-02-0720_24_17.938535\\model-00017-1.60817-0.22926-1.60749-0.16000.h5\n",
      "83/83 [==============================] - 127s 2s/step - loss: 1.6082 - categorical_accuracy: 0.2293 - val_loss: 1.6075 - val_categorical_accuracy: 0.1600 - lr: 6.2500e-05\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6085 - categorical_accuracy: 0.2232\n",
      "Epoch 18: saving model to model_init_2023-02-0720_24_17.938535\\model-00018-1.60851-0.22323-1.60679-0.26000.h5\n",
      "83/83 [==============================] - 126s 2s/step - loss: 1.6085 - categorical_accuracy: 0.2232 - val_loss: 1.6068 - val_categorical_accuracy: 0.2600 - lr: 6.2500e-05\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6093 - categorical_accuracy: 0.1810\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 19: saving model to model_init_2023-02-0720_24_17.938535\\model-00019-1.60932-0.18100-1.60656-0.21000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 1.6093 - categorical_accuracy: 0.1810 - val_loss: 1.6066 - val_categorical_accuracy: 0.2100 - lr: 6.2500e-05\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6083 - categorical_accuracy: 0.2247\n",
      "Epoch 20: saving model to model_init_2023-02-0720_24_17.938535\\model-00020-1.60833-0.22474-1.60613-0.21000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 1.6083 - categorical_accuracy: 0.2247 - val_loss: 1.6061 - val_categorical_accuracy: 0.2100 - lr: 3.1250e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken to Train this model was :  0:41:53.345990\n"
     ]
    }
   ],
   "source": [
    "batch_size=8\n",
    "trainmodel(model_vgg19,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg19.save(\"model_VGG19.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimenting with lesser trainable layers from the pretrained VGG19 model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG19_model = VGG19(weights='imagenet', include_top=False)\n",
    "\n",
    "# Experimenting with the same model as above but with only 5 layer as trainable\n",
    "for layer in VGG19_model.layers[:-5]:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 12 VGG19 -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "## Added GlobalAveragePooling3D and a dense layer of 1024 neurons followed by a dropout of 0.5\n",
    "model_vgg19_2 = Sequential()\n",
    "model_vgg19_2.add(TimeDistributed(VGG19_model,input_shape=(30,120,120,3)))\n",
    "model_vgg19_2.add(GlobalAveragePooling3D())\n",
    "model_vgg19_2.add(Dense(1024, activation='relu'))\n",
    "model_vgg19_2.add(Dropout(0.5))\n",
    "model_vgg19_2.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_31 (TimeDi  (None, 30, 3, 3, 512)    20024384  \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " global_average_pooling3d_4   (None, 512)              0         \n",
      " (GlobalAveragePooling3D)                                        \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 1024)              525312    \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 5)                 5125      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,554,821\n",
      "Trainable params: 9,969,669\n",
      "Non-trainable params: 10,585,152\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Compiling the model using Adam as optimizer\n",
    "model_vgg19_2.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_vgg19_2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________\n",
      "Batch Size is : 8\n",
      "\n",
      "Source path =  datasets/Project_data/train ; batch size = 8\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6821 - categorical_accuracy: 0.1840\n",
      "Source path =  datasets/Project_data/val ; batch size = 8\n",
      "\n",
      "Epoch 1: saving model to model_init_2023-02-0720_24_17.938535\\model-00001-1.68210-0.18401-1.60603-0.23000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 1.6821 - categorical_accuracy: 0.1840 - val_loss: 1.6060 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6100 - categorical_accuracy: 0.2051\n",
      "Epoch 2: saving model to model_init_2023-02-0720_24_17.938535\\model-00002-1.60995-0.20513-1.60511-0.25000.h5\n",
      "83/83 [==============================] - 124s 2s/step - loss: 1.6100 - categorical_accuracy: 0.2051 - val_loss: 1.6051 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6101 - categorical_accuracy: 0.1976\n",
      "Epoch 3: saving model to model_init_2023-02-0720_24_17.938535\\model-00003-1.61011-0.19759-1.60493-0.23000.h5\n",
      "83/83 [==============================] - 126s 2s/step - loss: 1.6101 - categorical_accuracy: 0.1976 - val_loss: 1.6049 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6095 - categorical_accuracy: 0.2051\n",
      "Epoch 4: saving model to model_init_2023-02-0720_24_17.938535\\model-00004-1.60950-0.20513-1.60854-0.23000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 1.6095 - categorical_accuracy: 0.2051 - val_loss: 1.6085 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6097 - categorical_accuracy: 0.1976\n",
      "Epoch 5: saving model to model_init_2023-02-0720_24_17.938535\\model-00005-1.60968-0.19759-1.60979-0.19000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 1.6097 - categorical_accuracy: 0.1976 - val_loss: 1.6098 - val_categorical_accuracy: 0.1900 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6091 - categorical_accuracy: 0.2021\n",
      "Epoch 6: saving model to model_init_2023-02-0720_24_17.938535\\model-00006-1.60909-0.20211-1.60406-0.20000.h5\n",
      "83/83 [==============================] - 126s 2s/step - loss: 1.6091 - categorical_accuracy: 0.2021 - val_loss: 1.6041 - val_categorical_accuracy: 0.2000 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6095 - categorical_accuracy: 0.2006\n",
      "Epoch 7: saving model to model_init_2023-02-0720_24_17.938535\\model-00007-1.60946-0.20060-1.60720-0.21000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 1.6095 - categorical_accuracy: 0.2006 - val_loss: 1.6072 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6095 - categorical_accuracy: 0.2066\n",
      "Epoch 8: saving model to model_init_2023-02-0720_24_17.938535\\model-00008-1.60948-0.20664-1.60753-0.22000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 1.6095 - categorical_accuracy: 0.2066 - val_loss: 1.6075 - val_categorical_accuracy: 0.2200 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6082 - categorical_accuracy: 0.2066\n",
      "Epoch 9: saving model to model_init_2023-02-0720_24_17.938535\\model-00009-1.60824-0.20664-1.60050-0.24000.h5\n",
      "83/83 [==============================] - 126s 2s/step - loss: 1.6082 - categorical_accuracy: 0.2066 - val_loss: 1.6005 - val_categorical_accuracy: 0.2400 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6099 - categorical_accuracy: 0.1976\n",
      "Epoch 10: saving model to model_init_2023-02-0720_24_17.938535\\model-00010-1.60988-0.19759-1.60659-0.27000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 1.6099 - categorical_accuracy: 0.1976 - val_loss: 1.6066 - val_categorical_accuracy: 0.2700 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6094 - categorical_accuracy: 0.1840\n",
      "Epoch 11: saving model to model_init_2023-02-0720_24_17.938535\\model-00011-1.60942-0.18401-1.60774-0.25000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 1.6094 - categorical_accuracy: 0.1840 - val_loss: 1.6077 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6088 - categorical_accuracy: 0.2006\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 12: saving model to model_init_2023-02-0720_24_17.938535\\model-00012-1.60881-0.20060-1.60935-0.18000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 1.6088 - categorical_accuracy: 0.2006 - val_loss: 1.6094 - val_categorical_accuracy: 0.1800 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6075 - categorical_accuracy: 0.2202\n",
      "Epoch 13: saving model to model_init_2023-02-0720_24_17.938535\\model-00013-1.60750-0.22021-1.60584-0.21000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 1.6075 - categorical_accuracy: 0.2202 - val_loss: 1.6058 - val_categorical_accuracy: 0.2100 - lr: 5.0000e-04\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6085 - categorical_accuracy: 0.1870\n",
      "Epoch 14: saving model to model_init_2023-02-0720_24_17.938535\\model-00014-1.60854-0.18703-1.60836-0.21000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 1.6085 - categorical_accuracy: 0.1870 - val_loss: 1.6084 - val_categorical_accuracy: 0.2100 - lr: 5.0000e-04\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6087 - categorical_accuracy: 0.2006\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 15: saving model to model_init_2023-02-0720_24_17.938535\\model-00015-1.60875-0.20060-1.60359-0.26000.h5\n",
      "83/83 [==============================] - 126s 2s/step - loss: 1.6087 - categorical_accuracy: 0.2006 - val_loss: 1.6036 - val_categorical_accuracy: 0.2600 - lr: 5.0000e-04\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6098 - categorical_accuracy: 0.2006\n",
      "Epoch 16: saving model to model_init_2023-02-0720_24_17.938535\\model-00016-1.60984-0.20060-1.60995-0.21000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 1.6098 - categorical_accuracy: 0.2006 - val_loss: 1.6099 - val_categorical_accuracy: 0.2100 - lr: 2.5000e-04\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6086 - categorical_accuracy: 0.2021\n",
      "Epoch 17: saving model to model_init_2023-02-0720_24_17.938535\\model-00017-1.60862-0.20211-1.60595-0.25000.h5\n",
      "83/83 [==============================] - 126s 2s/step - loss: 1.6086 - categorical_accuracy: 0.2021 - val_loss: 1.6059 - val_categorical_accuracy: 0.2500 - lr: 2.5000e-04\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6092 - categorical_accuracy: 0.1870\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 18: saving model to model_init_2023-02-0720_24_17.938535\\model-00018-1.60917-0.18703-1.60939-0.24000.h5\n",
      "83/83 [==============================] - 126s 2s/step - loss: 1.6092 - categorical_accuracy: 0.1870 - val_loss: 1.6094 - val_categorical_accuracy: 0.2400 - lr: 2.5000e-04\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6090 - categorical_accuracy: 0.1961\n",
      "Epoch 19: saving model to model_init_2023-02-0720_24_17.938535\\model-00019-1.60899-0.19608-1.60534-0.22000.h5\n",
      "83/83 [==============================] - 126s 2s/step - loss: 1.6090 - categorical_accuracy: 0.1961 - val_loss: 1.6053 - val_categorical_accuracy: 0.2200 - lr: 1.2500e-04\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6101 - categorical_accuracy: 0.2051\n",
      "Epoch 20: saving model to model_init_2023-02-0720_24_17.938535\\model-00020-1.61013-0.20513-1.60970-0.15000.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 1.6101 - categorical_accuracy: 0.2051 - val_loss: 1.6097 - val_categorical_accuracy: 0.1500 - lr: 1.2500e-04\n",
      "Time Taken to Train this model was :  0:41:46.871528\n"
     ]
    }
   ],
   "source": [
    "batch_size=8\n",
    "trainmodel(model_vgg19_2,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg19_2.save(\"model_vgg19_2.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Size Experiments\n",
    "#### Experimenting with reduced batch size for all models above that gave a validation accuracy of >60%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We will keep all parameters and architecture same as the original models used for experiment and reduce the batch size to half of what was originally used to test for difference in Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Batch size experiment with model_cnn_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the model_cnn_2 architecture with batch size of 4 because we used 8 in the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture - Same as model_cnn_2\n",
    "#Initial Filter of (3,3,3) and maxpooling with constant pool size of (3,3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 13 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture for model_cnn_3\n",
    "Input_shape = (len(img_idx), imgdim, imgdim, 3)\n",
    "\n",
    "model_cnn_3 = Sequential()\n",
    "model_cnn_3.add(Conv3D(16, (3,3,3),input_shape=Input_shape,padding=\"same\"))\n",
    "model_cnn_3.add(BatchNormalization())\n",
    "model_cnn_3.add(Activation('relu'))\n",
    "model_cnn_3.add(MaxPooling3D(pool_size=(3,3,3)))\n",
    "\n",
    "model_cnn_3.add(Conv3D(32,(3,3,3),padding='same'))\n",
    "model_cnn_3.add(BatchNormalization())\n",
    "model_cnn_3.add(Activation('relu'))\n",
    "model_cnn_3.add(MaxPooling3D(pool_size=(3,3,3)))\n",
    "model_cnn_3.add(layers.Dropout(0.25))\n",
    "\n",
    "model_cnn_3.add(Conv3D(64,(3,3,3),padding='same'))\n",
    "model_cnn_3.add(BatchNormalization())\n",
    "model_cnn_3.add(Activation('relu'))\n",
    "model_cnn_3.add(MaxPooling3D(pool_size=(3,3,3)))\n",
    "\n",
    "\n",
    "model_cnn_3.add(Flatten())\n",
    "model_cnn_3.add(Dense(512, activation='relu'))\n",
    "model_cnn_3.add(layers.Dropout(0.5))\n",
    "\n",
    "model_cnn_3.add(Dense(5,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T06:20:07.757837Z",
     "iopub.status.busy": "2021-10-26T06:20:07.757588Z",
     "iopub.status.idle": "2021-10-26T06:20:07.929675Z",
     "shell.execute_reply": "2021-10-26T06:20:07.929025Z",
     "shell.execute_reply.started": "2021-10-26T06:20:07.757808Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_3 (Conv3D)           (None, 30, 120, 120, 16)  1312      \n",
      "                                                                 \n",
      " batch_normalization_199 (Ba  (None, 30, 120, 120, 16)  64       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_191 (Activation)  (None, 30, 120, 120, 16)  0        \n",
      "                                                                 \n",
      " max_pooling3d_3 (MaxPooling  (None, 10, 40, 40, 16)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_4 (Conv3D)           (None, 10, 40, 40, 32)    13856     \n",
      "                                                                 \n",
      " batch_normalization_200 (Ba  (None, 10, 40, 40, 32)   128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_192 (Activation)  (None, 10, 40, 40, 32)   0         \n",
      "                                                                 \n",
      " max_pooling3d_4 (MaxPooling  (None, 3, 13, 13, 32)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 3, 13, 13, 32)     0         \n",
      "                                                                 \n",
      " conv3d_5 (Conv3D)           (None, 3, 13, 13, 64)     55360     \n",
      "                                                                 \n",
      " batch_normalization_201 (Ba  (None, 3, 13, 13, 64)    256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_193 (Activation)  (None, 3, 13, 13, 64)    0         \n",
      "                                                                 \n",
      " max_pooling3d_5 (MaxPooling  (None, 1, 4, 4, 64)      0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 598,341\n",
      "Trainable params: 598,117\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Compiling the model using Adam as optimizer\n",
    "model_cnn_3.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_cnn_3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________\n",
      "Batch Size is : 4\n",
      "\n",
      "Source path =  datasets/Project_data/train ; batch size = 4\n",
      "Epoch 1/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 2.9354 - categorical_accuracy: 0.2489\n",
      "Source path =  datasets/Project_data/val ; batch size = 4\n",
      "\n",
      "Epoch 1: saving model to model_init_2023-02-0906_58_43.961462\\model-00001-2.93538-0.24887-2.21123-0.21000.h5\n",
      "166/166 [==============================] - 121s 730ms/step - loss: 2.9354 - categorical_accuracy: 0.2489 - val_loss: 2.2112 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.4077 - categorical_accuracy: 0.3982\n",
      "Epoch 2: saving model to model_init_2023-02-0906_58_43.961462\\model-00002-1.40769-0.39819-1.82684-0.24000.h5\n",
      "166/166 [==============================] - 120s 726ms/step - loss: 1.4077 - categorical_accuracy: 0.3982 - val_loss: 1.8268 - val_categorical_accuracy: 0.2400 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.2729 - categorical_accuracy: 0.4600\n",
      "Epoch 3: saving model to model_init_2023-02-0906_58_43.961462\\model-00003-1.27293-0.46003-2.05043-0.29000.h5\n",
      "166/166 [==============================] - 120s 726ms/step - loss: 1.2729 - categorical_accuracy: 0.4600 - val_loss: 2.0504 - val_categorical_accuracy: 0.2900 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.1030 - categorical_accuracy: 0.5581\n",
      "Epoch 4: saving model to model_init_2023-02-0906_58_43.961462\\model-00004-1.10301-0.55807-0.93977-0.61000.h5\n",
      "166/166 [==============================] - 120s 727ms/step - loss: 1.1030 - categorical_accuracy: 0.5581 - val_loss: 0.9398 - val_categorical_accuracy: 0.6100 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.9638 - categorical_accuracy: 0.6214\n",
      "Epoch 5: saving model to model_init_2023-02-0906_58_43.961462\\model-00005-0.96385-0.62142-0.81024-0.71000.h5\n",
      "166/166 [==============================] - 120s 726ms/step - loss: 0.9638 - categorical_accuracy: 0.6214 - val_loss: 0.8102 - val_categorical_accuracy: 0.7100 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.7205 - categorical_accuracy: 0.7466\n",
      "Epoch 6: saving model to model_init_2023-02-0906_58_43.961462\\model-00006-0.72052-0.74661-1.06313-0.67000.h5\n",
      "166/166 [==============================] - 120s 727ms/step - loss: 0.7205 - categorical_accuracy: 0.7466 - val_loss: 1.0631 - val_categorical_accuracy: 0.6700 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.6376 - categorical_accuracy: 0.7572\n",
      "Epoch 7: saving model to model_init_2023-02-0906_58_43.961462\\model-00007-0.63763-0.75716-1.56988-0.40000.h5\n",
      "166/166 [==============================] - 120s 723ms/step - loss: 0.6376 - categorical_accuracy: 0.7572 - val_loss: 1.5699 - val_categorical_accuracy: 0.4000 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5231 - categorical_accuracy: 0.8069\n",
      "Epoch 8: saving model to model_init_2023-02-0906_58_43.961462\\model-00008-0.52309-0.80694-0.64213-0.72000.h5\n",
      "166/166 [==============================] - 119s 722ms/step - loss: 0.5231 - categorical_accuracy: 0.8069 - val_loss: 0.6421 - val_categorical_accuracy: 0.7200 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.4680 - categorical_accuracy: 0.8220\n",
      "Epoch 9: saving model to model_init_2023-02-0906_58_43.961462\\model-00009-0.46795-0.82202-1.06044-0.63000.h5\n",
      "166/166 [==============================] - 119s 722ms/step - loss: 0.4680 - categorical_accuracy: 0.8220 - val_loss: 1.0604 - val_categorical_accuracy: 0.6300 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.3922 - categorical_accuracy: 0.8567\n",
      "Epoch 10: saving model to model_init_2023-02-0906_58_43.961462\\model-00010-0.39222-0.85671-0.47417-0.83000.h5\n",
      "166/166 [==============================] - 121s 731ms/step - loss: 0.3922 - categorical_accuracy: 0.8567 - val_loss: 0.4742 - val_categorical_accuracy: 0.8300 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.3473 - categorical_accuracy: 0.8643\n",
      "Epoch 11: saving model to model_init_2023-02-0906_58_43.961462\\model-00011-0.34730-0.86425-0.72785-0.76000.h5\n",
      "166/166 [==============================] - 119s 721ms/step - loss: 0.3473 - categorical_accuracy: 0.8643 - val_loss: 0.7279 - val_categorical_accuracy: 0.7600 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.4360 - categorical_accuracy: 0.8431\n",
      "Epoch 12: saving model to model_init_2023-02-0906_58_43.961462\\model-00012-0.43600-0.84314-0.77899-0.75000.h5\n",
      "166/166 [==============================] - 120s 727ms/step - loss: 0.4360 - categorical_accuracy: 0.8431 - val_loss: 0.7790 - val_categorical_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.3154 - categorical_accuracy: 0.8914\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 13: saving model to model_init_2023-02-0906_58_43.961462\\model-00013-0.31541-0.89140-0.99961-0.74000.h5\n",
      "166/166 [==============================] - 120s 724ms/step - loss: 0.3154 - categorical_accuracy: 0.8914 - val_loss: 0.9996 - val_categorical_accuracy: 0.7400 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1888 - categorical_accuracy: 0.9291\n",
      "Epoch 14: saving model to model_init_2023-02-0906_58_43.961462\\model-00014-0.18883-0.92911-0.32273-0.91000.h5\n",
      "166/166 [==============================] - 119s 723ms/step - loss: 0.1888 - categorical_accuracy: 0.9291 - val_loss: 0.3227 - val_categorical_accuracy: 0.9100 - lr: 5.0000e-04\n",
      "Epoch 15/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1351 - categorical_accuracy: 0.9563\n",
      "Epoch 15: saving model to model_init_2023-02-0906_58_43.961462\\model-00015-0.13513-0.95626-0.36689-0.88000.h5\n",
      "166/166 [==============================] - 120s 726ms/step - loss: 0.1351 - categorical_accuracy: 0.9563 - val_loss: 0.3669 - val_categorical_accuracy: 0.8800 - lr: 5.0000e-04\n",
      "Epoch 16/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1763 - categorical_accuracy: 0.9397\n",
      "Epoch 16: saving model to model_init_2023-02-0906_58_43.961462\\model-00016-0.17635-0.93967-0.28011-0.92000.h5\n",
      "166/166 [==============================] - 120s 727ms/step - loss: 0.1763 - categorical_accuracy: 0.9397 - val_loss: 0.2801 - val_categorical_accuracy: 0.9200 - lr: 5.0000e-04\n",
      "Epoch 17/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1283 - categorical_accuracy: 0.9517\n",
      "Epoch 17: saving model to model_init_2023-02-0906_58_43.961462\\model-00017-0.12828-0.95173-0.34347-0.89000.h5\n",
      "166/166 [==============================] - 120s 724ms/step - loss: 0.1283 - categorical_accuracy: 0.9517 - val_loss: 0.3435 - val_categorical_accuracy: 0.8900 - lr: 5.0000e-04\n",
      "Epoch 18/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1111 - categorical_accuracy: 0.9608\n",
      "Epoch 18: saving model to model_init_2023-02-0906_58_43.961462\\model-00018-0.11114-0.96078-0.99978-0.80000.h5\n",
      "166/166 [==============================] - 120s 726ms/step - loss: 0.1111 - categorical_accuracy: 0.9608 - val_loss: 0.9998 - val_categorical_accuracy: 0.8000 - lr: 5.0000e-04\n",
      "Epoch 19/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1336 - categorical_accuracy: 0.9532\n",
      "Epoch 19: saving model to model_init_2023-02-0906_58_43.961462\\model-00019-0.13360-0.95324-0.20051-0.94000.h5\n",
      "166/166 [==============================] - 120s 724ms/step - loss: 0.1336 - categorical_accuracy: 0.9532 - val_loss: 0.2005 - val_categorical_accuracy: 0.9400 - lr: 5.0000e-04\n",
      "Epoch 20/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1683 - categorical_accuracy: 0.9382\n",
      "Epoch 20: saving model to model_init_2023-02-0906_58_43.961462\\model-00020-0.16828-0.93816-0.23085-0.92000.h5\n",
      "166/166 [==============================] - 119s 722ms/step - loss: 0.1683 - categorical_accuracy: 0.9382 - val_loss: 0.2309 - val_categorical_accuracy: 0.9200 - lr: 5.0000e-04\n",
      "Time Taken to Train this model was :  0:39:57.818460\n"
     ]
    }
   ],
   "source": [
    "batch_size=4\n",
    "trainmodel(model_cnn_3,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_3.save(\"model_cnn_3.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Batch size experiment with model_cnn_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the model_cnn_LSTM architecture with batch size of 4 because we used 8 in the \n",
    "# original model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture for model_cnn_LSTM_2\n",
    "Input_shape = (len(img_idx), imgdim, imgdim, 3)\n",
    "\n",
    "model_cnn_LSTM_2 = Sequential()\n",
    "\n",
    "model_cnn_LSTM_2.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'),input_shape=Input_shape))\n",
    "model_cnn_LSTM_2.add(TimeDistributed(BatchNormalization()))\n",
    "model_cnn_LSTM_2.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model_cnn_LSTM_2.add(Dropout(0.25))\n",
    "\n",
    "        \n",
    "model_cnn_LSTM_2.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n",
    "model_cnn_LSTM_2.add(TimeDistributed(BatchNormalization()))\n",
    "model_cnn_LSTM_2.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model_cnn_LSTM_2.add(Dropout(0.25))\n",
    "        \n",
    "model_cnn_LSTM_2.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\n",
    "model_cnn_LSTM_2.add(TimeDistributed(BatchNormalization()))\n",
    "model_cnn_LSTM_2.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model_cnn_LSTM_2.add(Dropout(0.25))\n",
    "        \n",
    "model_cnn_LSTM_2.add(TimeDistributed(Conv2D(128, (3, 3) , padding='same', activation='relu')))\n",
    "model_cnn_LSTM_2.add(TimeDistributed(BatchNormalization()))\n",
    "model_cnn_LSTM_2.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model_cnn_LSTM_2.add(Dropout(0.25))\n",
    "          \n",
    "\n",
    "model_cnn_LSTM_2.add(TimeDistributed(Flatten()))\n",
    "model_cnn_LSTM_2.add(Dropout(0.25))\n",
    "\n",
    "model_cnn_LSTM_2.add(LSTM(1024))\n",
    "model_cnn_LSTM_2.add(Dropout(0.5))\n",
    "\n",
    "model_cnn_LSTM_2.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T06:20:07.757837Z",
     "iopub.status.busy": "2021-10-26T06:20:07.757588Z",
     "iopub.status.idle": "2021-10-26T06:20:07.929675Z",
     "shell.execute_reply": "2021-10-26T06:20:07.929025Z",
     "shell.execute_reply.started": "2021-10-26T06:20:07.757808Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_28 (TimeDi  (None, 30, 120, 120, 16)  448      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_29 (TimeDi  (None, 30, 120, 120, 16)  64       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_30 (TimeDi  (None, 30, 60, 60, 16)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 30, 60, 60, 16)    0         \n",
      "                                                                 \n",
      " time_distributed_31 (TimeDi  (None, 30, 60, 60, 32)   4640      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_32 (TimeDi  (None, 30, 60, 60, 32)   128       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_33 (TimeDi  (None, 30, 30, 30, 32)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 30, 30, 30, 32)    0         \n",
      "                                                                 \n",
      " time_distributed_34 (TimeDi  (None, 30, 30, 30, 64)   18496     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_35 (TimeDi  (None, 30, 30, 30, 64)   256       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_36 (TimeDi  (None, 30, 15, 15, 64)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 30, 15, 15, 64)    0         \n",
      "                                                                 \n",
      " time_distributed_37 (TimeDi  (None, 30, 15, 15, 128)  73856     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_38 (TimeDi  (None, 30, 15, 15, 128)  512       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_39 (TimeDi  (None, 30, 7, 7, 128)    0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 30, 7, 7, 128)     0         \n",
      "                                                                 \n",
      " time_distributed_40 (TimeDi  (None, 30, 6272)         0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 30, 6272)          0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 1024)              29888512  \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 5)                 5125      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29,992,037\n",
      "Trainable params: 29,991,557\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Compiling the model using Adam as optimizer\n",
    "model_cnn_LSTM_2.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_cnn_LSTM_2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________\n",
      "Batch Size is : 4\n",
      "\n",
      "Source path =  datasets/Project_data/train ; batch size = 4\n",
      "Epoch 1/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.7162 - categorical_accuracy: 0.3288\n",
      "Source path =  datasets/Project_data/val ; batch size = 4\n",
      "\n",
      "Epoch 1: saving model to model_init_2023-02-0906_58_43.961462\\model-00001-1.71616-0.32881-3.04865-0.21000.h5\n",
      "166/166 [==============================] - 122s 730ms/step - loss: 1.7162 - categorical_accuracy: 0.3288 - val_loss: 3.0486 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.2964 - categorical_accuracy: 0.5204\n",
      "Epoch 2: saving model to model_init_2023-02-0906_58_43.961462\\model-00002-1.29641-0.52036-3.26711-0.22000.h5\n",
      "166/166 [==============================] - 120s 727ms/step - loss: 1.2964 - categorical_accuracy: 0.5204 - val_loss: 3.2671 - val_categorical_accuracy: 0.2200 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.1523 - categorical_accuracy: 0.5611\n",
      "Epoch 3: saving model to model_init_2023-02-0906_58_43.961462\\model-00003-1.15232-0.56109-3.11664-0.22000.h5\n",
      "166/166 [==============================] - 120s 728ms/step - loss: 1.1523 - categorical_accuracy: 0.5611 - val_loss: 3.1166 - val_categorical_accuracy: 0.2200 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.0260 - categorical_accuracy: 0.6290\n",
      "Epoch 4: saving model to model_init_2023-02-0906_58_43.961462\\model-00004-1.02597-0.62896-1.24388-0.51000.h5\n",
      "166/166 [==============================] - 121s 731ms/step - loss: 1.0260 - categorical_accuracy: 0.6290 - val_loss: 1.2439 - val_categorical_accuracy: 0.5100 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.0065 - categorical_accuracy: 0.6124\n",
      "Epoch 5: saving model to model_init_2023-02-0906_58_43.961462\\model-00005-1.00654-0.61237-1.02130-0.56000.h5\n",
      "166/166 [==============================] - 121s 733ms/step - loss: 1.0065 - categorical_accuracy: 0.6124 - val_loss: 1.0213 - val_categorical_accuracy: 0.5600 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.8634 - categorical_accuracy: 0.6908\n",
      "Epoch 6: saving model to model_init_2023-02-0906_58_43.961462\\model-00006-0.86339-0.69080-1.25639-0.52000.h5\n",
      "166/166 [==============================] - 120s 727ms/step - loss: 0.8634 - categorical_accuracy: 0.6908 - val_loss: 1.2564 - val_categorical_accuracy: 0.5200 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.8666 - categorical_accuracy: 0.6833\n",
      "Epoch 7: saving model to model_init_2023-02-0906_58_43.961462\\model-00007-0.86662-0.68326-1.04556-0.55000.h5\n",
      "166/166 [==============================] - 120s 727ms/step - loss: 0.8666 - categorical_accuracy: 0.6833 - val_loss: 1.0456 - val_categorical_accuracy: 0.5500 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.6457 - categorical_accuracy: 0.7647\n",
      "Epoch 8: saving model to model_init_2023-02-0906_58_43.961462\\model-00008-0.64570-0.76471-0.87178-0.73000.h5\n",
      "166/166 [==============================] - 121s 729ms/step - loss: 0.6457 - categorical_accuracy: 0.7647 - val_loss: 0.8718 - val_categorical_accuracy: 0.7300 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.6027 - categorical_accuracy: 0.7843\n",
      "Epoch 9: saving model to model_init_2023-02-0906_58_43.961462\\model-00009-0.60269-0.78431-1.23092-0.50000.h5\n",
      "166/166 [==============================] - 121s 735ms/step - loss: 0.6027 - categorical_accuracy: 0.7843 - val_loss: 1.2309 - val_categorical_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5628 - categorical_accuracy: 0.7858\n",
      "Epoch 10: saving model to model_init_2023-02-0906_58_43.961462\\model-00010-0.56277-0.78582-0.45294-0.84000.h5\n",
      "166/166 [==============================] - 120s 728ms/step - loss: 0.5628 - categorical_accuracy: 0.7858 - val_loss: 0.4529 - val_categorical_accuracy: 0.8400 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5140 - categorical_accuracy: 0.8145\n",
      "Epoch 11: saving model to model_init_2023-02-0906_58_43.961462\\model-00011-0.51399-0.81448-0.78149-0.72000.h5\n",
      "166/166 [==============================] - 120s 727ms/step - loss: 0.5140 - categorical_accuracy: 0.8145 - val_loss: 0.7815 - val_categorical_accuracy: 0.7200 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.4814 - categorical_accuracy: 0.8401\n",
      "Epoch 12: saving model to model_init_2023-02-0906_58_43.961462\\model-00012-0.48135-0.84012-0.62886-0.78000.h5\n",
      "166/166 [==============================] - 120s 728ms/step - loss: 0.4814 - categorical_accuracy: 0.8401 - val_loss: 0.6289 - val_categorical_accuracy: 0.7800 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.3983 - categorical_accuracy: 0.8537\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 13: saving model to model_init_2023-02-0906_58_43.961462\\model-00013-0.39832-0.85370-0.46320-0.87000.h5\n",
      "166/166 [==============================] - 121s 730ms/step - loss: 0.3983 - categorical_accuracy: 0.8537 - val_loss: 0.4632 - val_categorical_accuracy: 0.8700 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2816 - categorical_accuracy: 0.8974\n",
      "Epoch 14: saving model to model_init_2023-02-0906_58_43.961462\\model-00014-0.28157-0.89744-0.57993-0.78000.h5\n",
      "166/166 [==============================] - 120s 723ms/step - loss: 0.2816 - categorical_accuracy: 0.8974 - val_loss: 0.5799 - val_categorical_accuracy: 0.7800 - lr: 5.0000e-04\n",
      "Epoch 15/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2188 - categorical_accuracy: 0.9216\n",
      "Epoch 15: saving model to model_init_2023-02-0906_58_43.961462\\model-00015-0.21877-0.92157-0.98405-0.64000.h5\n",
      "166/166 [==============================] - 120s 728ms/step - loss: 0.2188 - categorical_accuracy: 0.9216 - val_loss: 0.9840 - val_categorical_accuracy: 0.6400 - lr: 5.0000e-04\n",
      "Epoch 16/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1672 - categorical_accuracy: 0.9382\n",
      "Epoch 16: saving model to model_init_2023-02-0906_58_43.961462\\model-00016-0.16722-0.93816-0.43959-0.87000.h5\n",
      "166/166 [==============================] - 120s 727ms/step - loss: 0.1672 - categorical_accuracy: 0.9382 - val_loss: 0.4396 - val_categorical_accuracy: 0.8700 - lr: 5.0000e-04\n",
      "Epoch 17/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1291 - categorical_accuracy: 0.9638\n",
      "Epoch 17: saving model to model_init_2023-02-0906_58_43.961462\\model-00017-0.12911-0.96380-0.48668-0.84000.h5\n",
      "166/166 [==============================] - 120s 726ms/step - loss: 0.1291 - categorical_accuracy: 0.9638 - val_loss: 0.4867 - val_categorical_accuracy: 0.8400 - lr: 5.0000e-04\n",
      "Epoch 18/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1111 - categorical_accuracy: 0.9729\n",
      "Epoch 18: saving model to model_init_2023-02-0906_58_43.961462\\model-00018-0.11106-0.97285-0.46858-0.84000.h5\n",
      "166/166 [==============================] - 120s 725ms/step - loss: 0.1111 - categorical_accuracy: 0.9729 - val_loss: 0.4686 - val_categorical_accuracy: 0.8400 - lr: 5.0000e-04\n",
      "Epoch 19/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1030 - categorical_accuracy: 0.9668\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 19: saving model to model_init_2023-02-0906_58_43.961462\\model-00019-0.10302-0.96682-0.61862-0.79000.h5\n",
      "166/166 [==============================] - 120s 728ms/step - loss: 0.1030 - categorical_accuracy: 0.9668 - val_loss: 0.6186 - val_categorical_accuracy: 0.7900 - lr: 5.0000e-04\n",
      "Epoch 20/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0815 - categorical_accuracy: 0.9804\n",
      "Epoch 20: saving model to model_init_2023-02-0906_58_43.961462\\model-00020-0.08150-0.98039-0.41036-0.88000.h5\n",
      "166/166 [==============================] - 121s 730ms/step - loss: 0.0815 - categorical_accuracy: 0.9804 - val_loss: 0.4104 - val_categorical_accuracy: 0.8800 - lr: 2.5000e-04\n",
      "Time Taken to Train this model was :  0:40:09.751458\n"
     ]
    }
   ],
   "source": [
    "batch_size=4\n",
    "trainmodel(model_cnn_LSTM_2,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_LSTM_2.save(\"model_cnn_LSTM_2.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Batch size experiment with model_cnn_GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the model_cnn_GRU architecture with batch size of 4 because we used 8 in the \n",
    "# original model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture for model_cnn_GRU_2\n",
    "Input_shape = (len(img_idx), imgdim, imgdim, 3)\n",
    "\n",
    "model_cnn_GRU_2 = Sequential()\n",
    "\n",
    "model_cnn_GRU_2.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'),input_shape=Input_shape))\n",
    "model_cnn_GRU_2.add(TimeDistributed(BatchNormalization()))\n",
    "model_cnn_GRU_2.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model_cnn_GRU_2.add(Dropout(0.25))\n",
    "\n",
    "        \n",
    "model_cnn_GRU_2.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n",
    "model_cnn_GRU_2.add(TimeDistributed(BatchNormalization()))\n",
    "model_cnn_GRU_2.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model_cnn_GRU_2.add(Dropout(0.25))\n",
    "        \n",
    "model_cnn_GRU_2.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\n",
    "model_cnn_GRU_2.add(TimeDistributed(BatchNormalization()))\n",
    "model_cnn_GRU_2.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model_cnn_GRU_2.add(Dropout(0.25))\n",
    "        \n",
    "model_cnn_GRU_2.add(TimeDistributed(Conv2D(128, (3, 3) , padding='same', activation='relu')))\n",
    "model_cnn_GRU_2.add(TimeDistributed(BatchNormalization()))\n",
    "model_cnn_GRU_2.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model_cnn_GRU_2.add(Dropout(0.25))\n",
    "          \n",
    "\n",
    "model_cnn_GRU_2.add(TimeDistributed(Flatten()))\n",
    "model_cnn_GRU_2.add(Dropout(0.25))\n",
    "\n",
    "model_cnn_GRU_2.add(GRU(1024))\n",
    "model_cnn_GRU_2.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model_cnn_GRU_2.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T06:20:07.757837Z",
     "iopub.status.busy": "2021-10-26T06:20:07.757588Z",
     "iopub.status.idle": "2021-10-26T06:20:07.929675Z",
     "shell.execute_reply": "2021-10-26T06:20:07.929025Z",
     "shell.execute_reply.started": "2021-10-26T06:20:07.757808Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_41 (TimeDi  (None, 30, 120, 120, 16)  448      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_42 (TimeDi  (None, 30, 120, 120, 16)  64       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_43 (TimeDi  (None, 30, 60, 60, 16)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 30, 60, 60, 16)    0         \n",
      "                                                                 \n",
      " time_distributed_44 (TimeDi  (None, 30, 60, 60, 32)   4640      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_45 (TimeDi  (None, 30, 60, 60, 32)   128       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_46 (TimeDi  (None, 30, 30, 30, 32)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 30, 30, 30, 32)    0         \n",
      "                                                                 \n",
      " time_distributed_47 (TimeDi  (None, 30, 30, 30, 64)   18496     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_48 (TimeDi  (None, 30, 30, 30, 64)   256       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_49 (TimeDi  (None, 30, 15, 15, 64)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 30, 15, 15, 64)    0         \n",
      "                                                                 \n",
      " time_distributed_50 (TimeDi  (None, 30, 15, 15, 128)  73856     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_51 (TimeDi  (None, 30, 15, 15, 128)  512       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_52 (TimeDi  (None, 30, 7, 7, 128)    0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, 30, 7, 7, 128)     0         \n",
      "                                                                 \n",
      " time_distributed_53 (TimeDi  (None, 30, 6272)         0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (None, 30, 6272)          0         \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 1024)              22419456  \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 5)                 5125      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,522,981\n",
      "Trainable params: 22,522,501\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Compiling the model using Adam as optimizer\n",
    "model_cnn_GRU_2.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_cnn_GRU_2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________\n",
      "Batch Size is : 4\n",
      "\n",
      "Source path =  datasets/Project_data/train ; batch size = 4\n",
      "Epoch 1/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 2.5652 - categorical_accuracy: 0.2202\n",
      "Source path =  datasets/Project_data/val ; batch size = 4\n",
      "\n",
      "Epoch 1: saving model to model_init_2023-02-0906_58_43.961462\\model-00001-2.56520-0.22021-2.32625-0.19000.h5\n",
      "166/166 [==============================] - 122s 732ms/step - loss: 2.5652 - categorical_accuracy: 0.2202 - val_loss: 2.3263 - val_categorical_accuracy: 0.1900 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.6787 - categorical_accuracy: 0.3876\n",
      "Epoch 2: saving model to model_init_2023-02-0906_58_43.961462\\model-00002-1.67869-0.38763-5.06315-0.21000.h5\n",
      "166/166 [==============================] - 121s 730ms/step - loss: 1.6787 - categorical_accuracy: 0.3876 - val_loss: 5.0632 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.5301 - categorical_accuracy: 0.4419\n",
      "Epoch 3: saving model to model_init_2023-02-0906_58_43.961462\\model-00003-1.53012-0.44193-1.61995-0.41000.h5\n",
      "166/166 [==============================] - 121s 733ms/step - loss: 1.5301 - categorical_accuracy: 0.4419 - val_loss: 1.6200 - val_categorical_accuracy: 0.4100 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.3476 - categorical_accuracy: 0.5023\n",
      "Epoch 4: saving model to model_init_2023-02-0906_58_43.961462\\model-00004-1.34757-0.50226-2.00348-0.35000.h5\n",
      "166/166 [==============================] - 121s 730ms/step - loss: 1.3476 - categorical_accuracy: 0.5023 - val_loss: 2.0035 - val_categorical_accuracy: 0.3500 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.1132 - categorical_accuracy: 0.6154\n",
      "Epoch 5: saving model to model_init_2023-02-0906_58_43.961462\\model-00005-1.11324-0.61538-0.86695-0.75000.h5\n",
      "166/166 [==============================] - 120s 727ms/step - loss: 1.1132 - categorical_accuracy: 0.6154 - val_loss: 0.8670 - val_categorical_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.9066 - categorical_accuracy: 0.6546\n",
      "Epoch 6: saving model to model_init_2023-02-0906_58_43.961462\\model-00006-0.90658-0.65460-0.68141-0.74000.h5\n",
      "166/166 [==============================] - 120s 726ms/step - loss: 0.9066 - categorical_accuracy: 0.6546 - val_loss: 0.6814 - val_categorical_accuracy: 0.7400 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.8475 - categorical_accuracy: 0.6923\n",
      "Epoch 7: saving model to model_init_2023-02-0906_58_43.961462\\model-00007-0.84749-0.69231-0.70761-0.77000.h5\n",
      "166/166 [==============================] - 121s 733ms/step - loss: 0.8475 - categorical_accuracy: 0.6923 - val_loss: 0.7076 - val_categorical_accuracy: 0.7700 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.7149 - categorical_accuracy: 0.7511\n",
      "Epoch 8: saving model to model_init_2023-02-0906_58_43.961462\\model-00008-0.71494-0.75113-2.15445-0.32000.h5\n",
      "166/166 [==============================] - 121s 731ms/step - loss: 0.7149 - categorical_accuracy: 0.7511 - val_loss: 2.1544 - val_categorical_accuracy: 0.3200 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.6797 - categorical_accuracy: 0.7451\n",
      "Epoch 9: saving model to model_init_2023-02-0906_58_43.961462\\model-00009-0.67967-0.74510-0.60618-0.80000.h5\n",
      "166/166 [==============================] - 121s 731ms/step - loss: 0.6797 - categorical_accuracy: 0.7451 - val_loss: 0.6062 - val_categorical_accuracy: 0.8000 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.6278 - categorical_accuracy: 0.7677\n",
      "Epoch 10: saving model to model_init_2023-02-0906_58_43.961462\\model-00010-0.62778-0.76772-1.12115-0.63000.h5\n",
      "166/166 [==============================] - 121s 734ms/step - loss: 0.6278 - categorical_accuracy: 0.7677 - val_loss: 1.1211 - val_categorical_accuracy: 0.6300 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.6690 - categorical_accuracy: 0.7632\n",
      "Epoch 11: saving model to model_init_2023-02-0906_58_43.961462\\model-00011-0.66897-0.76320-0.80023-0.73000.h5\n",
      "166/166 [==============================] - 121s 728ms/step - loss: 0.6690 - categorical_accuracy: 0.7632 - val_loss: 0.8002 - val_categorical_accuracy: 0.7300 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5782 - categorical_accuracy: 0.7813\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 12: saving model to model_init_2023-02-0906_58_43.961462\\model-00012-0.57817-0.78130-0.73172-0.71000.h5\n",
      "166/166 [==============================] - 121s 730ms/step - loss: 0.5782 - categorical_accuracy: 0.7813 - val_loss: 0.7317 - val_categorical_accuracy: 0.7100 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.4361 - categorical_accuracy: 0.8371\n",
      "Epoch 13: saving model to model_init_2023-02-0906_58_43.961462\\model-00013-0.43606-0.83710-0.52733-0.79000.h5\n",
      "166/166 [==============================] - 122s 739ms/step - loss: 0.4361 - categorical_accuracy: 0.8371 - val_loss: 0.5273 - val_categorical_accuracy: 0.7900 - lr: 5.0000e-04\n",
      "Epoch 14/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.3107 - categorical_accuracy: 0.8703\n",
      "Epoch 14: saving model to model_init_2023-02-0906_58_43.961462\\model-00014-0.31066-0.87029-0.42960-0.85000.h5\n",
      "166/166 [==============================] - 122s 740ms/step - loss: 0.3107 - categorical_accuracy: 0.8703 - val_loss: 0.4296 - val_categorical_accuracy: 0.8500 - lr: 5.0000e-04\n",
      "Epoch 15/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2723 - categorical_accuracy: 0.9020\n",
      "Epoch 15: saving model to model_init_2023-02-0906_58_43.961462\\model-00015-0.27229-0.90196-0.48953-0.84000.h5\n",
      "166/166 [==============================] - 125s 758ms/step - loss: 0.2723 - categorical_accuracy: 0.9020 - val_loss: 0.4895 - val_categorical_accuracy: 0.8400 - lr: 5.0000e-04\n",
      "Epoch 16/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2455 - categorical_accuracy: 0.9140\n",
      "Epoch 16: saving model to model_init_2023-02-0906_58_43.961462\\model-00016-0.24553-0.91403-0.60197-0.72000.h5\n",
      "166/166 [==============================] - 123s 746ms/step - loss: 0.2455 - categorical_accuracy: 0.9140 - val_loss: 0.6020 - val_categorical_accuracy: 0.7200 - lr: 5.0000e-04\n",
      "Epoch 17/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2080 - categorical_accuracy: 0.9186\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 17: saving model to model_init_2023-02-0906_58_43.961462\\model-00017-0.20804-0.91855-0.47233-0.81000.h5\n",
      "166/166 [==============================] - 124s 749ms/step - loss: 0.2080 - categorical_accuracy: 0.9186 - val_loss: 0.4723 - val_categorical_accuracy: 0.8100 - lr: 5.0000e-04\n",
      "Epoch 18/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1883 - categorical_accuracy: 0.9367\n",
      "Epoch 18: saving model to model_init_2023-02-0906_58_43.961462\\model-00018-0.18835-0.93665-0.47191-0.81000.h5\n",
      "166/166 [==============================] - 122s 741ms/step - loss: 0.1883 - categorical_accuracy: 0.9367 - val_loss: 0.4719 - val_categorical_accuracy: 0.8100 - lr: 2.5000e-04\n",
      "Epoch 19/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1458 - categorical_accuracy: 0.9517\n",
      "Epoch 19: saving model to model_init_2023-02-0906_58_43.961462\\model-00019-0.14584-0.95173-0.32001-0.86000.h5\n",
      "166/166 [==============================] - 122s 741ms/step - loss: 0.1458 - categorical_accuracy: 0.9517 - val_loss: 0.3200 - val_categorical_accuracy: 0.8600 - lr: 2.5000e-04\n",
      "Epoch 20/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1462 - categorical_accuracy: 0.9487\n",
      "Epoch 20: saving model to model_init_2023-02-0906_58_43.961462\\model-00020-0.14623-0.94872-0.37952-0.85000.h5\n",
      "166/166 [==============================] - 123s 742ms/step - loss: 0.1462 - categorical_accuracy: 0.9487 - val_loss: 0.3795 - val_categorical_accuracy: 0.8500 - lr: 2.5000e-04\n",
      "Time Taken to Train this model was :  0:40:35.469968\n"
     ]
    }
   ],
   "source": [
    "batch_size=4\n",
    "trainmodel(model_cnn_GRU_2,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_GRU_2.save(\"model_cnn_GRU_2.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with lesser trainable layers from the pretrained  model where we got the highest accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimenting with lesser trainable layers from the pretrained InceptionV3 model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the model and removing the top layer\n",
    "InceptionV3_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# Experimenting with the same model as above but with only 5 layer as trainable\n",
    "for layer in InceptionV3_model.layers:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Architecture \n",
    "## GlobalAveragePooling3D layer and a dense layer with 1024 neurons added after Inception layers\n",
    "model_Inception_3 = Sequential()\n",
    "model_Inception_3.add(TimeDistributed(InceptionV3_model,input_shape=(30,120,120,3)))\n",
    "model_Inception_3.add(GlobalAveragePooling3D())\n",
    "model_Inception_3.add(Dense(1024, activation='relu'))\n",
    "model_Inception_3.add(Dropout(0.5))\n",
    "model_Inception_3.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_1 (TimeDis  (None, 30, 2, 2, 2048)   21802784  \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " global_average_pooling3d_1   (None, 2048)             0         \n",
      " (GlobalAveragePooling3D)                                        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 5125      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,906,085\n",
      "Trainable params: 2,103,301\n",
      "Non-trainable params: 21,802,784\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Compiling the model using Adam as optimizer\n",
    "model_Inception_3.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_Inception_3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________\n",
      "Batch Size is : 4\n",
      "\n",
      "Source path =  datasets/Project_data/train ; batch size = 4\n",
      "Epoch 1/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 4.6831 - categorical_accuracy: 0.3409\n",
      "Source path =  datasets/Project_data/val ; batch size = 4\n",
      "\n",
      "Epoch 1: saving model to model_init_2023-02-0913_39_16.439780\\model-00001-4.68308-0.34087-1.37232-0.54000.h5\n",
      "166/166 [==============================] - 126s 740ms/step - loss: 4.6831 - categorical_accuracy: 0.3409 - val_loss: 1.3723 - val_categorical_accuracy: 0.5400 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.2468 - categorical_accuracy: 0.5581\n",
      "Epoch 2: saving model to model_init_2023-02-0913_39_16.439780\\model-00002-1.24677-0.55807-0.86845-0.61000.h5\n",
      "166/166 [==============================] - 121s 730ms/step - loss: 1.2468 - categorical_accuracy: 0.5581 - val_loss: 0.8685 - val_categorical_accuracy: 0.6100 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.9831 - categorical_accuracy: 0.6335\n",
      "Epoch 3: saving model to model_init_2023-02-0913_39_16.439780\\model-00003-0.98310-0.63348-1.12643-0.55000.h5\n",
      "166/166 [==============================] - 120s 728ms/step - loss: 0.9831 - categorical_accuracy: 0.6335 - val_loss: 1.1264 - val_categorical_accuracy: 0.5500 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.9294 - categorical_accuracy: 0.6456\n",
      "Epoch 4: saving model to model_init_2023-02-0913_39_16.439780\\model-00004-0.92939-0.64555-1.08654-0.57000.h5\n",
      "166/166 [==============================] - 121s 735ms/step - loss: 0.9294 - categorical_accuracy: 0.6456 - val_loss: 1.0865 - val_categorical_accuracy: 0.5700 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.8364 - categorical_accuracy: 0.6576\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 5: saving model to model_init_2023-02-0913_39_16.439780\\model-00005-0.83643-0.65762-0.94171-0.61000.h5\n",
      "166/166 [==============================] - 121s 730ms/step - loss: 0.8364 - categorical_accuracy: 0.6576 - val_loss: 0.9417 - val_categorical_accuracy: 0.6100 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.6506 - categorical_accuracy: 0.7466\n",
      "Epoch 6: saving model to model_init_2023-02-0913_39_16.439780\\model-00006-0.65058-0.74661-0.87491-0.65000.h5\n",
      "166/166 [==============================] - 121s 732ms/step - loss: 0.6506 - categorical_accuracy: 0.7466 - val_loss: 0.8749 - val_categorical_accuracy: 0.6500 - lr: 5.0000e-04\n",
      "Epoch 7/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5728 - categorical_accuracy: 0.7677\n",
      "Epoch 7: saving model to model_init_2023-02-0913_39_16.439780\\model-00007-0.57279-0.76772-1.03072-0.56000.h5\n",
      "166/166 [==============================] - 121s 734ms/step - loss: 0.5728 - categorical_accuracy: 0.7677 - val_loss: 1.0307 - val_categorical_accuracy: 0.5600 - lr: 5.0000e-04\n",
      "Epoch 8/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.4946 - categorical_accuracy: 0.8281\n",
      "Epoch 8: saving model to model_init_2023-02-0913_39_16.439780\\model-00008-0.49464-0.82805-0.78505-0.70000.h5\n",
      "166/166 [==============================] - 121s 735ms/step - loss: 0.4946 - categorical_accuracy: 0.8281 - val_loss: 0.7850 - val_categorical_accuracy: 0.7000 - lr: 5.0000e-04\n",
      "Epoch 9/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.4886 - categorical_accuracy: 0.8130\n",
      "Epoch 9: saving model to model_init_2023-02-0913_39_16.439780\\model-00009-0.48859-0.81297-0.92651-0.59000.h5\n",
      "166/166 [==============================] - 120s 729ms/step - loss: 0.4886 - categorical_accuracy: 0.8130 - val_loss: 0.9265 - val_categorical_accuracy: 0.5900 - lr: 5.0000e-04\n",
      "Epoch 10/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.4350 - categorical_accuracy: 0.8356\n",
      "Epoch 10: saving model to model_init_2023-02-0913_39_16.439780\\model-00010-0.43497-0.83560-0.83211-0.61000.h5\n",
      "166/166 [==============================] - 121s 733ms/step - loss: 0.4350 - categorical_accuracy: 0.8356 - val_loss: 0.8321 - val_categorical_accuracy: 0.6100 - lr: 5.0000e-04\n",
      "Epoch 11/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.4127 - categorical_accuracy: 0.8386\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 11: saving model to model_init_2023-02-0913_39_16.439780\\model-00011-0.41272-0.83861-0.83586-0.67000.h5\n",
      "166/166 [==============================] - 121s 732ms/step - loss: 0.4127 - categorical_accuracy: 0.8386 - val_loss: 0.8359 - val_categorical_accuracy: 0.6700 - lr: 5.0000e-04\n",
      "Epoch 12/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.3225 - categorical_accuracy: 0.8914\n",
      "Epoch 12: saving model to model_init_2023-02-0913_39_16.439780\\model-00012-0.32251-0.89140-0.91243-0.64000.h5\n",
      "166/166 [==============================] - 121s 731ms/step - loss: 0.3225 - categorical_accuracy: 0.8914 - val_loss: 0.9124 - val_categorical_accuracy: 0.6400 - lr: 2.5000e-04\n",
      "Epoch 13/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2826 - categorical_accuracy: 0.8929\n",
      "Epoch 13: saving model to model_init_2023-02-0913_39_16.439780\\model-00013-0.28262-0.89291-1.00873-0.68000.h5\n",
      "166/166 [==============================] - 122s 740ms/step - loss: 0.2826 - categorical_accuracy: 0.8929 - val_loss: 1.0087 - val_categorical_accuracy: 0.6800 - lr: 2.5000e-04\n",
      "Epoch 14/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2703 - categorical_accuracy: 0.9035\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 14: saving model to model_init_2023-02-0913_39_16.439780\\model-00014-0.27028-0.90347-0.88774-0.63000.h5\n",
      "166/166 [==============================] - 121s 732ms/step - loss: 0.2703 - categorical_accuracy: 0.9035 - val_loss: 0.8877 - val_categorical_accuracy: 0.6300 - lr: 2.5000e-04\n",
      "Epoch 15/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2248 - categorical_accuracy: 0.9276\n",
      "Epoch 15: saving model to model_init_2023-02-0913_39_16.439780\\model-00015-0.22481-0.92760-0.91102-0.66000.h5\n",
      "166/166 [==============================] - 122s 738ms/step - loss: 0.2248 - categorical_accuracy: 0.9276 - val_loss: 0.9110 - val_categorical_accuracy: 0.6600 - lr: 1.2500e-04\n",
      "Epoch 16/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1920 - categorical_accuracy: 0.9487\n",
      "Epoch 16: saving model to model_init_2023-02-0913_39_16.439780\\model-00016-0.19199-0.94872-0.99532-0.69000.h5\n",
      "166/166 [==============================] - 121s 734ms/step - loss: 0.1920 - categorical_accuracy: 0.9487 - val_loss: 0.9953 - val_categorical_accuracy: 0.6900 - lr: 1.2500e-04\n",
      "Epoch 17/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2154 - categorical_accuracy: 0.9261\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 17: saving model to model_init_2023-02-0913_39_16.439780\\model-00017-0.21539-0.92609-0.99053-0.60000.h5\n",
      "166/166 [==============================] - 122s 738ms/step - loss: 0.2154 - categorical_accuracy: 0.9261 - val_loss: 0.9905 - val_categorical_accuracy: 0.6000 - lr: 1.2500e-04\n",
      "Epoch 18/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1678 - categorical_accuracy: 0.9548\n",
      "Epoch 18: saving model to model_init_2023-02-0913_39_16.439780\\model-00018-0.16776-0.95475-0.94625-0.70000.h5\n",
      "166/166 [==============================] - 122s 736ms/step - loss: 0.1678 - categorical_accuracy: 0.9548 - val_loss: 0.9462 - val_categorical_accuracy: 0.7000 - lr: 6.2500e-05\n",
      "Epoch 19/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1557 - categorical_accuracy: 0.9638\n",
      "Epoch 19: saving model to model_init_2023-02-0913_39_16.439780\\model-00019-0.15566-0.96380-1.07774-0.63000.h5\n",
      "166/166 [==============================] - 122s 739ms/step - loss: 0.1557 - categorical_accuracy: 0.9638 - val_loss: 1.0777 - val_categorical_accuracy: 0.6300 - lr: 6.2500e-05\n",
      "Epoch 20/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1625 - categorical_accuracy: 0.9563\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 20: saving model to model_init_2023-02-0913_39_16.439780\\model-00020-0.16247-0.95626-0.85898-0.71000.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166/166 [==============================] - 121s 734ms/step - loss: 0.1625 - categorical_accuracy: 0.9563 - val_loss: 0.8590 - val_categorical_accuracy: 0.7100 - lr: 6.2500e-05\n",
      "Time Taken to Train this model was :  0:40:29.318725\n"
     ]
    }
   ],
   "source": [
    "batch_size=4\n",
    "trainmodel(model_Inception_3,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Inception_3.save(\"model_Inception_3.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimenting using the model_cnn_LSTM_2 architecture with batch size of 2 because a reduction of batch size from 8 to 4 \n",
    "# gave us a slight performance improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture for model_cnn_LSTM_3\n",
    "Input_shape = (len(img_idx), imgdim, imgdim, 3)\n",
    "\n",
    "model_cnn_LSTM_3 = Sequential()\n",
    "\n",
    "model_cnn_LSTM_3.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'),input_shape=Input_shape))\n",
    "model_cnn_LSTM_3.add(TimeDistributed(BatchNormalization()))\n",
    "model_cnn_LSTM_3.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model_cnn_LSTM_3.add(Dropout(0.25))\n",
    "\n",
    "        \n",
    "model_cnn_LSTM_3.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n",
    "model_cnn_LSTM_3.add(TimeDistributed(BatchNormalization()))\n",
    "model_cnn_LSTM_3.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model_cnn_LSTM_3.add(Dropout(0.25))\n",
    "        \n",
    "model_cnn_LSTM_3.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\n",
    "model_cnn_LSTM_3.add(TimeDistributed(BatchNormalization()))\n",
    "model_cnn_LSTM_3.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model_cnn_LSTM_3.add(Dropout(0.25))\n",
    "        \n",
    "model_cnn_LSTM_3.add(TimeDistributed(Conv2D(128, (3, 3) , padding='same', activation='relu')))\n",
    "model_cnn_LSTM_3.add(TimeDistributed(BatchNormalization()))\n",
    "model_cnn_LSTM_3.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model_cnn_LSTM_3.add(Dropout(0.25))\n",
    "\n",
    "model_cnn_LSTM_3.add(TimeDistributed(Conv2D(256, (3, 3) , padding='same', activation='relu')))\n",
    "model_cnn_LSTM_3.add(TimeDistributed(BatchNormalization()))\n",
    "model_cnn_LSTM_3.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model_cnn_LSTM_3.add(Dropout(0.25))\n",
    "\n",
    "model_cnn_LSTM_3.add(TimeDistributed(Conv2D(512, (3, 3) , padding='same', activation='relu')))\n",
    "model_cnn_LSTM_3.add(TimeDistributed(BatchNormalization()))\n",
    "model_cnn_LSTM_3.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model_cnn_LSTM_3.add(Dropout(0.25))\n",
    "\n",
    "model_cnn_LSTM_3.add(TimeDistributed(Flatten()))\n",
    "model_cnn_LSTM_3.add(Dropout(0.25))\n",
    "\n",
    "model_cnn_LSTM_3.add(LSTM(2048))\n",
    "model_cnn_LSTM_3.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model_cnn_LSTM_3.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T06:20:07.757837Z",
     "iopub.status.busy": "2021-10-26T06:20:07.757588Z",
     "iopub.status.idle": "2021-10-26T06:20:07.929675Z",
     "shell.execute_reply": "2021-10-26T06:20:07.929025Z",
     "shell.execute_reply.started": "2021-10-26T06:20:07.757808Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_101 (TimeD  (None, 30, 120, 120, 16)  448      \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_102 (TimeD  (None, 30, 120, 120, 16)  64       \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_103 (TimeD  (None, 30, 60, 60, 16)   0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " dropout_45 (Dropout)        (None, 30, 60, 60, 16)    0         \n",
      "                                                                 \n",
      " time_distributed_104 (TimeD  (None, 30, 60, 60, 32)   4640      \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_105 (TimeD  (None, 30, 60, 60, 32)   128       \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_106 (TimeD  (None, 30, 30, 30, 32)   0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " dropout_46 (Dropout)        (None, 30, 30, 30, 32)    0         \n",
      "                                                                 \n",
      " time_distributed_107 (TimeD  (None, 30, 30, 30, 64)   18496     \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_108 (TimeD  (None, 30, 30, 30, 64)   256       \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_109 (TimeD  (None, 30, 15, 15, 64)   0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " dropout_47 (Dropout)        (None, 30, 15, 15, 64)    0         \n",
      "                                                                 \n",
      " time_distributed_110 (TimeD  (None, 30, 15, 15, 128)  73856     \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_111 (TimeD  (None, 30, 15, 15, 128)  512       \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_112 (TimeD  (None, 30, 7, 7, 128)    0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " dropout_48 (Dropout)        (None, 30, 7, 7, 128)     0         \n",
      "                                                                 \n",
      " time_distributed_113 (TimeD  (None, 30, 7, 7, 256)    295168    \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_114 (TimeD  (None, 30, 7, 7, 256)    1024      \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_115 (TimeD  (None, 30, 3, 3, 256)    0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " dropout_49 (Dropout)        (None, 30, 3, 3, 256)     0         \n",
      "                                                                 \n",
      " time_distributed_116 (TimeD  (None, 30, 3, 3, 512)    1180160   \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_117 (TimeD  (None, 30, 3, 3, 512)    2048      \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_118 (TimeD  (None, 30, 1, 1, 512)    0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " dropout_50 (Dropout)        (None, 30, 1, 1, 512)     0         \n",
      "                                                                 \n",
      " time_distributed_119 (TimeD  (None, 30, 512)          0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " dropout_51 (Dropout)        (None, 30, 512)           0         \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (None, 2048)              20979712  \n",
      "                                                                 \n",
      " dropout_52 (Dropout)        (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 5)                 10245     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,566,757\n",
      "Trainable params: 22,564,741\n",
      "Non-trainable params: 2,016\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Compiling the model using Adam as optimizer\n",
    "model_cnn_LSTM_3.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_cnn_LSTM_3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________\n",
      "Batch Size is : 2\n",
      "\n",
      "Source path =  datasets/Project_data/train ; batch size = 2\n",
      "Epoch 1/20\n",
      "332/332 [==============================] - ETA: 0s - loss: 2.0209 - categorical_accuracy: 0.2504\n",
      "Source path =  datasets/Project_data/val ; batch size = 2\n",
      "\n",
      "Epoch 1: saving model to model_init_2023-02-0913_39_16.439780\\model-00001-2.02095-0.25038-2.19694-0.27000.h5\n",
      "332/332 [==============================] - 125s 371ms/step - loss: 2.0209 - categorical_accuracy: 0.2504 - val_loss: 2.1969 - val_categorical_accuracy: 0.2700 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "332/332 [==============================] - ETA: 0s - loss: 1.5963 - categorical_accuracy: 0.3891\n",
      "Epoch 2: saving model to model_init_2023-02-0913_39_16.439780\\model-00002-1.59626-0.38914-1.66149-0.37000.h5\n",
      "332/332 [==============================] - 123s 372ms/step - loss: 1.5963 - categorical_accuracy: 0.3891 - val_loss: 1.6615 - val_categorical_accuracy: 0.3700 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "332/332 [==============================] - ETA: 0s - loss: 1.4360 - categorical_accuracy: 0.4842\n",
      "Epoch 3: saving model to model_init_2023-02-0913_39_16.439780\\model-00003-1.43602-0.48416-1.81991-0.42000.h5\n",
      "332/332 [==============================] - 123s 371ms/step - loss: 1.4360 - categorical_accuracy: 0.4842 - val_loss: 1.8199 - val_categorical_accuracy: 0.4200 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "332/332 [==============================] - ETA: 0s - loss: 1.2676 - categorical_accuracy: 0.5173\n",
      "Epoch 4: saving model to model_init_2023-02-0913_39_16.439780\\model-00004-1.26757-0.51735-1.77716-0.42000.h5\n",
      "332/332 [==============================] - 123s 370ms/step - loss: 1.2676 - categorical_accuracy: 0.5173 - val_loss: 1.7772 - val_categorical_accuracy: 0.4200 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "332/332 [==============================] - ETA: 0s - loss: 1.0480 - categorical_accuracy: 0.6259\n",
      "Epoch 5: saving model to model_init_2023-02-0913_39_16.439780\\model-00005-1.04802-0.62594-1.21813-0.61000.h5\n",
      "332/332 [==============================] - 123s 372ms/step - loss: 1.0480 - categorical_accuracy: 0.6259 - val_loss: 1.2181 - val_categorical_accuracy: 0.6100 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "332/332 [==============================] - ETA: 0s - loss: 0.9952 - categorical_accuracy: 0.6652\n",
      "Epoch 6: saving model to model_init_2023-02-0913_39_16.439780\\model-00006-0.99519-0.66516-0.84528-0.69000.h5\n",
      "332/332 [==============================] - 123s 370ms/step - loss: 0.9952 - categorical_accuracy: 0.6652 - val_loss: 0.8453 - val_categorical_accuracy: 0.6900 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "332/332 [==============================] - ETA: 0s - loss: 0.9313 - categorical_accuracy: 0.6998\n",
      "Epoch 7: saving model to model_init_2023-02-0913_39_16.439780\\model-00007-0.93130-0.69985-1.46860-0.55000.h5\n",
      "332/332 [==============================] - 124s 373ms/step - loss: 0.9313 - categorical_accuracy: 0.6998 - val_loss: 1.4686 - val_categorical_accuracy: 0.5500 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "332/332 [==============================] - ETA: 0s - loss: 0.8122 - categorical_accuracy: 0.7285\n",
      "Epoch 8: saving model to model_init_2023-02-0913_39_16.439780\\model-00008-0.81220-0.72851-2.38625-0.42000.h5\n",
      "332/332 [==============================] - 124s 374ms/step - loss: 0.8122 - categorical_accuracy: 0.7285 - val_loss: 2.3863 - val_categorical_accuracy: 0.4200 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "332/332 [==============================] - ETA: 0s - loss: 0.7369 - categorical_accuracy: 0.7632\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 9: saving model to model_init_2023-02-0913_39_16.439780\\model-00009-0.73686-0.76320-1.18944-0.66000.h5\n",
      "332/332 [==============================] - 124s 373ms/step - loss: 0.7369 - categorical_accuracy: 0.7632 - val_loss: 1.1894 - val_categorical_accuracy: 0.6600 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "332/332 [==============================] - ETA: 0s - loss: 0.5011 - categorical_accuracy: 0.8190\n",
      "Epoch 10: saving model to model_init_2023-02-0913_39_16.439780\\model-00010-0.50114-0.81900-0.66655-0.79000.h5\n",
      "332/332 [==============================] - 123s 372ms/step - loss: 0.5011 - categorical_accuracy: 0.8190 - val_loss: 0.6666 - val_categorical_accuracy: 0.7900 - lr: 5.0000e-04\n",
      "Epoch 11/20\n",
      "332/332 [==============================] - ETA: 0s - loss: 0.2828 - categorical_accuracy: 0.8974\n",
      "Epoch 11: saving model to model_init_2023-02-0913_39_16.439780\\model-00011-0.28283-0.89744-0.81769-0.78000.h5\n",
      "332/332 [==============================] - 124s 373ms/step - loss: 0.2828 - categorical_accuracy: 0.8974 - val_loss: 0.8177 - val_categorical_accuracy: 0.7800 - lr: 5.0000e-04\n",
      "Epoch 12/20\n",
      "332/332 [==============================] - ETA: 0s - loss: 0.2547 - categorical_accuracy: 0.9170\n",
      "Epoch 12: saving model to model_init_2023-02-0913_39_16.439780\\model-00012-0.25466-0.91704-0.72520-0.76000.h5\n",
      "332/332 [==============================] - 122s 370ms/step - loss: 0.2547 - categorical_accuracy: 0.9170 - val_loss: 0.7252 - val_categorical_accuracy: 0.7600 - lr: 5.0000e-04\n",
      "Epoch 13/20\n",
      "332/332 [==============================] - ETA: 0s - loss: 0.2636 - categorical_accuracy: 0.9110\n",
      "Epoch 13: saving model to model_init_2023-02-0913_39_16.439780\\model-00013-0.26364-0.91101-0.35652-0.89000.h5\n",
      "332/332 [==============================] - 124s 375ms/step - loss: 0.2636 - categorical_accuracy: 0.9110 - val_loss: 0.3565 - val_categorical_accuracy: 0.8900 - lr: 5.0000e-04\n",
      "Epoch 14/20\n",
      "332/332 [==============================] - ETA: 0s - loss: 0.1152 - categorical_accuracy: 0.9608\n",
      "Epoch 14: saving model to model_init_2023-02-0913_39_16.439780\\model-00014-0.11522-0.96078-1.00591-0.77000.h5\n",
      "332/332 [==============================] - 124s 373ms/step - loss: 0.1152 - categorical_accuracy: 0.9608 - val_loss: 1.0059 - val_categorical_accuracy: 0.7700 - lr: 5.0000e-04\n",
      "Epoch 15/20\n",
      "332/332 [==============================] - ETA: 0s - loss: 0.1210 - categorical_accuracy: 0.9608\n",
      "Epoch 15: saving model to model_init_2023-02-0913_39_16.439780\\model-00015-0.12103-0.96078-0.56179-0.85000.h5\n",
      "332/332 [==============================] - 123s 371ms/step - loss: 0.1210 - categorical_accuracy: 0.9608 - val_loss: 0.5618 - val_categorical_accuracy: 0.8500 - lr: 5.0000e-04\n",
      "Epoch 16/20\n",
      "332/332 [==============================] - ETA: 0s - loss: 0.0972 - categorical_accuracy: 0.9608\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 16: saving model to model_init_2023-02-0913_39_16.439780\\model-00016-0.09718-0.96078-0.65084-0.84000.h5\n",
      "332/332 [==============================] - 122s 369ms/step - loss: 0.0972 - categorical_accuracy: 0.9608 - val_loss: 0.6508 - val_categorical_accuracy: 0.8400 - lr: 5.0000e-04\n",
      "Epoch 17/20\n",
      "332/332 [==============================] - ETA: 0s - loss: 0.0877 - categorical_accuracy: 0.9653\n",
      "Epoch 17: saving model to model_init_2023-02-0913_39_16.439780\\model-00017-0.08770-0.96531-0.58472-0.86000.h5\n",
      "332/332 [==============================] - 123s 372ms/step - loss: 0.0877 - categorical_accuracy: 0.9653 - val_loss: 0.5847 - val_categorical_accuracy: 0.8600 - lr: 2.5000e-04\n",
      "Epoch 18/20\n",
      "332/332 [==============================] - ETA: 0s - loss: 0.0667 - categorical_accuracy: 0.9789\n",
      "Epoch 18: saving model to model_init_2023-02-0913_39_16.439780\\model-00018-0.06665-0.97888-0.63944-0.85000.h5\n",
      "332/332 [==============================] - 122s 370ms/step - loss: 0.0667 - categorical_accuracy: 0.9789 - val_loss: 0.6394 - val_categorical_accuracy: 0.8500 - lr: 2.5000e-04\n",
      "Epoch 19/20\n",
      "332/332 [==============================] - ETA: 0s - loss: 0.0356 - categorical_accuracy: 0.9894\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 19: saving model to model_init_2023-02-0913_39_16.439780\\model-00019-0.03558-0.98944-0.53877-0.86000.h5\n",
      "332/332 [==============================] - 122s 369ms/step - loss: 0.0356 - categorical_accuracy: 0.9894 - val_loss: 0.5388 - val_categorical_accuracy: 0.8600 - lr: 2.5000e-04\n",
      "Epoch 20/20\n",
      "332/332 [==============================] - ETA: 0s - loss: 0.0179 - categorical_accuracy: 0.9940\n",
      "Epoch 20: saving model to model_init_2023-02-0913_39_16.439780\\model-00020-0.01786-0.99397-0.47279-0.88000.h5\n",
      "332/332 [==============================] - 123s 370ms/step - loss: 0.0179 - categorical_accuracy: 0.9940 - val_loss: 0.4728 - val_categorical_accuracy: 0.8800 - lr: 1.2500e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken to Train this model was :  0:41:06.691401\n"
     ]
    }
   ],
   "source": [
    "batch_size=2\n",
    "trainmodel(model_cnn_LSTM_3,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_LSTM_3.save(\"model_cnn_LSTM_3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the model_cnn_LSTM architecture with batch size of 4 and adding another LSTM layer (of 1024) to check if we get any\n",
    "# performance benefits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture for model_cnn_LSTM_4\n",
    "Input_shape = (len(img_idx), imgdim, imgdim, 3)\n",
    "\n",
    "model_cnn_LSTM_4 = Sequential()\n",
    "\n",
    "model_cnn_LSTM_4.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'),input_shape=Input_shape))\n",
    "model_cnn_LSTM_4.add(TimeDistributed(BatchNormalization()))\n",
    "model_cnn_LSTM_4.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model_cnn_LSTM_4.add(Dropout(0.25))\n",
    "\n",
    "        \n",
    "model_cnn_LSTM_4.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n",
    "model_cnn_LSTM_4.add(TimeDistributed(BatchNormalization()))\n",
    "model_cnn_LSTM_4.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model_cnn_LSTM_4.add(Dropout(0.25))\n",
    "        \n",
    "model_cnn_LSTM_4.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\n",
    "model_cnn_LSTM_4.add(TimeDistributed(BatchNormalization()))\n",
    "model_cnn_LSTM_4.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model_cnn_LSTM_4.add(Dropout(0.25))\n",
    "        \n",
    "model_cnn_LSTM_4.add(TimeDistributed(Conv2D(128, (3, 3) , padding='same', activation='relu')))\n",
    "model_cnn_LSTM_4.add(TimeDistributed(BatchNormalization()))\n",
    "model_cnn_LSTM_4.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model_cnn_LSTM_4.add(Dropout(0.25))\n",
    "\n",
    "model_cnn_LSTM_4.add(TimeDistributed(Conv2D(256, (3, 3) , padding='same', activation='relu')))\n",
    "model_cnn_LSTM_4.add(TimeDistributed(BatchNormalization()))\n",
    "model_cnn_LSTM_4.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model_cnn_LSTM_4.add(Dropout(0.25))\n",
    "\n",
    "model_cnn_LSTM_4.add(TimeDistributed(Conv2D(512, (3, 3) , padding='same', activation='relu')))\n",
    "model_cnn_LSTM_4.add(TimeDistributed(BatchNormalization()))\n",
    "model_cnn_LSTM_4.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model_cnn_LSTM_4.add(Dropout(0.25))\n",
    "\n",
    "model_cnn_LSTM_4.add(TimeDistributed(Flatten()))\n",
    "model_cnn_LSTM_4.add(Dropout(0.25))\n",
    "\n",
    "model_cnn_LSTM_4.add(LSTM(2048,return_sequences=True))\n",
    "model_cnn_LSTM_4.add(Dropout(0.5))\n",
    "model_cnn_LSTM_4.add(LSTM(1024))\n",
    "model_cnn_LSTM_4.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model_cnn_LSTM_4.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T06:20:07.757837Z",
     "iopub.status.busy": "2021-10-26T06:20:07.757588Z",
     "iopub.status.idle": "2021-10-26T06:20:07.929675Z",
     "shell.execute_reply": "2021-10-26T06:20:07.929025Z",
     "shell.execute_reply.started": "2021-10-26T06:20:07.757808Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_215 (TimeD  (None, 30, 120, 120, 16)  448      \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_216 (TimeD  (None, 30, 120, 120, 16)  64       \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_217 (TimeD  (None, 30, 60, 60, 16)   0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " dropout_101 (Dropout)       (None, 30, 60, 60, 16)    0         \n",
      "                                                                 \n",
      " time_distributed_218 (TimeD  (None, 30, 60, 60, 32)   4640      \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_219 (TimeD  (None, 30, 60, 60, 32)   128       \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_220 (TimeD  (None, 30, 30, 30, 32)   0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " dropout_102 (Dropout)       (None, 30, 30, 30, 32)    0         \n",
      "                                                                 \n",
      " time_distributed_221 (TimeD  (None, 30, 30, 30, 64)   18496     \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_222 (TimeD  (None, 30, 30, 30, 64)   256       \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_223 (TimeD  (None, 30, 15, 15, 64)   0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " dropout_103 (Dropout)       (None, 30, 15, 15, 64)    0         \n",
      "                                                                 \n",
      " time_distributed_224 (TimeD  (None, 30, 15, 15, 128)  73856     \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_225 (TimeD  (None, 30, 15, 15, 128)  512       \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_226 (TimeD  (None, 30, 7, 7, 128)    0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " dropout_104 (Dropout)       (None, 30, 7, 7, 128)     0         \n",
      "                                                                 \n",
      " time_distributed_227 (TimeD  (None, 30, 7, 7, 256)    295168    \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_228 (TimeD  (None, 30, 7, 7, 256)    1024      \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_229 (TimeD  (None, 30, 3, 3, 256)    0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " dropout_105 (Dropout)       (None, 30, 3, 3, 256)     0         \n",
      "                                                                 \n",
      " time_distributed_230 (TimeD  (None, 30, 3, 3, 512)    1180160   \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_231 (TimeD  (None, 30, 3, 3, 512)    2048      \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_232 (TimeD  (None, 30, 1, 1, 512)    0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " dropout_106 (Dropout)       (None, 30, 1, 1, 512)     0         \n",
      "                                                                 \n",
      " time_distributed_233 (TimeD  (None, 30, 512)          0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " dropout_107 (Dropout)       (None, 30, 512)           0         \n",
      "                                                                 \n",
      " lstm_19 (LSTM)              (None, 30, 2048)          20979712  \n",
      "                                                                 \n",
      " dropout_108 (Dropout)       (None, 30, 2048)          0         \n",
      "                                                                 \n",
      " lstm_20 (LSTM)              (None, 1024)              12587008  \n",
      "                                                                 \n",
      " dropout_109 (Dropout)       (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 5)                 5125      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 35,148,645\n",
      "Trainable params: 35,146,629\n",
      "Non-trainable params: 2,016\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Compiling the model using Adam as optimizer\n",
    "model_cnn_LSTM_4.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_cnn_LSTM_4.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________\n",
      "Batch Size is : 4\n",
      "\n",
      "Source path =  datasets/Project_data/train ; batch size = 4\n",
      "Epoch 1/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.8824 - categorical_accuracy: 0.2127\n",
      "Source path =  datasets/Project_data/val ; batch size = 4\n",
      "\n",
      "Epoch 1: saving model to model_init_2023-02-0913_39_16.439780\\model-00001-1.88235-0.21267-1.71655-0.21000.h5\n",
      "166/166 [==============================] - 124s 738ms/step - loss: 1.8824 - categorical_accuracy: 0.2127 - val_loss: 1.7166 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.6748 - categorical_accuracy: 0.2956\n",
      "Epoch 2: saving model to model_init_2023-02-0913_39_16.439780\\model-00002-1.67481-0.29563-2.16548-0.20000.h5\n",
      "166/166 [==============================] - 122s 736ms/step - loss: 1.6748 - categorical_accuracy: 0.2956 - val_loss: 2.1655 - val_categorical_accuracy: 0.2000 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.6306 - categorical_accuracy: 0.2670\n",
      "Epoch 3: saving model to model_init_2023-02-0913_39_16.439780\\model-00003-1.63063-0.26697-1.54388-0.32000.h5\n",
      "166/166 [==============================] - 121s 732ms/step - loss: 1.6306 - categorical_accuracy: 0.2670 - val_loss: 1.5439 - val_categorical_accuracy: 0.3200 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.5080 - categorical_accuracy: 0.3183\n",
      "Epoch 4: saving model to model_init_2023-02-0913_39_16.439780\\model-00004-1.50801-0.31825-1.77014-0.17000.h5\n",
      "166/166 [==============================] - 121s 735ms/step - loss: 1.5080 - categorical_accuracy: 0.3183 - val_loss: 1.7701 - val_categorical_accuracy: 0.1700 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.4478 - categorical_accuracy: 0.3605\n",
      "Epoch 5: saving model to model_init_2023-02-0913_39_16.439780\\model-00005-1.44780-0.36048-1.35393-0.33000.h5\n",
      "166/166 [==============================] - 121s 734ms/step - loss: 1.4478 - categorical_accuracy: 0.3605 - val_loss: 1.3539 - val_categorical_accuracy: 0.3300 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.3710 - categorical_accuracy: 0.3575\n",
      "Epoch 6: saving model to model_init_2023-02-0913_39_16.439780\\model-00006-1.37103-0.35747-1.59158-0.31000.h5\n",
      "166/166 [==============================] - 121s 732ms/step - loss: 1.3710 - categorical_accuracy: 0.3575 - val_loss: 1.5916 - val_categorical_accuracy: 0.3100 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.3247 - categorical_accuracy: 0.3997\n",
      "Epoch 7: saving model to model_init_2023-02-0913_39_16.439780\\model-00007-1.32469-0.39970-1.34682-0.33000.h5\n",
      "166/166 [==============================] - 121s 733ms/step - loss: 1.3247 - categorical_accuracy: 0.3997 - val_loss: 1.3468 - val_categorical_accuracy: 0.3300 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.3116 - categorical_accuracy: 0.3922\n",
      "Epoch 8: saving model to model_init_2023-02-0913_39_16.439780\\model-00008-1.31156-0.39216-1.74209-0.20000.h5\n",
      "166/166 [==============================] - 121s 731ms/step - loss: 1.3116 - categorical_accuracy: 0.3922 - val_loss: 1.7421 - val_categorical_accuracy: 0.2000 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.2699 - categorical_accuracy: 0.4344\n",
      "Epoch 9: saving model to model_init_2023-02-0913_39_16.439780\\model-00009-1.26992-0.43439-1.25813-0.55000.h5\n",
      "166/166 [==============================] - 121s 732ms/step - loss: 1.2699 - categorical_accuracy: 0.4344 - val_loss: 1.2581 - val_categorical_accuracy: 0.5500 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.0392 - categorical_accuracy: 0.5626\n",
      "Epoch 10: saving model to model_init_2023-02-0913_39_16.439780\\model-00010-1.03918-0.56259-2.35125-0.41000.h5\n",
      "166/166 [==============================] - 122s 736ms/step - loss: 1.0392 - categorical_accuracy: 0.5626 - val_loss: 2.3513 - val_categorical_accuracy: 0.4100 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.8978 - categorical_accuracy: 0.6365\n",
      "Epoch 11: saving model to model_init_2023-02-0913_39_16.439780\\model-00011-0.89783-0.63650-0.63342-0.82000.h5\n",
      "166/166 [==============================] - 121s 730ms/step - loss: 0.8978 - categorical_accuracy: 0.6365 - val_loss: 0.6334 - val_categorical_accuracy: 0.8200 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.7616 - categorical_accuracy: 0.7255\n",
      "Epoch 12: saving model to model_init_2023-02-0913_39_16.439780\\model-00012-0.76164-0.72549-0.67205-0.72000.h5\n",
      "166/166 [==============================] - 122s 738ms/step - loss: 0.7616 - categorical_accuracy: 0.7255 - val_loss: 0.6721 - val_categorical_accuracy: 0.7200 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5522 - categorical_accuracy: 0.8100\n",
      "Epoch 13: saving model to model_init_2023-02-0913_39_16.439780\\model-00013-0.55223-0.80995-0.62103-0.77000.h5\n",
      "166/166 [==============================] - 121s 730ms/step - loss: 0.5522 - categorical_accuracy: 0.8100 - val_loss: 0.6210 - val_categorical_accuracy: 0.7700 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5085 - categorical_accuracy: 0.8311\n",
      "Epoch 14: saving model to model_init_2023-02-0913_39_16.439780\\model-00014-0.50845-0.83107-0.48200-0.86000.h5\n",
      "166/166 [==============================] - 122s 734ms/step - loss: 0.5085 - categorical_accuracy: 0.8311 - val_loss: 0.4820 - val_categorical_accuracy: 0.8600 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.4427 - categorical_accuracy: 0.8597\n",
      "Epoch 15: saving model to model_init_2023-02-0913_39_16.439780\\model-00015-0.44271-0.85973-0.68484-0.77000.h5\n",
      "166/166 [==============================] - 120s 729ms/step - loss: 0.4427 - categorical_accuracy: 0.8597 - val_loss: 0.6848 - val_categorical_accuracy: 0.7700 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.4945 - categorical_accuracy: 0.8356\n",
      "Epoch 16: saving model to model_init_2023-02-0913_39_16.439780\\model-00016-0.49452-0.83560-0.54913-0.81000.h5\n",
      "166/166 [==============================] - 123s 743ms/step - loss: 0.4945 - categorical_accuracy: 0.8356 - val_loss: 0.5491 - val_categorical_accuracy: 0.8100 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.3532 - categorical_accuracy: 0.8824\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 17: saving model to model_init_2023-02-0913_39_16.439780\\model-00017-0.35321-0.88235-0.91332-0.80000.h5\n",
      "166/166 [==============================] - 123s 743ms/step - loss: 0.3532 - categorical_accuracy: 0.8824 - val_loss: 0.9133 - val_categorical_accuracy: 0.8000 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2203 - categorical_accuracy: 0.9276\n",
      "Epoch 18: saving model to model_init_2023-02-0913_39_16.439780\\model-00018-0.22032-0.92760-0.45766-0.83000.h5\n",
      "166/166 [==============================] - 122s 740ms/step - loss: 0.2203 - categorical_accuracy: 0.9276 - val_loss: 0.4577 - val_categorical_accuracy: 0.8300 - lr: 5.0000e-04\n",
      "Epoch 19/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2094 - categorical_accuracy: 0.9291\n",
      "Epoch 19: saving model to model_init_2023-02-0913_39_16.439780\\model-00019-0.20944-0.92911-0.74635-0.83000.h5\n",
      "166/166 [==============================] - 121s 733ms/step - loss: 0.2094 - categorical_accuracy: 0.9291 - val_loss: 0.7464 - val_categorical_accuracy: 0.8300 - lr: 5.0000e-04\n",
      "Epoch 20/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1555 - categorical_accuracy: 0.9487\n",
      "Epoch 20: saving model to model_init_2023-02-0913_39_16.439780\\model-00020-0.15551-0.94872-0.28620-0.89000.h5\n",
      "166/166 [==============================] - 123s 745ms/step - loss: 0.1555 - categorical_accuracy: 0.9487 - val_loss: 0.2862 - val_categorical_accuracy: 0.8900 - lr: 5.0000e-04\n",
      "Time Taken to Train this model was :  0:40:33.731109\n"
     ]
    }
   ],
   "source": [
    "batch_size=4\n",
    "trainmodel(model_cnn_LSTM_4,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_LSTM_4.save(\"model_cnn_LSTM_4.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model model_cnn_3 gave us the highest validation accuracy till now, we will try and squeeze some more improvement from the model by running it for another 20 epochs (with batch_size of 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model_cnn_3_new = load_model('model_cnn_3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________\n",
      "Batch Size is : 4\n",
      "\n",
      "Source path =  datasets/Project_data/train ; batch size = 4\n",
      "Epoch 1/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1413 - categorical_accuracy: 0.9517\n",
      "Source path =  datasets/Project_data/val ; batch size = 4\n",
      "\n",
      "Epoch 1: saving model to model_init_2023-02-0913_39_16.439780\\model-00001-0.14129-0.95173-0.40586-0.87000.h5\n",
      "166/166 [==============================] - 121s 732ms/step - loss: 0.1413 - categorical_accuracy: 0.9517 - val_loss: 0.4059 - val_categorical_accuracy: 0.8700 - lr: 5.0000e-04\n",
      "Epoch 2/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0945 - categorical_accuracy: 0.9729\n",
      "Epoch 2: saving model to model_init_2023-02-0913_39_16.439780\\model-00002-0.09455-0.97285-0.25388-0.92000.h5\n",
      "166/166 [==============================] - 121s 735ms/step - loss: 0.0945 - categorical_accuracy: 0.9729 - val_loss: 0.2539 - val_categorical_accuracy: 0.9200 - lr: 5.0000e-04\n",
      "Epoch 3/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0656 - categorical_accuracy: 0.9759\n",
      "Epoch 3: saving model to model_init_2023-02-0913_39_16.439780\\model-00003-0.06563-0.97587-0.87645-0.78000.h5\n",
      "166/166 [==============================] - 121s 735ms/step - loss: 0.0656 - categorical_accuracy: 0.9759 - val_loss: 0.8764 - val_categorical_accuracy: 0.7800 - lr: 5.0000e-04\n",
      "Epoch 4/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0696 - categorical_accuracy: 0.9713\n",
      "Epoch 4: saving model to model_init_2023-02-0913_39_16.439780\\model-00004-0.06960-0.97134-0.51379-0.89000.h5\n",
      "166/166 [==============================] - 121s 734ms/step - loss: 0.0696 - categorical_accuracy: 0.9713 - val_loss: 0.5138 - val_categorical_accuracy: 0.8900 - lr: 5.0000e-04\n",
      "Epoch 5/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0571 - categorical_accuracy: 0.9774\n",
      "Epoch 5: saving model to model_init_2023-02-0913_39_16.439780\\model-00005-0.05710-0.97738-0.24173-0.92000.h5\n",
      "166/166 [==============================] - 122s 737ms/step - loss: 0.0571 - categorical_accuracy: 0.9774 - val_loss: 0.2417 - val_categorical_accuracy: 0.9200 - lr: 5.0000e-04\n",
      "Epoch 6/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0954 - categorical_accuracy: 0.9698\n",
      "Epoch 6: saving model to model_init_2023-02-0913_39_16.439780\\model-00006-0.09538-0.96983-0.39018-0.88000.h5\n",
      "166/166 [==============================] - 122s 735ms/step - loss: 0.0954 - categorical_accuracy: 0.9698 - val_loss: 0.3902 - val_categorical_accuracy: 0.8800 - lr: 5.0000e-04\n",
      "Epoch 7/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0515 - categorical_accuracy: 0.9864\n",
      "Epoch 7: saving model to model_init_2023-02-0913_39_16.439780\\model-00007-0.05152-0.98643-0.51552-0.87000.h5\n",
      "166/166 [==============================] - 121s 732ms/step - loss: 0.0515 - categorical_accuracy: 0.9864 - val_loss: 0.5155 - val_categorical_accuracy: 0.8700 - lr: 5.0000e-04\n",
      "Epoch 8/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0666 - categorical_accuracy: 0.9774\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 8: saving model to model_init_2023-02-0913_39_16.439780\\model-00008-0.06663-0.97738-0.29179-0.95000.h5\n",
      "166/166 [==============================] - 122s 737ms/step - loss: 0.0666 - categorical_accuracy: 0.9774 - val_loss: 0.2918 - val_categorical_accuracy: 0.9500 - lr: 5.0000e-04\n",
      "Epoch 9/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0458 - categorical_accuracy: 0.9834\n",
      "Epoch 9: saving model to model_init_2023-02-0913_39_16.439780\\model-00009-0.04576-0.98341-0.29874-0.92000.h5\n",
      "166/166 [==============================] - 121s 732ms/step - loss: 0.0458 - categorical_accuracy: 0.9834 - val_loss: 0.2987 - val_categorical_accuracy: 0.9200 - lr: 2.5000e-04\n",
      "Epoch 10/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0568 - categorical_accuracy: 0.9834\n",
      "Epoch 10: saving model to model_init_2023-02-0913_39_16.439780\\model-00010-0.05679-0.98341-0.27371-0.95000.h5\n",
      "166/166 [==============================] - 122s 736ms/step - loss: 0.0568 - categorical_accuracy: 0.9834 - val_loss: 0.2737 - val_categorical_accuracy: 0.9500 - lr: 2.5000e-04\n",
      "Epoch 11/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0357 - categorical_accuracy: 0.9864\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 11: saving model to model_init_2023-02-0913_39_16.439780\\model-00011-0.03565-0.98643-0.30683-0.91000.h5\n",
      "166/166 [==============================] - 121s 732ms/step - loss: 0.0357 - categorical_accuracy: 0.9864 - val_loss: 0.3068 - val_categorical_accuracy: 0.9100 - lr: 2.5000e-04\n",
      "Epoch 12/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0257 - categorical_accuracy: 0.9925\n",
      "Epoch 12: saving model to model_init_2023-02-0913_39_16.439780\\model-00012-0.02573-0.99246-0.42861-0.92000.h5\n",
      "166/166 [==============================] - 121s 733ms/step - loss: 0.0257 - categorical_accuracy: 0.9925 - val_loss: 0.4286 - val_categorical_accuracy: 0.9200 - lr: 1.2500e-04\n",
      "Epoch 13/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0219 - categorical_accuracy: 0.9970\n",
      "Epoch 13: saving model to model_init_2023-02-0913_39_16.439780\\model-00013-0.02191-0.99698-0.15583-0.96000.h5\n",
      "166/166 [==============================] - 121s 732ms/step - loss: 0.0219 - categorical_accuracy: 0.9970 - val_loss: 0.1558 - val_categorical_accuracy: 0.9600 - lr: 1.2500e-04\n",
      "Epoch 14/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0231 - categorical_accuracy: 0.9910\n",
      "Epoch 14: saving model to model_init_2023-02-0913_39_16.439780\\model-00014-0.02307-0.99095-0.32434-0.93000.h5\n",
      "166/166 [==============================] - 121s 734ms/step - loss: 0.0231 - categorical_accuracy: 0.9910 - val_loss: 0.3243 - val_categorical_accuracy: 0.9300 - lr: 1.2500e-04\n",
      "Epoch 15/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0232 - categorical_accuracy: 0.9955\n",
      "Epoch 15: saving model to model_init_2023-02-0913_39_16.439780\\model-00015-0.02323-0.99548-0.14874-0.93000.h5\n",
      "166/166 [==============================] - 122s 739ms/step - loss: 0.0232 - categorical_accuracy: 0.9955 - val_loss: 0.1487 - val_categorical_accuracy: 0.9300 - lr: 1.2500e-04\n",
      "Epoch 16/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0224 - categorical_accuracy: 0.9940\n",
      "Epoch 16: saving model to model_init_2023-02-0913_39_16.439780\\model-00016-0.02238-0.99397-0.45492-0.90000.h5\n",
      "166/166 [==============================] - 121s 734ms/step - loss: 0.0224 - categorical_accuracy: 0.9940 - val_loss: 0.4549 - val_categorical_accuracy: 0.9000 - lr: 1.2500e-04\n",
      "Epoch 17/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0286 - categorical_accuracy: 0.9940\n",
      "Epoch 17: saving model to model_init_2023-02-0913_39_16.439780\\model-00017-0.02862-0.99397-0.37143-0.87000.h5\n",
      "166/166 [==============================] - 122s 740ms/step - loss: 0.0286 - categorical_accuracy: 0.9940 - val_loss: 0.3714 - val_categorical_accuracy: 0.8700 - lr: 1.2500e-04\n",
      "Epoch 18/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0263 - categorical_accuracy: 0.9940\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 18: saving model to model_init_2023-02-0913_39_16.439780\\model-00018-0.02627-0.99397-0.28362-0.93000.h5\n",
      "166/166 [==============================] - 121s 732ms/step - loss: 0.0263 - categorical_accuracy: 0.9940 - val_loss: 0.2836 - val_categorical_accuracy: 0.9300 - lr: 1.2500e-04\n",
      "Epoch 19/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0147 - categorical_accuracy: 0.9985\n",
      "Epoch 19: saving model to model_init_2023-02-0913_39_16.439780\\model-00019-0.01473-0.99849-0.31364-0.93000.h5\n",
      "166/166 [==============================] - 121s 733ms/step - loss: 0.0147 - categorical_accuracy: 0.9985 - val_loss: 0.3136 - val_categorical_accuracy: 0.9300 - lr: 6.2500e-05\n",
      "Epoch 20/20\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0199 - categorical_accuracy: 0.9925\n",
      "Epoch 20: saving model to model_init_2023-02-0913_39_16.439780\\model-00020-0.01986-0.99246-0.20321-0.96000.h5\n",
      "166/166 [==============================] - 121s 733ms/step - loss: 0.0199 - categorical_accuracy: 0.9925 - val_loss: 0.2032 - val_categorical_accuracy: 0.9600 - lr: 6.2500e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken to Train this model was :  0:40:28.473049\n"
     ]
    }
   ],
   "source": [
    "# Running 20 more epoch \n",
    "num_epochs=20\n",
    "batch_size=4\n",
    "trainmodel(model_cnn_3_new,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model that now has run for 40 epoch\n",
    "model_cnn_3_new.save(\"model_cnn_3_new.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation \n",
    "\n",
    "Visualizing and comparing metrics using the Top 3 models where we got the best validation accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing model on Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the generator to load back the data as we know we have 100 files in our Val folder\n",
    "# we will set the batch size to 100 so all the val data can be loaded at once\n",
    "batch_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_val_data=generator(val_path, val_doc, batch_size,imgdim,img_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Source path =  datasets/Project_data/val ; batch size = 100\n"
     ]
    }
   ],
   "source": [
    "#The frames are assigned to the var frames and labels to labels\n",
    "frames, labels = next(all_val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The labels returned are in the numpy array format, we will assign it to list for further processing\n",
    "labels=labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Also the labels are in one-hot coded format, we will change it to simple class label format\n",
    "actual=[] \n",
    "for i in range(len(labels)):\n",
    "    actual.append(labels[i].index(1)) #--- Find the index of 1 and that will be our class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "namelist=list(classname.keys()) #--- We has assigned this dictionary during the initial steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Swipe Left', 'Swipe Right', 'Stop', 'Thumbs Down', 'Thumbs Up']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "namelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The predictanddraw function will be used to evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating function that will\n",
    "## 1) Predict the output on the validation set using the model passed to this function\n",
    "## 2) Draw the heatmap \n",
    "## 3) Calculate the mean precision and recall \n",
    "## 4) Calculate the precision and recall by class\n",
    "######\n",
    "# input to this functions are the model to be used for prediction, frames, actual label (0-4) and the name of the labels\n",
    "# and title that will be used as plot title\n",
    "def predictanddraw(modelname,frames,actual,namelist,title): \n",
    "        predicted=modelname.predict(frames) ##---------Predicting\n",
    "        actual = tf.stack(actual, axis=0)\n",
    "        predicted = tf.concat(predicted, axis=0)\n",
    "        predicted = tf.argmax(predicted, axis=1)\n",
    "        cm = tf.math.confusion_matrix(actual, predicted) #----- Confusion matrix using the actual and predicted values\n",
    "        ### Heat map\n",
    "        plt.figure(figsize=(4,4))\n",
    "        axes = sns.heatmap(cm, annot=True, square=True)\n",
    "        axes.set_title(title)\n",
    "        axes.set_xlabel('Predicted Action')\n",
    "        axes.set_ylabel('Actual Action')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.yticks(rotation=0)\n",
    "        axes.xaxis.set_ticklabels(namelist)\n",
    "        axes.yaxis.set_ticklabels(namelist)\n",
    "        ## Precision and Recall\n",
    "        tp = np.diag(cm) #------ ---Diagonal represents true positives\n",
    "        recall = np.mean(np.diag(cm) / np.sum(cm, axis = 1))    # ----- -- Mean Recall\n",
    "        precision = np.mean(np.diag(cm) / np.sum(cm, axis = 0)) # ----- -- Mean Precision\n",
    "         ## Precision and Recall by class\n",
    "        print()\n",
    "        print(\"Precision and Recall by Class for\",title,\" :\")\n",
    "        print()\n",
    "        print(\"{:<14} {:<10} {:<10}\".format('Class Name', 'Precision', 'Recall'))\n",
    "        for i in range(len(namelist)):\n",
    "            col = cm[:, i]\n",
    "            fp = np.sum(col) - tp[i] # Sum of column minus true positive is false positive\n",
    "            row = cm[i, :]\n",
    "            fn = np.sum(row) - tp[i] # Sum of row minus true positive, is false negative\n",
    "            precision1=tp[i] / (tp[i] + fp)\n",
    "            recall1= tp[i] / (tp[i] + fn)\n",
    "            print(\"{:<14} {:<10} {:<10}\".format(namelist[i],\"%.3f\" % precision1,\"%.3f\" % recall1))\n",
    "        print()\n",
    "        print(\"The mean precision of this model is :\",\"%.3f\" % precision)\n",
    "        print(\"The mean Recall of this model is    :\",\"%.3f\" %recall)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 3s 187ms/step\n",
      "\n",
      "Precision and Recall by Class for model_cnn_3_new  :\n",
      "\n",
      "Class Name     Precision  Recall    \n",
      "Swipe Left     1.000      0.944     \n",
      "Swipe Right    1.000      1.000     \n",
      "Stop           0.952      0.909     \n",
      "Thumbs Down    0.909      0.952     \n",
      "Thumbs Up      0.882      0.938     \n",
      "\n",
      "The mean precision of this model is : 0.949\n",
      "The mean Recall of this model is    : 0.949\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 400x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_cnn_3_new = load_model('model_cnn_3_new.h5')\n",
    "predictanddraw(model_cnn_3_new,frames,actual,namelist,\"model_cnn_3_new\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 208ms/step\n",
      "\n",
      "Precision and Recall by Class for model_cnn_3  :\n",
      "\n",
      "Class Name     Precision  Recall    \n",
      "Swipe Left     1.000      0.944     \n",
      "Swipe Right    1.000      1.000     \n",
      "Stop           0.833      0.909     \n",
      "Thumbs Down    0.864      0.905     \n",
      "Thumbs Up      0.857      0.750     \n",
      "\n",
      "The mean precision of this model is : 0.911\n",
      "The mean Recall of this model is    : 0.902\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 400x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_cnn_3 = load_model('model_cnn_3.h5')\n",
    "predictanddraw(model_cnn_3,frames,actual,namelist,\"model_cnn_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 210ms/step\n",
      "\n",
      "Precision and Recall by Class for model_cnn_2  :\n",
      "\n",
      "Class Name     Precision  Recall    \n",
      "Swipe Left     0.850      0.944     \n",
      "Swipe Right    1.000      1.000     \n",
      "Stop           0.952      0.909     \n",
      "Thumbs Down    1.000      0.667     \n",
      "Thumbs Up      0.682      0.938     \n",
      "\n",
      "The mean precision of this model is : 0.897\n",
      "The mean Recall of this model is    : 0.892\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 400x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_cnn_2 = load_model('model_cnn_2.h5')\n",
    "predictanddraw(model_cnn_2,frames,actual,namelist,\"model_cnn_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
